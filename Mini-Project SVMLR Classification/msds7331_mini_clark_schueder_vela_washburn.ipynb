{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jjschueder/7331DataMiningNotebooks/blob/master/Mini-Project%20SVMLR%20Classification/msds7331_mini_clark_schueder_vela_washburn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhqdMtT3nSiG"
   },
   "source": [
    "# Mini-Project: SVM & LR Classification - project guidelines\n",
    "You are to perform predictive analysis (classification) upon a data set: model the dataset using\n",
    "methods we have discussed in class: logistic regression & support vector machines and making\n",
    "conclusions from the analysis. Follow the CRISP-DM framework in your analysis (you are not\n",
    "performing all of the CRISP-DM outline, only the portions relevant to the grading rubric outlined\n",
    "below). This report is worth 10% of the final grade. You may complete this assignment in teams\n",
    "of as many as three people.\n",
    "Write a report covering all the steps of the project. The format of the document can be PDF,\n",
    "*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in\n",
    "the rendered Jupyter notebook. The results should be reproducible using your report. Please\n",
    "carefully describe every assumption and every step in your report.\n",
    "A note on grading: A common mistake I see in this lab is not investigating different input\n",
    "parameters for each model. Try a number of parameter combinations and discuss how the model changed. \n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSgE3De7drJu"
   },
   "source": [
    "# Mini-Project: SVM & LR Classification - project requirements\n",
    "*   [50 points] Logistic Regression and Support Vector Machine models\n",
    "   * Create logistic regression model and support vector machine model for classification task in dataset.\n",
    "      * Logistic regression - fulfilled w/ M1, M2, M5, M6\n",
    "      * SVM - fulfilled w/ M3, M4\n",
    "   * Assess how well each model performs (use 80/20 training/testing split of our data)\n",
    "      * Fulfilled w/ prediction section of each model\n",
    "   * Adjust parameters of the models to make them more accurate\n",
    "      * Logistic Regression - fulfilled w/ M2\n",
    "      * SVM - fulled w/ M4\n",
    "   * If dataset size requires stochastic gradient descent, then linear kernel only is fine - meaning: SGDClassifier can be used to optimize logistic regression and linear support vector machines.  For many problems, SGD will be required in order to train the SVM model in reasonable timeframe\n",
    "      * Logistic Regression - fulfilled w/ M5, M6\n",
    "      * SVM - fulfilled w/ M7\n",
    "*   [10 points] Model advantages\n",
    "   * Discuss advantages of each model for each classfication task\n",
    "      * Fulfilled in Evaluation-->Model Advantages\n",
    "   * Does type of model offer superior performance over another in terms of prediction accuracy\n",
    "      * Fulfilled in Evaluation-->Model Advantages Conclusion\n",
    "   * Does type of model offer superior performance over another in terms training time or efficiency\n",
    "      * Fulfilled in the Evaluation-->Model Advantages (each type)\n",
    "*   [30 points] Feature importance\n",
    "   * Use the weights from logistic regression to interpret the importance of different features for the classification task in detail\n",
    "      * Fulfilled in Evaluation-->Feature Importance\n",
    "   * Why do we think some variables are more important than others\n",
    "      * Fulfilled in Evaluation-->Feature Importance\n",
    "*   [10 points] Support Vector Analysis\n",
    "   * Looking at the chosen support vectors for the classfication task, answer the following:\n",
    "      * Do these provide any insight into the data? Explain\n",
    "      * If stochastic gradient descent used; therefore, did not explicitlity solve for support vectors\n",
    "         * Subsample the data to train the SVC model\n",
    "            * SVM - fulfilled in M7\n",
    "         * Analyze support vectors from subsampled dataset\n",
    "            * Fulfilled in Evaluation-->Support Vector Analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmTlKM_UnSiP"
   },
   "source": [
    "#Mini-Project: SVM & LR Classification"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBwy_rCue3sN"
   },
   "source": [
    "##Team\n",
    "\n",
    "* Joseph Schueder\n",
    "* Armando Vela\n",
    "* Daniel Clark\n",
    "* Jeff Washburn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBTqQnSVewMl"
   },
   "source": [
    "##Business Understanding\n",
    "\n",
    "Introduction - The Iowa Liquor Sales dataset is an API from Google’s Bigquery which contains the wholesale purchases by retail stores in the Iowa area.  The dataset includes the spirit purchase details by product, date of purchase, and location the item was purchased from an Iowa Class “E” liquor license holder (retail stores). The time frame of this data starts from January 1, 2012 through December 31, 2019. As part of the study commissioned by the Iowa Department of Commerce, all alcoholic sales within the state were logged into the Department system, and in turn, published as open data by the State of Iowa. The dataset contains detail on the name, product, quantity and location of the individual container or package sale between the wholesaler (vendor) and the retailer.\n",
    "\n",
    "\n",
    "\n",
    "###1.   Set Objectives\n",
    "\n",
    "\n",
    "> Having found that Iowa is a prime market for different kind of liquor sales, we want to investigate the current sales trends between Whiskey Sales vs Non-Whiskey sales.  We would like to classify the liquor sales based on if it will be a Whiskey sale or not.  \n",
    "\n",
    "\n",
    "###2.   Product Project Plan\n",
    "\n",
    "> To meet our goals, we will build upon the dataset we used in our initial analysis to focus in on the Whiskey and Non-Whiskey sales.  We will use the same 400k rows from the last analysis and in addition further cleansing was done to 1 hot encode some of the categorical features, which will be our starting point on the data.\n",
    "\n",
    "###3.   Business Success Criteria\n",
    "\n",
    "> For our classification models, we will consider our model successful if we can classify the presence of Whiskey vs Non-Whiskey in the sales tranaction with an accuracy of 89% or higher.  We will use cross-validaton using a split of 80% of the data for training and 20% of the data for testing to evaulate the performance of our classifiers.  We will build 2 different classification models - 1) using logistic regression, and 2) support vector machines (SVMs).  Since our data set is so large, we will also optimize both classification models using stochastic gradient descent using the linear kernel.  This will help train our models in a reasonable timeframe\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsKFLvq2j5su"
   },
   "source": [
    "## Data Understanding\n",
    "\n",
    "The initial data set was 4.63GB with 17.7 million rows.  We subsetted that down to 400k rows using a random set of data from the 2019 sales data.  This work was done in our initial processing from earlier.\n",
    "\n",
    "So taking our initial processing that was done from previous work, we further refined the 400k dataset by one-hot encoding the categorical features for sales month, liquor categories, and the stores.  The stores will not be used in this analysis; however, it was still one-hot encoded for future analysis\n",
    "\n",
    "It's also worth noting, that we used the log transformed values for sales dollars, cost per liter, state bottle cost and volume sold in liters along with the liquor catogory to focus on our classification\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_WkOv5ll__p"
   },
   "source": [
    "## Data Preparation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages for complete analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6I6a7elv4W0h"
   },
   "source": [
    "### Load Dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was produced from the Iowa Alcohol Dataset 2019 year data.  The total data set size has over 17 million rows.  We took\n",
    "# a subet of that data, 400,000 rows\n",
    "\n",
    "# The 400k rows were cleansed and transformed lab1 and exported as a csv to github for use here.\n",
    "# source python notebook: https://github.com/jjschueder/7331DataMiningNotebooks/blob/master/lab1/msds7331_clark_schueder_vela_washburn.ipynb\n",
    "\n",
    "# Further cleansing was done to 1 hot encode some of the categorical features, which is what we are reading in here.  Still using 400k rows of data\n",
    "\n",
    "# read csv from github directly\n",
    "url_dataset = 'https://raw.githubusercontent.com/jjschueder/7331DataMiningNotebooks/master/Live%20Assignments/df1hotmerge2.csv'\n",
    "data = pd.read_csv(url_dataset) #full dataset of 400k rows\n",
    "#data = pd.read_csv(url_dataset, nrows=100000) #smaller subset of data of nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cx4qDu_gnml3"
   },
   "source": [
    "###Columns and Descriptors"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 400000 entries, 0 to 399999\nData columns (total 68 columns):\nUnnamed: 0                  400000 non-null int64\npack                        400000 non-null int64\nbottle_volume_ml            400000 non-null int64\nstate_bottle_cost           400000 non-null float64\nstate_bottle_retail         400000 non-null float64\nbottles_sold                400000 non-null int64\nsale_dollars                400000 non-null float64\nvolume_sold_liters          400000 non-null float64\nvolume_sold_gallons         400000 non-null float64\ncounter                     400000 non-null int64\nliquor_category             400000 non-null object\nstore_parent                400000 non-null object\nmonth                       400000 non-null object\nyear                        400000 non-null int64\nmonthyear                   400000 non-null object\nliquor_category_AMARETTO    400000 non-null int64\nliquor_category_BRANDY      400000 non-null int64\nliquor_category_GIN         400000 non-null int64\nliquor_category_LIQUEUR     400000 non-null int64\nliquor_category_Other       400000 non-null int64\nliquor_category_RUM         400000 non-null int64\nliquor_category_SCHNAPPS    400000 non-null int64\nliquor_category_TEQUILA     400000 non-null int64\nliquor_category_VODKA       400000 non-null int64\nliquor_category_WHISKY      400000 non-null int64\nstore_parent_CVS            400000 non-null int64\nstore_parent_Caseys         400000 non-null int64\nstore_parent_Hy-Vee         400000 non-null int64\nstore_parent_Kum&Go         400000 non-null int64\nstore_parent_Other          400000 non-null int64\nstore_parent_QuikTrip       400000 non-null int64\nstore_parent_SamsClub       400000 non-null int64\nstore_parent_SmokingJoes    400000 non-null int64\nstore_parent_Target         400000 non-null int64\nstore_parent_Wal-Mart       400000 non-null int64\nstore_parent_Walgreens      400000 non-null int64\nmonth_Apr                   400000 non-null int64\nmonth_Aug                   400000 non-null int64\nmonth_Dec                   400000 non-null int64\nmonth_Feb                   400000 non-null int64\nmonth_Jan                   400000 non-null int64\nmonth_Jul                   400000 non-null int64\nmonth_Jun                   400000 non-null int64\nmonth_Mar                   400000 non-null int64\nmonth_May                   400000 non-null int64\nmonth_Nov                   400000 non-null int64\nmonth_Oct                   400000 non-null int64\nmonth_Sep                   400000 non-null int64\nyear_2019                   400000 non-null int64\nmonthyear_Apr-2019          400000 non-null int64\nmonthyear_Aug-2019          400000 non-null int64\nmonthyear_Dec-2019          400000 non-null int64\nmonthyear_Feb-2019          400000 non-null int64\nmonthyear_Jan-2019          400000 non-null int64\nmonthyear_Jul-2019          400000 non-null int64\nmonthyear_Jun-2019          400000 non-null int64\nmonthyear_Mar-2019          400000 non-null int64\nmonthyear_May-2019          400000 non-null int64\nmonthyear_Nov-2019          400000 non-null int64\nmonthyear_Oct-2019          400000 non-null int64\nmonthyear_Sep-2019          400000 non-null int64\nsale_dollars_trans          400000 non-null float64\ncost_per_liter              400000 non-null float64\ncost_per_liter_trans        400000 non-null float64\nstate_bottle_cost_trans     400000 non-null float64\nbottles_sold_trans          400000 non-null float64\nvolume_sold_liters_trans    400000 non-null float64\ngrossmargin                 400000 non-null float64\ndtypes: float64(12), int64(52), object(4)\nmemory usage: 207.5+ MB\n"
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(400000, 68)\n['Unnamed: 0', 'pack', 'bottle_volume_ml', 'state_bottle_cost', 'state_bottle_retail', 'bottles_sold', 'sale_dollars', 'volume_sold_liters', 'volume_sold_gallons', 'counter', 'liquor_category', 'store_parent', 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'liquor_category_WHISKY', 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb', 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', 'year_2019', 'monthyear_Apr-2019', 'monthyear_Aug-2019', 'monthyear_Dec-2019', 'monthyear_Feb-2019', 'monthyear_Jan-2019', 'monthyear_Jul-2019', 'monthyear_Jun-2019', 'monthyear_Mar-2019', 'monthyear_May-2019', 'monthyear_Nov-2019', 'monthyear_Oct-2019', 'monthyear_Sep-2019', 'sale_dollars_trans', 'cost_per_liter', 'cost_per_liter_trans', 'state_bottle_cost_trans', 'bottles_sold_trans', 'volume_sold_liters_trans', 'grossmargin']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        Unnamed: 0  pack  bottle_volume_ml  state_bottle_cost  \\\n0                0    20               375               3.85   \n1                1     8                50               8.75   \n2                2    12              1000              16.50   \n3                3     6               750              21.17   \n4                4     6              1750               9.31   \n...            ...   ...               ...                ...   \n399995      399995    12               750               7.49   \n399996      399996    12               750               7.49   \n399997      399997    12               750               7.49   \n399998      399998    12               750               7.49   \n399999      399999    12               750               7.49   \n\n        state_bottle_retail  bottles_sold  sale_dollars  volume_sold_liters  \\\n0                      5.78            20        115.60                7.50   \n1                     13.13             1         13.13                0.05   \n2                     24.75             6        148.50                6.00   \n3                     31.76            24        762.24               18.00   \n4                     13.97            12        167.64               21.00   \n...                     ...           ...           ...                 ...   \n399995                11.24            12        134.88                9.00   \n399996                11.24            12        134.88                9.00   \n399997                11.24            12        134.88                9.00   \n399998                11.24            12        134.88                9.00   \n399999                11.24            12        134.88                9.00   \n\n        volume_sold_gallons  counter  ... monthyear_Nov-2019  \\\n0                      1.98        1  ...                  1   \n1                      0.01        1  ...                  1   \n2                      1.58        1  ...                  0   \n3                      4.75        1  ...                  1   \n4                      5.54        1  ...                  1   \n...                     ...      ...  ...                ...   \n399995                 2.37        1  ...                  0   \n399996                 2.37        1  ...                  0   \n399997                 2.37        1  ...                  0   \n399998                 2.37        1  ...                  0   \n399999                 2.37        1  ...                  1   \n\n       monthyear_Oct-2019 monthyear_Sep-2019  sale_dollars_trans  \\\n0                       0                  0            4.750136   \n1                       0                  0            2.574900   \n2                       0                  0            5.000585   \n3                       0                  0            6.636261   \n4                       0                  0            5.121819   \n...                   ...                ...                 ...   \n399995                  0                  0            4.904385   \n399996                  0                  0            4.904385   \n399997                  0                  0            4.904385   \n399998                  0                  0            4.904385   \n399999                  0                  0            4.904385   \n\n       cost_per_liter  cost_per_liter_trans  state_bottle_cost_trans  \\\n0           15.413333              2.735233                 1.348073   \n1          262.600000              5.570632                 2.169054   \n2           24.750000              3.208825                 2.803360   \n3           42.346667              3.745890                 3.052585   \n4            7.982857              2.077296                 2.231089   \n...               ...                   ...                      ...   \n399995      14.986667              2.707161                 2.013569   \n399996      14.986667              2.707161                 2.013569   \n399997      14.986667              2.707161                 2.013569   \n399998      14.986667              2.707161                 2.013569   \n399999      14.986667              2.707161                 2.013569   \n\n        bottles_sold_trans  volume_sold_liters_trans  grossmargin  \n0                 2.995732                  2.014903     0.333910  \n1                 0.000000                 -2.995732     0.333587  \n2                 1.791759                  1.791759     0.333333  \n3                 3.178054                  2.890372     0.333438  \n4                 2.484907                  3.044522     0.333572  \n...                    ...                       ...          ...  \n399995            2.484907                  2.197225     0.333630  \n399996            2.484907                  2.197225     0.333630  \n399997            2.484907                  2.197225     0.333630  \n399998            2.484907                  2.197225     0.333630  \n399999            2.484907                  2.197225     0.333630  \n\n[400000 rows x 68 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>pack</th>\n      <th>bottle_volume_ml</th>\n      <th>state_bottle_cost</th>\n      <th>state_bottle_retail</th>\n      <th>bottles_sold</th>\n      <th>sale_dollars</th>\n      <th>volume_sold_liters</th>\n      <th>volume_sold_gallons</th>\n      <th>counter</th>\n      <th>...</th>\n      <th>monthyear_Nov-2019</th>\n      <th>monthyear_Oct-2019</th>\n      <th>monthyear_Sep-2019</th>\n      <th>sale_dollars_trans</th>\n      <th>cost_per_liter</th>\n      <th>cost_per_liter_trans</th>\n      <th>state_bottle_cost_trans</th>\n      <th>bottles_sold_trans</th>\n      <th>volume_sold_liters_trans</th>\n      <th>grossmargin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>375</td>\n      <td>3.85</td>\n      <td>5.78</td>\n      <td>20</td>\n      <td>115.60</td>\n      <td>7.50</td>\n      <td>1.98</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.750136</td>\n      <td>15.413333</td>\n      <td>2.735233</td>\n      <td>1.348073</td>\n      <td>2.995732</td>\n      <td>2.014903</td>\n      <td>0.333910</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>50</td>\n      <td>8.75</td>\n      <td>13.13</td>\n      <td>1</td>\n      <td>13.13</td>\n      <td>0.05</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.574900</td>\n      <td>262.600000</td>\n      <td>5.570632</td>\n      <td>2.169054</td>\n      <td>0.000000</td>\n      <td>-2.995732</td>\n      <td>0.333587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>12</td>\n      <td>1000</td>\n      <td>16.50</td>\n      <td>24.75</td>\n      <td>6</td>\n      <td>148.50</td>\n      <td>6.00</td>\n      <td>1.58</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.000585</td>\n      <td>24.750000</td>\n      <td>3.208825</td>\n      <td>2.803360</td>\n      <td>1.791759</td>\n      <td>1.791759</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>6</td>\n      <td>750</td>\n      <td>21.17</td>\n      <td>31.76</td>\n      <td>24</td>\n      <td>762.24</td>\n      <td>18.00</td>\n      <td>4.75</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6.636261</td>\n      <td>42.346667</td>\n      <td>3.745890</td>\n      <td>3.052585</td>\n      <td>3.178054</td>\n      <td>2.890372</td>\n      <td>0.333438</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>1750</td>\n      <td>9.31</td>\n      <td>13.97</td>\n      <td>12</td>\n      <td>167.64</td>\n      <td>21.00</td>\n      <td>5.54</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.121819</td>\n      <td>7.982857</td>\n      <td>2.077296</td>\n      <td>2.231089</td>\n      <td>2.484907</td>\n      <td>3.044522</td>\n      <td>0.333572</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>399995</td>\n      <td>399995</td>\n      <td>12</td>\n      <td>750</td>\n      <td>7.49</td>\n      <td>11.24</td>\n      <td>12</td>\n      <td>134.88</td>\n      <td>9.00</td>\n      <td>2.37</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.904385</td>\n      <td>14.986667</td>\n      <td>2.707161</td>\n      <td>2.013569</td>\n      <td>2.484907</td>\n      <td>2.197225</td>\n      <td>0.333630</td>\n    </tr>\n    <tr>\n      <td>399996</td>\n      <td>399996</td>\n      <td>12</td>\n      <td>750</td>\n      <td>7.49</td>\n      <td>11.24</td>\n      <td>12</td>\n      <td>134.88</td>\n      <td>9.00</td>\n      <td>2.37</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.904385</td>\n      <td>14.986667</td>\n      <td>2.707161</td>\n      <td>2.013569</td>\n      <td>2.484907</td>\n      <td>2.197225</td>\n      <td>0.333630</td>\n    </tr>\n    <tr>\n      <td>399997</td>\n      <td>399997</td>\n      <td>12</td>\n      <td>750</td>\n      <td>7.49</td>\n      <td>11.24</td>\n      <td>12</td>\n      <td>134.88</td>\n      <td>9.00</td>\n      <td>2.37</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.904385</td>\n      <td>14.986667</td>\n      <td>2.707161</td>\n      <td>2.013569</td>\n      <td>2.484907</td>\n      <td>2.197225</td>\n      <td>0.333630</td>\n    </tr>\n    <tr>\n      <td>399998</td>\n      <td>399998</td>\n      <td>12</td>\n      <td>750</td>\n      <td>7.49</td>\n      <td>11.24</td>\n      <td>12</td>\n      <td>134.88</td>\n      <td>9.00</td>\n      <td>2.37</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.904385</td>\n      <td>14.986667</td>\n      <td>2.707161</td>\n      <td>2.013569</td>\n      <td>2.484907</td>\n      <td>2.197225</td>\n      <td>0.333630</td>\n    </tr>\n    <tr>\n      <td>399999</td>\n      <td>399999</td>\n      <td>12</td>\n      <td>750</td>\n      <td>7.49</td>\n      <td>11.24</td>\n      <td>12</td>\n      <td>134.88</td>\n      <td>9.00</td>\n      <td>2.37</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.904385</td>\n      <td>14.986667</td>\n      <td>2.707161</td>\n      <td>2.013569</td>\n      <td>2.484907</td>\n      <td>2.197225</td>\n      <td>0.333630</td>\n    </tr>\n  </tbody>\n</table>\n<p>400000 rows × 68 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Validate dataset\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeJ06hpUqLJW"
   },
   "source": [
    "###Remove unwanted columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['sale_dollars_trans', 'cost_per_liter_trans',\n       'state_bottle_cost_trans', 'bottles_sold_trans',\n       'volume_sold_liters_trans', 'pack', 'bottle_volume_ml',\n       'liquor_category_WHISKY'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# Remove unwanted columns, which include all the specific liquor categories, \n",
    "# except for liquor_category_WHISKY since that is what we want to classify on, along\n",
    "# with all the store_ attributes\n",
    "\"\"\"\n",
    "cat_vars=['counter', 'liquor_category', 'store_parent',\n",
    " 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', \n",
    " 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', \n",
    " 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb',\n",
    " 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', \n",
    " 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', \n",
    " 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', \n",
    " 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens']\n",
    "data_vars=data.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in cat_vars]\n",
    "\"\"\"\n",
    "to_keep=['sale_dollars_trans', 'cost_per_liter_trans',\n",
    "      'state_bottle_cost_trans', 'bottles_sold_trans',\n",
    "       'volume_sold_liters_trans','pack', 'bottle_volume_ml',\n",
    "       'liquor_category_WHISKY']\n",
    "data_final=data[to_keep]\n",
    "data_final.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIlHFqU5mjwq"
   },
   "source": [
    "###Simple Statistics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       sale_dollars_trans  cost_per_liter_trans  state_bottle_cost_trans  \\\ncount       400000.000000         400000.000000            400000.000000   \nmean             4.342939              2.766402                 2.090235   \nstd              1.061412              0.828823                 0.707591   \nmin              0.292670              0.916291                -0.116534   \n25%              3.727619              2.090805                 1.690096   \n50%              4.319087              2.736221                 2.111425   \n75%              5.030046              3.097687                 2.539237   \nmax             10.823499              7.495542                 5.926899   \n\n       bottles_sold_trans  volume_sold_liters_trans           pack  \\\ncount       400000.000000             400000.000000  400000.000000   \nmean             1.847176                  1.576537      11.950140   \nstd              1.078662                  1.352197       8.382596   \nmin              0.000000                 -3.912023       1.000000   \n25%              1.098612                  0.810930       6.000000   \n50%              1.791759                  2.197225      12.000000   \n75%              2.484907                  2.351375      12.000000   \nmax              8.237479                  8.797095      48.000000   \n\n       bottle_volume_ml  liquor_category_WHISKY  \ncount     400000.000000           400000.000000  \nmean         960.109795                0.104387  \nstd          557.811485                0.305763  \nmin           25.000000                0.000000  \n25%          750.000000                0.000000  \n50%          750.000000                0.000000  \n75%         1750.000000                0.000000  \nmax         6000.000000                1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sale_dollars_trans</th>\n      <th>cost_per_liter_trans</th>\n      <th>state_bottle_cost_trans</th>\n      <th>bottles_sold_trans</th>\n      <th>volume_sold_liters_trans</th>\n      <th>pack</th>\n      <th>bottle_volume_ml</th>\n      <th>liquor_category_WHISKY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n      <td>400000.000000</td>\n    </tr>\n    <tr>\n      <td>mean</td>\n      <td>4.342939</td>\n      <td>2.766402</td>\n      <td>2.090235</td>\n      <td>1.847176</td>\n      <td>1.576537</td>\n      <td>11.950140</td>\n      <td>960.109795</td>\n      <td>0.104387</td>\n    </tr>\n    <tr>\n      <td>std</td>\n      <td>1.061412</td>\n      <td>0.828823</td>\n      <td>0.707591</td>\n      <td>1.078662</td>\n      <td>1.352197</td>\n      <td>8.382596</td>\n      <td>557.811485</td>\n      <td>0.305763</td>\n    </tr>\n    <tr>\n      <td>min</td>\n      <td>0.292670</td>\n      <td>0.916291</td>\n      <td>-0.116534</td>\n      <td>0.000000</td>\n      <td>-3.912023</td>\n      <td>1.000000</td>\n      <td>25.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25%</td>\n      <td>3.727619</td>\n      <td>2.090805</td>\n      <td>1.690096</td>\n      <td>1.098612</td>\n      <td>0.810930</td>\n      <td>6.000000</td>\n      <td>750.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50%</td>\n      <td>4.319087</td>\n      <td>2.736221</td>\n      <td>2.111425</td>\n      <td>1.791759</td>\n      <td>2.197225</td>\n      <td>12.000000</td>\n      <td>750.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75%</td>\n      <td>5.030046</td>\n      <td>3.097687</td>\n      <td>2.539237</td>\n      <td>2.484907</td>\n      <td>2.351375</td>\n      <td>12.000000</td>\n      <td>1750.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>max</td>\n      <td>10.823499</td>\n      <td>7.495542</td>\n      <td>5.926899</td>\n      <td>8.237479</td>\n      <td>8.797095</td>\n      <td>48.000000</td>\n      <td>6000.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "data_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<bound method DataFrame.info of         sale_dollars_trans  cost_per_liter_trans  state_bottle_cost_trans  \\\n0                 4.750136              2.735233                 1.348073   \n1                 2.574900              5.570632                 2.169054   \n2                 5.000585              3.208825                 2.803360   \n3                 6.636261              3.745890                 3.052585   \n4                 5.121819              2.077296                 2.231089   \n...                    ...                   ...                      ...   \n399995            4.904385              2.707161                 2.013569   \n399996            4.904385              2.707161                 2.013569   \n399997            4.904385              2.707161                 2.013569   \n399998            4.904385              2.707161                 2.013569   \n399999            4.904385              2.707161                 2.013569   \n\n        bottles_sold_trans  volume_sold_liters_trans  pack  bottle_volume_ml  \\\n0                 2.995732                  2.014903    20               375   \n1                 0.000000                 -2.995732     8                50   \n2                 1.791759                  1.791759    12              1000   \n3                 3.178054                  2.890372     6               750   \n4                 2.484907                  3.044522     6              1750   \n...                    ...                       ...   ...               ...   \n399995            2.484907                  2.197225    12               750   \n399996            2.484907                  2.197225    12               750   \n399997            2.484907                  2.197225    12               750   \n399998            2.484907                  2.197225    12               750   \n399999            2.484907                  2.197225    12               750   \n\n        liquor_category_WHISKY  \n0                            0  \n1                            0  \n2                            0  \n3                            1  \n4                            0  \n...                        ...  \n399995                       0  \n399996                       0  \n399997                       0  \n399998                       0  \n399999                       0  \n\n[400000 rows x 8 columns]>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "data_final.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Percentage of not whiskey is 89.56125\nPercentage of whiskey 10.438749999999999\n"
    }
   ],
   "source": [
    "# From the entire dataset (400k rows) take a look at what our Whiskey vs Non-Whiskey distribution looks like\n",
    "#data = data.drop(columns = ['Unnamed: 0'])\n",
    "count_not_whiskey = len(data_final[data_final['liquor_category_WHISKY']==0])\n",
    "count_whiskey = len(data_final[data_final['liquor_category_WHISKY']==1])\n",
    "pct_of_no_whiskey = count_not_whiskey/(count_not_whiskey+count_whiskey)\n",
    "print(\"Percentage of not whiskey is\", pct_of_no_whiskey*100)\n",
    "pct_of_whiskey = count_whiskey/(count_not_whiskey+count_whiskey)\n",
    "print(\"Percentage of whiskey\", pct_of_whiskey*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw60isQ0rXD0"
   },
   "source": [
    "###Locate Features and Label\n",
    "\n",
    "Our features will be everything that isn't the liquor_category_WHISKY\n",
    "\n",
    "And our label, what we are trying to classify on, will be the liquor_category_WHISKY"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will be our features\n",
    "X = data_final.loc[:, data_final.columns != 'liquor_category_WHISKY']\n",
    "\n",
    "# y will be our label, what we are trying to classify\n",
    "y = data_final.loc[:, data_final.columns == 'liquor_category_WHISKY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sale_dollars_trans  cost_per_liter_trans  state_bottle_cost_trans  \\\n0            4.750136              2.735233                 1.348073   \n1            2.574900              5.570632                 2.169054   \n2            5.000585              3.208825                 2.803360   \n3            6.636261              3.745890                 3.052585   \n4            5.121819              2.077296                 2.231089   \n\n   bottles_sold_trans  volume_sold_liters_trans  pack  bottle_volume_ml  \n0            2.995732                  2.014903    20               375  \n1            0.000000                 -2.995732     8                50  \n2            1.791759                  1.791759    12              1000  \n3            3.178054                  2.890372     6               750  \n4            2.484907                  3.044522     6              1750  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sale_dollars_trans</th>\n      <th>cost_per_liter_trans</th>\n      <th>state_bottle_cost_trans</th>\n      <th>bottles_sold_trans</th>\n      <th>volume_sold_liters_trans</th>\n      <th>pack</th>\n      <th>bottle_volume_ml</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>4.750136</td>\n      <td>2.735233</td>\n      <td>1.348073</td>\n      <td>2.995732</td>\n      <td>2.014903</td>\n      <td>20</td>\n      <td>375</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.574900</td>\n      <td>5.570632</td>\n      <td>2.169054</td>\n      <td>0.000000</td>\n      <td>-2.995732</td>\n      <td>8</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5.000585</td>\n      <td>3.208825</td>\n      <td>2.803360</td>\n      <td>1.791759</td>\n      <td>1.791759</td>\n      <td>12</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.636261</td>\n      <td>3.745890</td>\n      <td>3.052585</td>\n      <td>3.178054</td>\n      <td>2.890372</td>\n      <td>6</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.121819</td>\n      <td>2.077296</td>\n      <td>2.231089</td>\n      <td>2.484907</td>\n      <td>3.044522</td>\n      <td>6</td>\n      <td>1750</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<bound method NDFrame.head of         liquor_category_WHISKY\n0                            0\n1                            0\n2                            0\n3                            1\n4                            0\n...                        ...\n399995                       0\n399996                       0\n399997                       0\n399998                       0\n399999                       0\n\n[400000 rows x 1 columns]>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIfgUT054dBe"
   },
   "source": [
    "## Modeling"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgWC8imwsEj2"
   },
   "source": [
    "###80/20 training/testing split\n",
    "<a id='datasplit'></a> \n",
    "\n",
    "Split our dataset into train/test with 80% of the data used for training and 20% of the data used to test the model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Full dataset shape\nX_train shape:  (320000, 7)\nX_test shape:  (80000, 7)\nY_train shape:  (320000, 1)\nY_test shape:  (80000, 1)\n\nSubset of dataset for SVM models\nX_train_svc shape:  (4000, 7)\nX_test_svc shape:  (396000, 7)\nY_train_svc shape:  (4000, 1)\nY_test_svc shape:  (396000, 1)\n"
    }
   ],
   "source": [
    "# Create the training/testing datasets\n",
    "\n",
    "# This works well for M1, M2, M5, M6, M7 on full dataset to have models train in decent\n",
    "# amount of time (no model takes longer than 3 mins)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# This works well for M3, M4 (SVM models) - need a smaller train size to get model to train\n",
    "# in decent amount of time\n",
    "X_train_svc, X_test_svc, y_train_svc, y_test_svc = train_test_split(X, y, test_size=0.99, random_state=0)\n",
    "\n",
    "columns = X_train.columns\n",
    "\n",
    "print('Full dataset shape')\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('Y_train shape: ', y_train.shape)\n",
    "print('Y_test shape: ', y_test.shape)\n",
    "\n",
    "print('\\nSubset of dataset for SVM models')\n",
    "print('X_train_svc shape: ', X_train_svc.shape)\n",
    "print('X_test_svc shape: ', X_test_svc.shape)\n",
    "print('Y_train_svc shape: ', y_train_svc.shape)\n",
    "print('Y_test_svc shape: ', y_test_svc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pxfgw169seqP"
   },
   "source": [
    "###M1-Logistic Regression Model\n",
    "<a id='m1'></a>\n",
    "Create a logistic regression model with all defaults used to the model, except for max_iter as we were seeing convergence warnings.  The default max_iter setting according to documentation is set to 100, so we increased to 500 and that prevented the warnings from showing up."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\ProgramData\\Anaconda3\\envs\\ML7331\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nCoefficients:  \n [ 3.54135111e-02 -7.05452114e-01  4.31067217e+00 -9.25586024e-01\n  7.40865611e-01  9.82919549e-02 -3.28831168e-03]\n\nClasses:  \n 0\n\nIntercept:  \n -8.272919088744453\n\nParams:  \n {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 500, 'multi_class': 'warn', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'warn', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# seeing message 'lbfgs failed to converge. Increase the number of iterations'\n",
    "# https://stats.stackexchange.com/questions/184017/how-to-fix-non-convergence-in-logisticregressioncv\n",
    "# increased max_iter to 500 \n",
    "#simplelogisticRegr = LogisticRegression()# class_weight = 'balanced')\n",
    "simplelogisticRegr = LogisticRegression(max_iter=500)# class_weight = 'balanced')\n",
    "\n",
    "\n",
    "# fit (train) the model\n",
    "# was seing an error about column-vector y was passed when 1d array expected\n",
    "# fix: https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected\n",
    "#simplelogisticRegr.fit(X_train, y_train)\n",
    "simplelogisticRegr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "coef = simplelogisticRegr.coef_[0]\n",
    "print ('Coefficients: ', '\\n', coef)\n",
    "\n",
    "classes = simplelogisticRegr.classes_[0]\n",
    "print()\n",
    "print ('Classes: ', '\\n', classes)\n",
    "\n",
    "intercept = simplelogisticRegr.intercept_[0]\n",
    "print()\n",
    "print ('Intercept: ', '\\n', intercept)\n",
    "\n",
    "params =simplelogisticRegr.get_params(deep=True)\n",
    "print()\n",
    "print ('Params: ', '\\n', params)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wRKRHAmuFzL"
   },
   "source": [
    "#### M1-Predictions (92% full ds)\n",
    "<a id='m1_predict'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy:\n 0.9189875\n\nConfusion Matrix:\n [[70673  1004]\n [ 5477  2846]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96     71677\n           1       0.74      0.34      0.47      8323\n\n    accuracy                           0.92     80000\n   macro avg       0.83      0.66      0.71     80000\nweighted avg       0.91      0.92      0.91     80000\n\n"
    }
   ],
   "source": [
    "# Do predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = simplelogisticRegr.score(X_test, y_test)\n",
    "print('Accuracy:\\n', score)\n",
    "\n",
    "# Predictions\n",
    "predictions = simplelogisticRegr.predict(X_test)\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Classification Report\n",
    "\n",
    "###M1-Evaluation\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dj-uDeWtElF"
   },
   "source": [
    "####M1-Evaluation\n",
    "We ran a \"simple\" logistic regression model using the default hyperparameters of C=1.0, no weighting of the classes and with a ridge regression penalty. \n",
    "\n",
    "As a result, our model achieved 92% accuracy which is a bit higher than the accuracy we would achieve if we were to guess randomly based on the distribution of whiskeys vs non whiskeys. From here, we can use a grid search procedure to tune our hyperparameters to improve our accuracy score. \n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mC7230pKVoWS"
   },
   "source": [
    "### M2-Logistic Regression Model w/ Gridsearch for Hyperparameter Tuning\n",
    "<a id='m2'></a>\n",
    "\n",
    "Find scalings for each column that make this zero mean and unit std the line of code above only looks at training data to get mean and std and we can use it to transform new feature data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) \n",
    "\n",
    "# apply to training on full dataset\n",
    "X_train_scaled = scl_obj.transform(X_train) \n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "# do scaling on the subset of data used for svm models\n",
    "scl_obj_svc = StandardScaler()\n",
    "scl_obj_svc.fit(X_train_svc) \n",
    "\n",
    "# apply to training on full dataset\n",
    "X_train_svc_scaled = scl_obj_svc.transform(X_train_svc) \n",
    "X_test_svc_scaled = scl_obj_svc.transform(X_test_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fbRn1vBWo6M"
   },
   "source": [
    "Use GridSearch and CrossValidation to find the best logistic regression hyperparameter combination"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logisitic regression 10-fold cross-validation \n",
    "\n",
    "#Divide data into test and training splits\n",
    "# full dataset w/ splits=5, test_size=.5 takes over 5 mins to get to just 196 tasks (out of 600 fits), stopped process, taking too long\n",
    "#cv = ShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "# full dataset w/ splits=2, test_size=.8 takes 3 mins\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.8, random_state=0)\n",
    "\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "\n",
    "parameters = { 'penalty':['l2', 'l1', 'elasticnet']\n",
    "              ,'C': [0.001, 1, 10, 100]\n",
    "              ,'class_weight': ['balanced', 'none']\n",
    "              ,'random_state': [0]\n",
    "              ,'solver': ['lbfgs', 'saga', 'liblinear', 'newton-cg', 'sag']\n",
    "              ,'max_iter':[100]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=-1 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HthK3VD2cfph"
   },
   "source": [
    "Show the best estimators from GridSearch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_52IJqizdEh"
   },
   "source": [
    "After running our grid search, we found that the ideal model uses a C=0.001 which is different from our default model of C=1.0. Otherwise, the other parameters remained the same. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####M2-Predictions (92% full ds)\n",
    "<a id='m2_predict'></a>\n",
    "\n",
    "\n",
    "Let's see how this does with our accuracy score. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(regGridSearch)\n",
    "\n",
    "gridresults = pd.DataFrame(regGridSearch.cv_results_)\n",
    "\n",
    "columns = ['param_solver','param_C', 'param_max_iter', 'class_weight', 'param_penalty', 'mean_test_score', 'rank_test_score']\n",
    "gridresults = pd.DataFrame(gridresults, columns=columns)\n",
    "\n",
    "gridresults.sort_values(by=['rank_test_score'], ascending=True).head(20)\n",
    "\n",
    "y_hat = regGridSearch.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "print('Accuracy:\\n', acc)\n",
    "\n",
    "# Confustion matrix\n",
    "conf = mt.confusion_matrix(y_test, y_hat)\n",
    "print('\\nConfusion Matrix:\\n', conf)\n",
    "\n",
    "# Get predictions and print the classification report\n",
    "predictions2 = regGridSearch.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6ZiT2noqgGS"
   },
   "source": [
    "####M2-Evaluation\n",
    "After running a grid search to uncover the top performing logistic regression hyperparameter combination, we are seeing that the accuracy score is close to our simple logistic regression model, however, it did come in close to the same. \n",
    "\n",
    "The Simple Logistic Regression model was the winner with a C=1 which is less regularization than the grid search that called for a stronger regularization at C = 0.001. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "ROC Curves and AUC in Python\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "Since our accuracy was better with our logistic regression model, we will use this in building an ROC curve to show how our model is performing. \n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_probs = simplelogisticRegr.predict_proba(X_test_scaled)\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XsPBghgPsugq"
   },
   "source": [
    "Looking at our true positive rate compared to false-positive rate, with the diagonal line representing a 50% false-positive rate, we can see that our model is producing more True positives than false-positives.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Running a Precision/ Recall line plot"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywhiskcnt = y_test.apply(pd.value_counts)\n",
    "ycnt = ywhiskcnt.loc[1:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_probs = simplelogisticRegr.predict_proba(X_test_scaled)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = regGridSearch.predict(X_test_scaled)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "ywhiskcnt = y_test.apply(pd.value_counts)\n",
    "ycnt = ywhiskcnt.loc[1:1, :]\n",
    "no_skill = ycnt.values[0] / len(y_test)\n",
    "#no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "no_skill\n",
    "#no_skill = 0.2\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhJSZKDgtn08"
   },
   "source": [
    "From the precision/ recall line plot above, we can see that our model tends to lose precision as we get a higher recall, with the bottoming out of 0.5 precision and 0.2 being the point of spiking the performance. \n",
    "\n",
    "So essentially, it is saying that our model is better at predicting the correct line items that were whiskeys than correctly discerning between whiskeys and non whiskeys. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dC7CCKAznSkt"
   },
   "source": [
    "### M3-SVM Model\n",
    "<a id='m3'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#possibility to scale it?\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "X_train_svc_scaled \n",
    "X_test_svc_scaled \n",
    "#X_train = preprocessing.scale(X_train_svc)\n",
    "#X_test = preprocessing.scale(X_test_svc)\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "#https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "from sklearn.svm import SVC\n",
    "#svclassifier = SVC(kernel='poly', degree=8)\n",
    "#svclassifier = SVC(kernel='rbf')\n",
    "#svclassifier = SVC(kernel='linear', C = 1000, random_state=0)\n",
    "#svclassifier = LinearSVC(C=100)\n",
    "\n",
    "# still getting ConvergenceWarning w/ max_iter=100000 w/ full dataset\n",
    "# C=100, max_iter=100000 taking over up to an hour to run on full dataset\n",
    "# In the 80/20 training/testing split, use the smaller train set of only 1% and max_iter=10000\n",
    "# takes less than 2 mins to complete.  Keep seeing convergance warnings on values less than 100000\n",
    "# with more than 4000 rows of training data\n",
    "svclassifier = LinearSVC(C=100, max_iter=100000) \n",
    "\n",
    "# Fit (train) the model\n",
    "#model = svclassifier.fit(X_train_svc_scaled,  y_train_svc)\n",
    "model = svclassifier.fit(X_train_svc_scaled,  y_train_svc.values.ravel())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVTedd2GpLtU"
   },
   "source": [
    "####M3-Predictions (92% sub ds)\n",
    "<a id='m3_predict'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a NumPy Array\n",
    "predictions_svc = svclassifier.predict(X_test_svc_scaled)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "acc = mt.accuracy_score(y_test_svc,predictions_svc)\n",
    "print(\"Accuracy:\\n\", acc )\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test_svc, predictions_svc))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test_svc, predictions_svc))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRO9IorFD6A6"
   },
   "source": [
    "####M3-Evaluation\n",
    "For our Support Vector Machine, we were able to derive an 92% Accuracy with a precision of 92% and Recall of 99%. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now, will look at the weights to determine feature importance\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svclassifier.coef_)\n",
    "weights = pd.Series(svclassifier.coef_[0],index=X.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EsEVp7IFH1Q"
   },
   "source": [
    "Looking at the key values associated with driving the prediction classifier of whiskey vs non whiskey, we can see that the state bottle cost and bottles sold are the key factor in driving performance, followed by the sale amount of the transation which has a negative relationship to whether or not a sale was a whiskey."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wF80UyFy1DN"
   },
   "source": [
    "###M4-SVM Model w/ Hyperparameter Tuning\n",
    "<a id='m4'></a>\n",
    "\n",
    "Create a Support Vector Machine model.  Find scalings for each column that make this zero mean and standard deviation.   Will scale at the training data to get mean and standard deviation and can use it to transform new feature data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train_svc) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "# apply to training\n",
    "X_train_svc_scaled = scl_obj.transform(X_train_svc) \n",
    "X_test_svc_scaled = scl_obj.transform(X_test_svc)\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "#model3 = svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "model3 = svm_clf.fit(X_train_svc_scaled, y_train_svc.values.ravel()) \n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfoS-ClrP2bv"
   },
   "source": [
    "####M4-Predictions (92% sub ds)\n",
    "<a id='m4_predict'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set precitions\n",
    "y_hat_svc = svm_clf.predict(X_test_svc_scaled) \n",
    "\n",
    "acc_svc = mt.accuracy_score(y_test_svc,y_hat_svc)\n",
    "conf_svc = mt.confusion_matrix(y_test_svc,y_hat_svc)\n",
    "print('Accuracy:\\n', acc_svc )\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(conf_svc)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test_svc,y_hat_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWHeJoWBEkvw"
   },
   "source": [
    "####M4-Evaluation\n",
    "With SVM Model with Hyperparameter Tuning, we attain the highest accuracy score with over 92%, with the highest precision, recall and F1 Score achieved with our model. The SVM CLF will be the model we chose in our interpretation. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGMJSwdCra0Z"
   },
   "source": [
    "So the analysis here is basically telling us what the original statistics of the data looked like, and also what the statistics of the support vectors looked like. We can see that the separation in distributions is not as great as the separation for the original data. This is because the support vectors tend to be instances on the edge of the class boundaries and also instances that are classified incorrectly in the training data.\n",
    "\n",
    "You can also look at joint plots of the data and see how relationships have changed."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yybaX_KjymeS"
   },
   "source": [
    "### M5-Logistic Regression w/ SGD Optimizer \n",
    "<a id='m5'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# now divide the data into test and train using scikit learn built-ins\n",
    "# n_splits=2, test_size=0.8, random_state=0 takes only a few seconds to run\n",
    "cv = StratifiedShuffleSplit(n_splits=2, test_size=0.8, random_state=0)\n",
    "regularize_const = 0.01\n",
    "iterations = 5\n",
    "\n",
    "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
    "log_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNIK_794dr1n"
   },
   "source": [
    "####M5-Predictions (91% full ds)\n",
    "<a id='m5_predict'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "  log_sgd.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx].values.ravel())\n",
    "  yhat2 = log_sgd.predict(scl.transform(X.iloc[test_idx]))\n",
    "    \n",
    "  conf = mt.confusion_matrix(y.iloc[test_idx],yhat2)\n",
    "  acc = mt.accuracy_score(y.iloc[test_idx],yhat2)\n",
    "\n",
    "\n",
    "print('\\nAccuracy:\\n', acc)\n",
    "print('\\nConfusion Matrix:\\n', conf)\n",
    "print('\\nClassification Report:\\n', classification_report(y.iloc[test_idx], yhat2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MRoMK7RG1fk"
   },
   "source": [
    "####M5-Evaluation\n",
    "Of all our models, the SGD \"Simple\" model (M5) had the worst accuracy score of 90%. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2q8d2SfnCnQ"
   },
   "source": [
    "###M6-Logistic Regression w/ SGD Optimizer and ParameterGrid for Hyperparameter Tuning (M6)\n",
    "<a id='m6'></a>\n",
    "\n",
    "With this parameterization tuning set, we will run a hyperparameter fitting model called Parfit."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Install parfit package \n",
    "\n",
    "# The ! is if you're in colab\n",
    "!pip install parfit\n",
    "\n",
    "# if not running in colab\n",
    "#pip install parfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import parfit.parfit as pf\n",
    "\n",
    "# takes less than 2 mins to complete\n",
    "grid = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n",
    "   # 'n_iter': [1000], # number of epochs\n",
    "    'loss': ['log'], # logistic regression,\n",
    "    'penalty': ['l2'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "paramGrid = ParameterGrid(grid)\n",
    "\n",
    "bestModel = pf.bestFit(SGDClassifier, paramGrid,\n",
    "           X_train, y_train, X_test, y_test, \n",
    "           metric = roc_auc_score, \n",
    "           scoreLabel = \"AUC\")\n",
    "\n",
    "print(bestModel)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6FjXsAg7X1N"
   },
   "source": [
    "The parameter fit helped us derive a custom model for the SGD model which gave us an alpha of 0.001, with a penalty of L2 which is a ridge regression. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7thJ6lG7dA0o"
   },
   "source": [
    "####M6-Predictions (92% full ds)\n",
    "<a id='m6_predict'></a>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
    "log_sgd2 = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "model2 = log_sgd2.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx])\n",
    "\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "  log_sgd2.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx])\n",
    "  yhat = log_sgd2.predict(scl.transform(X.iloc[test_idx]))\n",
    "    \n",
    "  conf = mt.confusion_matrix(y.iloc[test_idx],yhat)\n",
    "  acc = mt.accuracy_score(y.iloc[test_idx],yhat)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\\n\", acc )\n",
    "print(\"\\nConfusion matrix:\", conf)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y.iloc[test_idx], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3lgvVqN0BXO"
   },
   "source": [
    "####M6-Evaluation\n",
    "After installing and running the Hyperparameter Optimization using parfit (https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4), we can see that we improved upon our original Stochastic Gradient Descent Model with an accuracy score of 91.64%\n",
    "\n",
    "This SGD and the Tuned SVM model will be the ones we use moving forward. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtIYmTob6w0E"
   },
   "source": [
    "###M7-SVM w/ SGD Optimizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Divide data into test and training splits\n",
    "# n_splits=2, test_size=0.8, random_state=0 takes less than a min\n",
    "cv = StratifiedShuffleSplit(n_splits=2, test_size=0.8, random_state=0) \n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "regularize_const = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# use some compact notation for creating a linear SVM classifier with stochastic gradient descent\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCmYin_H_Iit"
   },
   "source": [
    "####M7-Predictions (90% full ds)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "  svm_sgd.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx].values.ravel())\n",
    "  yhat = svm_sgd.predict(scl.transform(X.iloc[test_idx]))\n",
    "    \n",
    "  conf = mt.confusion_matrix(y.iloc[test_idx],yhat)\n",
    "  acc = mt.accuracy_score(y.iloc[test_idx],yhat)\n",
    "\n",
    "print(\"Accuracy:\\n\", acc )\n",
    "print(\"\\nConfusion matrix:\\n\", conf)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y.iloc[test_idx], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nw-SuYBc_ML3"
   },
   "source": [
    "####M7-Evaluation\n",
    "\n",
    "Using SVM model with the Stochastic Gradient Descent with an accuracy of only 90%, which is a bit lower than what we were seeing with our other SVM models (M3 and M4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqIG1rKcr0E-"
   },
   "source": [
    "## Evaluation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDyKKT0mnSmE"
   },
   "source": [
    "### Model Advantages\n",
    "\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail. \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKcSMao0r_E_"
   },
   "source": [
    "#### Logistic Regression Advantage\n",
    "\n",
    "* Advantages - There is less transformation needed in order to make the model return in a reasonable amount of time. Scaler transformation will increase the accuracy, but is not required for peroformance as the model performs well at scale regardless of whether the data is normalized or not. \n",
    "* Performance - There are many different setting that allow the model to have the best possible accuracy. These methods can be iterated with a grid search method. \n",
    "*  Training Time efficiency is very good at large scales and does not suffer as volume of data increases."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFV9BCRosDzl"
   },
   "source": [
    "#### Support Vector Machine Advantage\n",
    "\n",
    "* Advantages - From Tan book Chapter 5 - A classification technique that has received considerable attention is support vector machine (SVM). This technique has its roots in statistical learning theory and has shown promising empirical results in many practical applications, from handwritten digit recognition to text categorization. SVM also works very well with high-dimensional data and avoids the curse of dimensionality problem. Another unique aspect of this approach is that it represents the decision boundary using a subset of the training examples, known as the support vectors.  \n",
    " However, in order to get high accuracy scaling and high penalties must be set to achieve accuracy with unbalanced data. \n",
    "* Performance - Changing the cost to value 1000, after previously using 1, 10, and 100 the performance of prediciton accuracy improves over that of logistic regession.  In order to get a result in a similiar amount of time as logistic regression accuracy is sacrficed. \n",
    "* Training Time\n",
    "    * Slow with large data volume and features\n",
    "    * Transformation of data helps\n",
    "    * Very slow with high cost."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2h_M_QSsGkT"
   },
   "source": [
    "#### Stochastic Gradiant Descent Advantage\n",
    "\n",
    "* Advantages - The design of Stochastic Gradient Smith is set up that it only considers a single randonm point in the stochastic array, and changes weights to classify values. The advantage of running stochastic gradient descent is that it runs much quicker than a gradient descent model, which can take hours / days to run. \n",
    "* Performance - While a single stochastic gradient descent model didn't perform better than any of the previous models we ran, we were able to install and run a Parameter Fit, which acted as a grid search to help us choose the optimal hyperparameters. Doing this improved our SGD performace to 83.5% accuracy. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWPhc_j6sKeR"
   },
   "source": [
    "#### Model Advantages Conclusion\n",
    "\n",
    "The logistic regression provides a way to quickly get to a model, though has less accuracy when compared to SVM.  SVM is very accurate while its performance degrades very quickly with large volumes of data with rows and also with a large number of features. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VX-oO4qOnSmF"
   },
   "source": [
    "###  Feature Importance\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important? \n",
    "\n",
    "The model seems to point out that cost is the largest differentiator of the available characteristics. As you will see within the table and charts below, the “state_bottle_cost_trans” has the largest positive coefficient. This output seems reasonable since Whiskey is typically more expensive than other liquors, thus helping our model predict whether the type of liquor is Whiskey vs Non-Whiskey.\n",
    "\n",
    "Furthermore, the model is displaying a large negative coefficient for “bottles_sold_trans” & “cost_per_liter_trans”. In other words, the model is anticipating that as these features increase the more likely that the transaction will be associated to a Non-Whiskey purchase. This output also makes sense as well, since within the previous paragraph, we mentioned that Whiskey is typically more expensive, so because of this we would expect a customer to purchase fewer bottles.\n",
    "\n",
    "Lastly, the “volume_sold_liters_trans” is the last important feature that materially affects the model. With that said, as the volume sold in liters increases than most likely the purchase is associated to Whiskey.\n",
    "\n",
    "Note, the remaining unmentioned features such as bottle sizes are fairly standard across the different types of liquors, thus it did not improve the predictive capabilities enough to highlight accordingly\n",
    "\n",
    "Feedback - model simplelogitic\n",
    "\n",
    "30 points Feature importance\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task in detail\n",
    "Fulfilled in Evaluation-->Feature Importance\n",
    "Why do we think some variables are more important than others\n",
    "Fulfilled in Evaluation-->Feature Importance\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "# iterate over the coefficients\n",
    "weights2 = simplelogisticRegr.coef_.T # take transpose to make a column vector\n",
    "variable_names = X.columns\n",
    "for coef, name in zip(weights2,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(simplelogisticRegr.coef_[0],index=X.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVHNEFrTnSmG"
   },
   "source": [
    "### Support Vector Analysis\n",
    "Look at the chosen support vectors for the classification task. Do these provide\n",
    "any insight into the data? Explain. If you used stochastic gradient descent (and therefore did\n",
    "not explicitly solve for support vectors), try subsampling your data to train the SVC model—\n",
    "then analyze the support vectors from the subsampled dataset. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view support vectors\n",
    "svm_clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View indicies of support vectors\n",
    "svm_clf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxAYjH4AWOmK"
   },
   "source": [
    "This chooses all the misclassified items as a support vectors."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view number of support vectors for each class\n",
    "svm_clf.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4XKEUr0WVPU"
   },
   "source": [
    "We used 12,000 instances of Whiskey and Nonwhiskey to build our decision boundary."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(model.coef_)\n",
    "weights3 = pd.Series(model.coef_[0],index=X_train.columns)\n",
    "weights3.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNo5j26LKCD_"
   },
   "source": [
    "Using our SVC model weights, we can see some consistency with our earlier analysis by ssing that state bottle cost and bottles solde are the strongest vectors in driving the decision boundaries for whiskies and non whiskies. From a negative perspective, the sale dollars and cost per liter are showing a negative relationship here. \n",
    "\n",
    "Next we are going to make a stratified shuffle split by plotting our SVC models."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'liquor_category_WHISKY' in data_final:\n",
    "    y = data_final['liquor_category_WHISKY'].values # get the labels we want\n",
    "    del data_final['liquor_category_WHISKY'] # get rid of the class label\n",
    "    X = data_final.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations, \n",
    "                            test_size = 0.20, train_size = 0.80, random_state=1)\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # for the SVM models \n",
    "    X_train_svc = X[train_indices]\n",
    "    y_train_svc = y[train_indices]\n",
    "    \n",
    "    X_test_svc = X[test_indices]\n",
    "    y_test_svc = y[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvfo-WryKiDE"
   },
   "source": [
    "Now that we established our new training and testing indicies, let's run it against our top performing SVC CLF model (the one that got 87% accuracy)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = data_final.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:].copy()\n",
    "#df_support['liquor_category_WHISKY'].head\n",
    "data_final2 = data_final.copy()\n",
    "df_support.loc[:,'liquor_category_WHISKY'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "data_final2.loc[:,'liquor_category_WHISKY'] = y # also add it back in for the original data\n",
    "print(df_support.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdLxZ2S1K2i2"
   },
   "source": [
    "Doing this granted us 738 non null values within what became our testing set. Using this, we will plot out the original vs the svm distribution of our state bottle cost trans, bottles sold trans, bottle volume ml and sale dollars trans, which was shown to be our strongest performing variables indicative of Whiskeys vs Non Whiskeys."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['liquor_category_WHISKY'])\n",
    "df_grouped = data_final2.groupby(['liquor_category_WHISKY'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['state_bottle_cost_trans','bottles_sold_trans','bottle_volume_ml','sale_dollars_trans']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Whiskey','Not Whiskey'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Whiskey','Not Whiskey'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xpn_F7R6LoNC"
   },
   "source": [
    "By plotting our data density graphs next to one another, we can see the separation between distributions of the original data sets next to our support vector data on key variables, on the whiskey classifier. \n",
    "\n",
    "What we are looking for here is the value of separation between what can be classified as a whiskey vs non whiskey (blue vs red lines). Since you can see that the separation on state bottle cost and the state bottle volume is more pronounced on the support vector model, this shows that they both play the most role in the prediction of whiskey. \n",
    "\n",
    "The other two variables, bottles sold and sale dollars, don't diverge as much between the original and SVM model, they are not as strong as predictors as the other two we discussed. And this is indicated on our weights we showed previously. "
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "msds7331_mini_clark_schueder_vela_washburn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
   "language": "python",
   "name": "python37164bitml7331condaf5c5c353a7fa46f5b783ad99991c2b10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}