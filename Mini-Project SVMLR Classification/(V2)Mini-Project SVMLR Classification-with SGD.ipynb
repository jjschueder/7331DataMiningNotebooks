{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(V2)Mini-Project SVMLR Classification-with SGD.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/jjschueder/7331DataMiningNotebooks/blob/master/Mini-Project%20SVMLR%20Classification/(V2)Mini-Project%20SVMLR%20Classification-with%20SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qhqdMtT3nSiG"},"source":["# Mini-Project: SVM & LR Classification - project guidelines\n","You are to perform predictive analysis (classification) upon a data set: model the dataset using\n","methods we have discussed in class: logistic regression & support vector machines and making\n","conclusions from the analysis. Follow the CRISP-DM framework in your analysis (you are not\n","performing all of the CRISP-DM outline, only the portions relevant to the grading rubric outlined\n","below). This report is worth 10% of the final grade. You may complete this assignment in teams\n","of as many as three people.\n","Write a report covering all the steps of the project. The format of the document can be PDF,\n","*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in\n","the rendered Jupyter notebook. The results should be reproducible using your report. Please\n","carefully describe every assumption and every step in your report.\n","A note on grading: A common mistake I see in this lab is not investigating different input\n","parameters for each model. Try a number of parameter combinations and discuss how the model changed. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"OSgE3De7drJu","colab_type":"text"},"source":["# Mini-Project: SVM & LR Classification - project requirements\n","*   [50 points] Logistic Regression and Support Vector Machine models\n","   * Create logistic regression model and support vector machine model for classification task in dataset.\n","   * Assess how well each model performs (use 80/20 training/testing split of our data)\n","   * Adjust parameters of the models to make them more accurate\n","   * If dataset size requires stochastic gradient descent, then linear kernel only is fine - meaning: SGDClassifier can be used to optimize logistic regression and linear support vector machines\n","   * For many problems, SGD will be required in order to train the SVM model in reasonable timeframe\n","*   [10 points] Model advantages\n","   * Discuss advantages of each model for each classfication task\n","   * Does type of model offer superior performance over another in terms of prediction accuracy\n","   * Does type of model offer superior performance over another in terms training time or efficiency\n","*   [30 points] Feature importance\n","   * Use the weights from logistic regression to interpret the importance of different features for the classification task in detail\n","   * Why do we think some variables are more important than others\n","*   [10 points] Support Vector Analysis\n","   * Looking at the chosen support vectors for the classfication task, answer the following:\n","   * Do these provide any insight into the data? Explain\n","   * If stochastic gradient descent used; therefore, did not explicitlity solve for support vectors\n","      * Subsample the data to train the SVC model\n","      * Analyze support vectors from subsampled dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jmTlKM_UnSiP"},"source":["#Mini-Project: SVM & LR Classification"]},{"cell_type":"markdown","metadata":{"id":"SBwy_rCue3sN","colab_type":"text"},"source":["##Team\n","\n","* Joseph Schueder\n","* Armando Vela\n","* Daniel Clark\n","* Jeff Washburn"]},{"cell_type":"markdown","metadata":{"id":"cBTqQnSVewMl","colab_type":"text"},"source":["##Business Understanding\n","\n","Introduction - The Iowa Liquor Sales dataset is an API from Google’s Bigquery which contains the wholesale purchases by retail stores in the Iowa area.  The dataset includes the spirit purchase details by product, date of purchase, and location the item was purchased from an Iowa Class “E” liquor license holder (retail stores). The time frame of this data starts from January 1, 2012 through December 31, 2019. As part of the study commissioned by the Iowa Department of Commerce, all alcoholic sales within the state were logged into the Department system, and in turn, published as open data by the State of Iowa. The dataset contains detail on the name, product, quantity and location of the individual container or package sale between the wholesaler (vendor) and the retailer.\n","\n","\n","\n","###1.   Set Objectives\n","\n","\n","> Having found that Iowa is a prime market for different kind of liquor sales, we want to investigate the current sales trends between Whiskey Sales vs Non-Whiskey sales.  We would like to classify the liquor sales based on if it will be a Whiskey sale or not.  \n","\n","\n","###2.   Product Project Plan\n","\n","> To meet our goals, we will build upon the dataset we used in our initial analysis to focus in on the Whiskey and Non-Whiskey sales.  We will use the same 400k rows from the last analysis and in addition further cleansing was done to 1 hot encode some of the categorical features, which will be our starting point on the data.\n","\n","###3.   Business Success Criteria\n","\n","> For our classification models, we will consider our model successful if we can classify the Whiskey vs Non-Whiskey sales with an accuracy of 51% or higher.  We will use cross-validaton using a split of 80% of the data for training and 20% of the data for testing to evaulate the performance of our classifiers.  We will build 2 different classification models - 1) using logistic regression, and 2) support vector machines (SVMs).  Since our data set is so large, we will also optimize both classification models using stochastic gradient descent using the linear kernel.  This will help train our models in a reasonable timeframe\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AsKFLvq2j5su","colab_type":"text"},"source":["## Data Understanding\n","\n","The initial data set was 4.63GB with 17.7 million rows.  We subsetted that down to 400k rows using a random set of data from the 2019 sales data.  This work was done in our initial processing from earlier.\n","\n","So taking our initial processing that was done from previous work, we further refined the 400k dataset by one-hot encoding the categorical features for sales month, liquor categories, and the stores.  The stores will not be used in this analysis; however, it was still one-hot encoded for future analysis\n","\n","It's also worth noting, that we used the log transformed values for sales dollars, cost per liter, state bottle cost and volume sold in liters along with the liquor catogory to focus on our classification\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F_WkOv5ll__p","colab_type":"text"},"source":["## Data Preperation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lPXTcIccnSiW","colab":{}},"source":["# import necessary packages for complete analysis\n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt \n","plt.rc(\"font\", size=14)\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","import seaborn as sns\n","sns.set(style=\"white\")\n","sns.set(style=\"whitegrid\", color_codes=True)\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics as mt\n","from sklearn.model_selection import ShuffleSplit\n","from sklearn import metrics \n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import auc\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn import preprocessing\n","from sklearn.svm import SVC\n","from sklearn.model_selection import StratifiedShuffleSplit \n","from sklearn.linear_model import SGDClassifier"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6I6a7elv4W0h"},"source":["### Load Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mo0kRfD4nSik","colab":{}},"source":["# The data was produced from the Iowa Alcohol Dataset 2019 year data.  The total data set size has over 17 million rows.  We took\n","# a subet of that data, 400,000 rows\n","\n","# The 400k rows were cleansed and transformed lab1 and exported as a csv to github for use here.\n","# source python notebook: https://github.com/jjschueder/7331DataMiningNotebooks/blob/master/lab1/msds7331_clark_schueder_vela_washburn.ipynb\n","\n","# Further cleansing was done to 1 hot encode some of the categorical features, which is what we are reading in here.  Still using 400k rows of data\n","\n","# read csv from github directly\n","url_dataset = 'https://raw.githubusercontent.com/jjschueder/7331DataMiningNotebooks/master/Live%20Assignments/df1hotmerge2.csv'\n","data = pd.read_csv(url_dataset)\n","#data = pd.read_csv(url_dataset, nrows=100000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cx4qDu_gnml3","colab_type":"text"},"source":["###Columns and Descriptors"]},{"cell_type":"code","metadata":{"id":"7m5y9Iw-nsYs","colab_type":"code","colab":{}},"source":["data.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DQKPLfW1nSiv","scrolled":true,"colab":{}},"source":["# Validate dataset\n","print(data.shape)\n","print(list(data.columns))\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeJ06hpUqLJW","colab_type":"text"},"source":["###Remove unwanted columns"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jSmKpN1snSjB","colab":{}},"source":["# Remove unwanted columns, which include all the specific liquor categories, \n","# except for liquor_category_WHISKY since that is what we want to classify on, along\n","# with all the store_ attributes\n","\"\"\"\n","cat_vars=['counter', 'liquor_category', 'store_parent',\n"," 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', \n"," 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', \n"," 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb',\n"," 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', \n"," 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', \n"," 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', \n"," 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens']\n","data_vars=data.columns.values.tolist()\n","to_keep=[i for i in data_vars if i not in cat_vars]\n","\"\"\"\n","to_keep=['sale_dollars_trans', 'cost_per_liter_trans',\n","      'state_bottle_cost_trans', 'bottles_sold_trans',\n","       'volume_sold_liters_trans','pack', 'bottle_volume_ml',\n","       'liquor_category_WHISKY']\n","data_final=data[to_keep]\n","data_final.columns.values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIlHFqU5mjwq","colab_type":"text"},"source":["###Simple Statistics"]},{"cell_type":"code","metadata":{"id":"CA4_x29_n_Gc","colab_type":"code","colab":{}},"source":["data_final.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tzx9_JhrnSi7","colab":{}},"source":["# From the entire dataset (400k rows) take a look at what our Whiskey vs Non-Whiskey distribution looks like\n","#data = data.drop(columns = ['Unnamed: 0'])\n","count_not_whiskey = len(data_final[data_final['liquor_category_WHISKY']==0])\n","count_whiskey = len(data_final[data_final['liquor_category_WHISKY']==1])\n","pct_of_no_whiskey = count_not_whiskey/(count_not_whiskey+count_whiskey)\n","print(\"Percentage of not whiskey is\", pct_of_no_whiskey*100)\n","pct_of_whiskey = count_whiskey/(count_not_whiskey+count_whiskey)\n","print(\"Percentage of whiskey\", pct_of_whiskey*100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3f2Dcd8LEpB","colab_type":"code","colab":{}},"source":["data_final.info"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rw60isQ0rXD0","colab_type":"text"},"source":["###Locate Features and Label\n","\n","Our features will be everything that isn't the liquor_category_WHISKY\n","\n","And our label, what we are trying to classify on, will be the liquor_category_WHISKY"]},{"cell_type":"code","metadata":{"id":"PQkL5USKrazL","colab_type":"code","colab":{}},"source":["# X will be our features\n","X = data_final.loc[:, data_final.columns != 'liquor_category_WHISKY']\n","\n","# y will be our label, what we are trying to classify\n","y = data_final.loc[:, data_final.columns == 'liquor_category_WHISKY']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G65Y_NFdrazQ","colab_type":"code","colab":{}},"source":["X.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNajo2nqr7JR","colab_type":"code","colab":{}},"source":["y.head"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zIfgUT054dBe"},"source":["## Modeling"]},{"cell_type":"markdown","metadata":{"id":"dgWC8imwsEj2","colab_type":"text"},"source":["###80/20 training/testing split "]},{"cell_type":"code","metadata":{"id":"g3Z9m5pcsT5R","colab_type":"code","colab":{}},"source":["# Create the training/testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","columns = X_train.columns\n","\n","print('X_train shape: ', X_train.shape)\n","print('X_test shape: ', X_test.shape)\n","print('Y_train shape: ', y_train.shape)\n","print('Y_test shape: ', y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pxfgw169seqP","colab_type":"text"},"source":["### Create Logistic Regression Model"]},{"cell_type":"code","metadata":{"id":"efU9cWoAsmSj","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","# all parameters not specified are set to their defaults\n","# seeing message 'lbfgs failed to converge. Increase the number of iterations'\n","# https://stats.stackexchange.com/questions/184017/how-to-fix-non-convergence-in-logisticregressioncv\n","# increased max_iter to 500 \n","#simplelogisticRegr = LogisticRegression()# class_weight = 'balanced')\n","simplelogisticRegr = LogisticRegression(max_iter=500)# class_weight = 'balanced')\n","\n","\n","# fit (train) the model\n","# was seing an error about column-vector y was passed when 1d array expected\n","# fix: https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected\n","#simplelogisticRegr.fit(X_train, y_train)\n","simplelogisticRegr.fit(X_train, y_train.values.ravel())\n","\n","coef = simplelogisticRegr.coef_[0]\n","print ('Coefficients: ', '\\n', coef)\n","\n","classes = simplelogisticRegr.classes_[0]\n","print()\n","print ('Classes: ', '\\n', classes)\n","\n","intercept = simplelogisticRegr.intercept_[0]\n","print()\n","print ('Intercept: ', '\\n', intercept)\n","\n","params =simplelogisticRegr.get_params(deep=True)\n","print()\n","print ('Params: ', '\\n', params)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-wRKRHAmuFzL","colab_type":"text"},"source":["### Logistic Regression Predictions"]},{"cell_type":"code","metadata":{"id":"mJldaxo6uMTW","colab_type":"code","colab":{}},"source":["# Do predictions\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn import metrics\n","\n","# Use score method to get accuracy of model\n","score = simplelogisticRegr.score(X_test, y_test)\n","print(\"Accuracy: \", '\\n', score)\n","print()\n","\n","# Predictions\n","predictions = simplelogisticRegr.predict(X_test)\n","print('Confusion Matrix:\\n', confusion_matrix(y_test, predictions))\n","print()\n","\n","# Classification Report\n","print('Classification Report:\\n', classification_report(y_test, predictions))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dj-uDeWtElF","colab_type":"text"},"source":["We ran a \"simple\" logistic regression model using the default hyper parameters of C=1.0, no weighting of the classes and with a ridge regression penalty. \n","\n","As a result, our model achieved 92% accuracy which is a bit higher than the accuracy we would achieve if we were to guess randomly based on the distribution of whiskies vs non whiskies. From here, we can use a grid search procedure to tune our hyper parameters to improve our accuracy score. "]},{"cell_type":"markdown","metadata":{"id":"AqDz7OOJ5Sjg","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","# Jeff - Stopped here\n","\n","I got this far as of friday night on the documentation.  There was an issue w/ the model about ravel and also it wasn't converging.  Both those have been fixed.  I added the CRISP-dm stuff (business understanding, data understanding, data prep, Modeling)\n","\n","This will show up a bit nicer in colab by viewing the table of contents.  I still need to go through the parts below here and will work on that tomorrow (saturday 02/15) \n","\n","I'm committing where I'm at now w/ the output cleared.\n","\n","The very top sections (on project guidelines and project requirements) can probably be removed when we turn in.  I just wanted to have those in place to make sure all his asks are touched \n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j6-0y2G9nSjI"},"source":["### Models Using Grid Search to compare best possible model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rq1Jn4krnSjK","colab":{}},"source":["#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","#columns = X_train.columns\n","# scale attributes by the training set\n","scl_obj = StandardScaler()\n","scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n","# the line of code above only looks at training data to get mean and std and we can use it \n","# to transform new feature data\n","\n","X_train_scaled = scl_obj.transform(X_train) # apply to training\n","X_test_scaled = scl_obj.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0VmHek3j7A0u","colab":{}},"source":["#Logisitic regression 10-fold cross-validation \n","\n","#Divide data into test and training splits\n","cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)\n","\n","regEstimator = LogisticRegression()\n","\n","\n","parameters = { 'penalty':['l2', 'l1', 'elasticnet']\n","              ,'C': [0.001, 1, 10, 100]\n","              ,'class_weight': ['balanced', 'none']\n","              ,'random_state': [0]\n","              ,'solver': ['lbfgs', 'saga', 'liblinear', 'newton-cg', 'sag']\n","              ,'max_iter':[100]\n","             }\n","\n","#Create a grid search object using the  \n","\n","regGridSearch = GridSearchCV(estimator=regEstimator\n","                   , n_jobs=8 # jobs to run in parallel\n","                   , verbose=1 # low verbosity\n","                   , param_grid=parameters\n","                   , cv=cv # KFolds = 10\n","                   , scoring='accuracy')\n","\n","#Perform hyperparameter search to find the best combination of parameters for our data\n","regGridSearch.fit(X_train_scaled, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a2Gdgl8k62xQ","colab":{}},"source":["#Diplay the top model parameters\n","regGridSearch.best_estimator_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H_52IJqizdEh","colab_type":"text"},"source":["After running our grid search, we found that the ideal model uses a C=0.001 which is different from our default model of C=1.0. Otherwise, the other parameters remained the same. \n","\n","Let's see how this does with our accuracy score. "]},{"cell_type":"code","metadata":{"id":"D1KcZcOKrazj","colab_type":"code","colab":{}},"source":["type(regGridSearch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1ya8u75razm","colab_type":"code","colab":{}},"source":["gridresults = pd.DataFrame(regGridSearch.cv_results_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"APOVOlDmrazq","colab_type":"code","colab":{}},"source":["columns = ['param_solver','param_C', 'param_max_iter', 'class_weight', 'param_penalty', 'mean_test_score', 'rank_test_score']\n","gridresults = pd.DataFrame(gridresults, columns=columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1TRv-bDrazv","colab_type":"code","colab":{}},"source":["gridresults.sort_values(by=['rank_test_score'], ascending=True).head(20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1pmxU6zkraz2","colab_type":"code","colab":{}},"source":["y_hat = regGridSearch.predict(X_test_scaled) # get test set precitions\n","\n","# now let's get the accuracy and confusion matrix for this iterations of training/testing\n","acc = mt.accuracy_score(y_test,y_hat)\n","conf = mt.confusion_matrix(y_test,y_hat)\n","\n","print(\"accuracy\", acc )\n","print(\"confusion matrix\\n\",conf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnDfa6wHBZWL","colab_type":"code","colab":{}},"source":["predictions2 = regGridSearch.predict(X_test)\n","from sklearn.metrics import classification_report, confusion_matrix\n","print(confusion_matrix(y_test, y_hat))\n","print(classification_report(y_test, y_hat))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6ZiT2noqgGS","colab_type":"text"},"source":["After running a grid search to uncover the top performing logistic regression hyperparameter combination, we are seeing that the accuracy score is close to our simple logistic regression model, however, it did come in slightly lower. \n","\n","The Simple Logistic Regression model was the winner with a C=1 which is less regularization than the grid search that called for a stronger regularization at C = 0.001. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iuBdzdM8nSjo"},"source":["ROC Curves and AUC in Python\n","https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n","\n","Since our accuracy was better was the better with our logistic regression model, we will use this in building an ROC curve to show how our model is performing. "]},{"cell_type":"code","metadata":{"id":"MNU1Kc-wraz8","colab_type":"code","colab":{}},"source":["# predict probabilities\n","lr_probs = simplelogisticRegr.predict_proba(X_test_scaled)\n","ns_probs = [0 for _ in range(len(y_test))]\n","# keep probabilities for the positive outcome only\n","lr_probs = lr_probs[:, 1]\n","# calculate scores\n","ns_auc = roc_auc_score(y_test, ns_probs)\n","lr_auc = roc_auc_score(y_test, lr_probs)\n","# summarize scores\n","print('No Skill: ROC AUC=%.3f' % (ns_auc))\n","print('Logistic: ROC AUC=%.3f' % (lr_auc))\n","# calculate roc curves\n","ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n","lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvPfcXa9raz_","colab_type":"code","colab":{}},"source":["# calculate roc curves\n","ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n","lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n","# plot the roc curve for the model\n","pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n","pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n","# axis labels\n","pyplot.xlabel('False Positive Rate')\n","pyplot.ylabel('True Positive Rate')\n","# show the legend\n","pyplot.legend()\n","# show the plot\n","pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XsPBghgPsugq","colab_type":"text"},"source":["Looking at our true positive rate compared to false positive rate, with the diagonal line representing a 50% false positive rate, we can see that our model is producing more True positives than false positives. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kDI0MDSYnSj0","colab":{}},"source":["ywhiskcnt = y_test.apply(pd.value_counts)\n","ycnt = ywhiskcnt.loc[1:1, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"egJSv8ZOnSj5","colab":{}},"source":["# predict probabilities\n","lr_probs = simplelogisticRegr.predict_proba(X_test_scaled)\n","# keep probabilities for the positive outcome only\n","lr_probs = lr_probs[:, 1]\n","# predict class values\n","yhat = regGridSearch.predict(X_test_scaled)\n","lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n","lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n","# summarize scores\n","print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n","# plot the precision-recall curves\n","ywhiskcnt = y_test.apply(pd.value_counts)\n","ycnt = ywhiskcnt.loc[1:1, :]\n","no_skill = ycnt.values[0] / len(y_test)\n","#no_skill = len(y_test[y_test==1]) / len(y_test)\n","no_skill\n","#no_skill = 0.2\n","pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n","pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n","# axis labels\n","pyplot.xlabel('Recall')\n","pyplot.ylabel('Precision')\n","# show the legend\n","pyplot.legend()\n","# show the plot\n","pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhJSZKDgtn08","colab_type":"text"},"source":["Running a Precision/ Recall line plot, we can see that our model tends to lose precision as we get a higher recall, with the bottoming out of 0.5 precision and 0.2 being the point of spiking the performance. \n","\n","So essentially, it is saying that our model is better at predicting the correct line items that were whiskies than correctly discerning between whiskies and non whiskies. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dC7CCKAznSkt"},"source":["#### Support Vector Machine Model 1:"]},{"cell_type":"code","metadata":{"id":"6AIn_2Sora0N","colab_type":"code","colab":{}},"source":["%%time\n","\n","#possibility to scale it?\n","from sklearn.svm import LinearSVC\n","from sklearn import preprocessing\n","X_train_scaled \n","X_test_scaled \n","#X_train = preprocessing.scale(X_train)\n","#X_test = preprocessing.scale(X_test)\n","\n","# all parameters not specified are set to their defaults\n","#https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n","from sklearn.svm import SVC\n","#svclassifier = SVC(kernel='poly', degree=8)\n","#svclassifier = SVC(kernel='rbf')\n","#svclassifier = SVC(kernel='linear', C = 1000, random_state=0)\n","svclassifier = LinearSVC(C=100)\n","model = svclassifier.fit(X_train_scaled,  y_train)\n","model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"syBDZ4Tnra0Q","colab_type":"code","colab":{}},"source":["# Returns a NumPy Array\n","predictions = svclassifier.predict(X_test_scaled)\n","from sklearn.metrics import classification_report, confusion_matrix\n","print(\"confusion matrix\\n\")\n","print(confusion_matrix(y_test, predictions))\n","print(\"\\nClassification report\\n\")\n","print(classification_report(y_test, predictions))\n","acc = mt.accuracy_score(y_test,predictions)\n","print(\"\\naccuracy\\n\", acc )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRO9IorFD6A6","colab_type":"text"},"source":["For our Support Vector Machine, we were able to derive an 83% Accuracy with a precision of 85% and Recall of 95%. \n","\n"]},{"cell_type":"code","metadata":{"id":"PZpn-RaXE9oI","colab_type":"code","colab":{}},"source":["# if using linear kernel, these make sense to look at (not otherwise, why?)\n","print(svclassifier.coef_)\n","weights = pd.Series(svclassifier.coef_[0],index=X.columns)\n","weights.plot(kind='bar')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_EsEVp7IFH1Q","colab_type":"text"},"source":["Looking at the key values associated with driving the prediction classifier of whiskey vs non whiskey, we can see that the state bottle cost is the key factor in driving performance, followed by the sale amount of the transation which has a negative relationship to whether or not a sale was a whiskey."]},{"cell_type":"markdown","metadata":{"id":"uuxE-PliDowG","colab_type":"text"},"source":["#### SVM Classifier 2 (with Hyper Parameter Tuning)"]},{"cell_type":"code","metadata":{"id":"PzfDHSjiDnqZ","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn.preprocessing import StandardScaler\n","\n","# scale attributes by the training set\n","scl_obj = StandardScaler()\n","scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n","# the line of code above only looks at training data to get mean and std and we can use it \n","# to transform new feature data\n","\n","X_train_scaled = scl_obj.transform(X_train) # apply to training\n","X_test_scaled = scl_obj.transform(X_test)\n","\n","# train the model just as before\n","svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n","model3 = svm_clf.fit(X_train_scaled, y_train)  # train object\n","model3\n","\n","y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n","\n","acc = mt.accuracy_score(y_test,y_hat)\n","conf = mt.confusion_matrix(y_test,y_hat)\n","print('accuracy:', acc )\n","print(conf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s32GiA8NEIOV","colab_type":"code","colab":{}},"source":["print(\"\\nClassification report SVM Model 2 \\n\")\n","print(classification_report(y_test,y_hat))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWHeJoWBEkvw","colab_type":"text"},"source":["With SVM Model 2, we attain the highest accuracy score with 87%, with alos the highest precision, recall and F1 Score achieved with our model. The SVM CLF will be the model we chose in our interpretation. "]},{"cell_type":"code","metadata":{"id":"5USCaQW_ra0S","colab_type":"code","colab":{}},"source":["# look at the support vectors\n","print(svm_clf.support_vectors_.shape)\n","print(svm_clf.support_.shape)\n","print(svm_clf.n_support_ )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGMJSwdCra0Z","colab_type":"text"},"source":["So the analysis here is basically telling us what the original statistics of the data looked like, and also what the statistics of the support vectors looked like. We can see that the separation in distributions is not as great as the separation for the original data. This is because the support vectors tend to be instances on the edge of the class boundaries and also instances that are classified incorrectly in the training data.\n","\n","You can also look at joint plots of the data and see how relationships have changed."]},{"cell_type":"markdown","metadata":{"id":"4tgMvzB_ra0a","colab_type":"text"},"source":["### SGD (\"Simple\" Model)"]},{"cell_type":"code","metadata":{"id":"HFzmNkUOra0c","colab_type":"code","colab":{}},"source":["%%time\n","# now divide the data into test and train using scikit learn built-ins\n","\n","cv = StratifiedShuffleSplit( n_splits=1,test_size=0.8)\n","# use some compact notation for creating a linear SVM classifier with stochastic descent\n","\n","regularize_const = 0.1\n","iterations = 5\n","\n","# use some compact notation for creating a logistic regression classifier with stochastic descent\n","log_sgd = SGDClassifier(alpha=regularize_const,\n","        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n","        loss='log', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n","\n","scl = StandardScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gtuVvh1bra0f","colab_type":"code","colab":{}},"source":["for train_idx, test_idx in cv.split(X,y):\n","    log_sgd.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx])\n","    yhat2 = log_sgd.predict(scl.transform(X.iloc[test_idx]))\n","    \n","    conf = mt.confusion_matrix(y.iloc[test_idx],yhat2)\n","    acc = mt.accuracy_score(y.iloc[test_idx],yhat2)\n","\n","print('Logistic Regression:', acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiKuDrFira0j","colab_type":"code","colab":{}},"source":["conf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MRoMK7RG1fk","colab_type":"text"},"source":["Of all our models, the SGD \"simple\" model had the worst accuracy score of 80%. "]},{"cell_type":"markdown","metadata":{"id":"aDdX6drTvR9a","colab_type":"text"},"source":["### SGD 2 (with parameterization testing)\n","\n","With this parameterization tuning set, we will run a hyper-parameter fitting model called Parfit. "]},{"cell_type":"code","metadata":{"id":"wzrjOwATxnRh","colab_type":"code","colab":{}},"source":["#pip install parfit"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72Uc333JvU3M","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import ParameterGrid\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","import parfit.parfit as pf\n","\n","grid = {\n","    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n","   # 'n_iter': [1000], # number of epochs\n","    'loss': ['log'], # logistic regression,\n","    'penalty': ['l2'],\n","    'n_jobs': [-1]\n","}\n","\n","paramGrid = ParameterGrid(grid)\n","\n","bestModel = pf.bestFit(SGDClassifier, paramGrid,\n","           X_train, y_train, X_test, y_test, \n","           metric = roc_auc_score, \n","           scoreLabel = \"AUC\")\n","\n","print(bestModel)\n","\n","#Perform hyperparameter search to find the best combination of parameters for our data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJg7hzIswFtW","colab_type":"code","colab":{}},"source":["\n","# use some compact notation for creating a logistic regression classifier with stochastic descent\n","log_sgd2 = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n","              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n","              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n","              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n","              random_state=None, shuffle=True, tol=0.001,\n","              validation_fraction=0.1, verbose=0, warm_start=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iR4sQbPjwBvD","colab_type":"code","colab":{}},"source":["model2 = log_sgd2.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx])\n","\n","for train_idx, test_idx in cv.split(X,y):\n","    log_sgd2.fit(scl.fit_transform(X.iloc[train_idx]),y.iloc[train_idx])\n","    yhat = log_sgd2.predict(scl.transform(X.iloc[test_idx]))\n","    \n","    conf = mt.confusion_matrix(y.iloc[test_idx],yhat)\n","    acc = mt.accuracy_score(y.iloc[test_idx],yhat)\n","\n","print('Logistic Regression:', acc)\n","print('Confusion Matrix: ', conf)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3lgvVqN0BXO","colab_type":"text"},"source":["After installing and running the Hyper-Parameter Optimization using parfit (https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4), we can see that we improved upon our original Stochastic Gradient Descent Model with an accuracy score of 85.22%\n","\n","This SGD and the Tuned SVM model will be the ones we use moving forward. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GDyKKT0mnSmE"},"source":["### Advantages  [10 points] \n","Discuss the advantages of each model for each classification task. Does one\n","type of model offer superior performance over another in terms of prediction accuracy? In\n","terms of training time or efficiency? Explain in detail. \n","\n","#### Logistic Regression\n","* Advantages - There is less transformation needed in order to make the model return in a reasonable amount of time. Scaler transformation will increase the accuracy, but is not required for peroformance as the model performs well at scale regardless of whether the data is normalized or not. \n","* Performance - There are many different setting that allow the model to have the best possible accuracy. These methods can be iterated with a grid search method. \n","*  Training Time efficiency is very good at large scales and does not suffer as volume of data increases.\n","\n","#### Support Vector Machine\n","* Advantages - From Tan book Chapter 5 - A classification technique that has received considerable attention is support vector machine (SVM). This technique has its roots in statistical learning theory and has shown promising empirical results in many practical applications, from handwritten digit recognition to text categorization. SVM also works very well with high-dimensional data and avoids the curse of dimensionality problem. Another unique aspect of this approach is that it represents the decision boundary using a subset of the training examples, known as the support vectors.  \n"," However, in order to get high accuracy scaling and high penalties must be set to achieve accuracy with unbalanced data. \n","* Performance - Changing the cost to value 1000, after previously using 1, 10, and 100 the performance of prediciton accuracy improves over that of logistic regession.  In order to get a result in a similiar amount of time as logistic regression accuracy is sacrficed. \n","* Training Time\n","    * Slow with large data volume and features\n","    * Transformation of data helps\n","    * Very slow with high cost.\n","\n","#### Stochastic Gradiant Descent\n","* Advantages - The design of Stochastic Gradient Smith is set up that it only considers a single randonm point in the stochastic array, and changes weights to classify values. The advantage of running stochastic gradient descent is that it runs much quicker than a gradient descent model, which can take hours / days to run. \n","* Performance - While a single stochastic gradient descent model didn't perform better than any of the previous models we ran, we were able to install and run a Parameter Fit, which acted as a grid search to help us choose the optimal hyper parameters. Doing this improved our SGD performace to 83.5% accuracy. \n","   \n","    \n","\n","#### Conclusion\n","The logistic regression provides a way to quickly get to a model, though has less accuracy when compared to SVM.  SVM is very accurate while its performance degrades very quickly with large volumes of data with rows and also with a large number of features. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VX-oO4qOnSmF"},"source":["###  Interpret  [30 points] \n","Use the weights from logistic regression to interpret the importance of different\n","features for the classification task. Explain your interpretation in detail. Why do you think\n","some variables are more important? \n","\n","The two fields \"Average of cost_per_liter_trans\" and \"Average of state_bottle_cost_trans\" seem to have the highest coeficients on the logistic regression model. \n","\n","Type\t     Average of cost_per_liter_trans\tAverage of state_bottle_cost_trans\n","Not Whiskey\t 2.675545886\t                    1.999205623\n","Whiskey      3.545918698\t                    2.871239458\n","\n","Add a t-test to show they are significantly different averages? \n","\n","The model seems to point out that price is the largest differentiator of the available characteristics. Whiskey is more expensive than other liquors. A higher cost per bottle can be used to identify whether a trancation was Whiskey or not. Other numeric features that are present represent bottle size and quanities of bottles sold. Since bottle sizes are fairly standard across the different types of liquors and quantity sold are well balanced across the liquor types and Whiskey is in the middle of the pack for these measures they do not have as much impact."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ye0EUAwn19qH","colab":{}},"source":["# interpret the weights\n","# iterate over the coefficients\n","weights2 = simplelogisticRegr.coef_.T # take transpose to make a column vector\n","variable_names = X.columns\n","for coef, name in zip(weights2,variable_names):\n","    print(name, 'has weight of', coef[0])\n","    \n","# does this look correct? "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NFTKvdcf2DyB","colab":{}},"source":["# now let's make a pandas Series with the names and values, and plot them\n","%matplotlib inline\n","plt.style.use('ggplot')\n","\n","\n","weights = pd.Series(simplelogisticRegr.coef_[0],index=X.columns)\n","weights.plot(kind='bar')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yVHNEFrTnSmG"},"source":["### Insight into the data [10 points]\n","Look at the chosen support vectors for the classification task. Do these provide\n","any insight into the data? Explain. If you used stochastic gradient descent (and therefore did\n","not explicitly solve for support vectors), try subsampling your data to train the SVC model—\n","then analyze the support vectors from the subsampled dataset. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OXMl4cgmnSmH","colab":{}},"source":["#view support vectors\n","svm_clf.support_vectors_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L2A_wTYZ1p7X","colab":{}},"source":["# View indicies of support vectors\n","svm_clf.support_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qxAYjH4AWOmK"},"source":["This chooses all the misclassified items as a support vectors."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mHkRqABz1zz2","colab":{}},"source":["#view number of support vectors for each class\n","svm_clf.n_support_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d4XKEUr0WVPU"},"source":["We used 12,000 instances of Whiskey and Nonwhiskey to build our decision boundary."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aPO1D6PsWd0P","colab":{}},"source":["# if using linear kernel, these make sense to look at (not otherwise, why?)\n","print(model.coef_)\n","weights3 = pd.Series(model.coef_[0],index=X_train.columns)\n","weights3.plot(kind='bar')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNo5j26LKCD_","colab_type":"text"},"source":["Next we are going to make a stratified shuffle split by plotting our SVC models."]},{"cell_type":"code","metadata":{"id":"Us7Mt_R3KBV-","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn import metrics as mt\n","\n","# we want to predict the X and y data as follows:\n","if 'liquor_category_WHISKY' in data_final:\n","    y = data_final['liquor_category_WHISKY'].values # get the labels we want\n","    del data_final['liquor_category_WHISKY'] # get rid of the class label\n","    X = data_final.values # use everything else to predict!\n","\n","num_cv_iterations = 3\n","num_instances = len(y)\n","cv_object = ShuffleSplit(n_splits = num_cv_iterations, \n","                            test_size = 0.20, train_size = 0.80, random_state=1)\n","\n","for train_indices, test_indices in cv_object.split(X,y): \n","    # I will create new variables here so that it is more obvious what \n","    # the code is doing (you can compact this syntax and avoid duplicating memory,\n","    # but it makes this code less readable)\n","    X_train = X[train_indices]\n","    y_train = y[train_indices]\n","    \n","    X_test = X[test_indices]\n","    y_test = y[test_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zvfo-WryKiDE","colab_type":"text"},"source":["Now that we established our new training and testing indicies, let's run it against our top performing SVC CLF model (the one that got 87% accuracy)."]},{"cell_type":"code","metadata":{"id":"GswE8eMgKJlR","colab_type":"code","colab":{}},"source":["# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n","\n","# now lets look at the support for the vectors and see if we they are indicative of anything\n","# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n","\n","# make a dataframe of the training data\n","df_tested_on = data_final.iloc[train_indices].copy() # saved from above, the indices chosen for training\n","# now get the support vectors from the trained model\n","df_support = df_tested_on.iloc[svm_clf.support_,:]\n","\n","df_support['liquor_category_WHISKY'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n","data_final['liquor_category_WHISKY'] = y # also add it back in for the original data\n","df_support.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdLxZ2S1K2i2","colab_type":"text"},"source":["Doing this granted us 24k non null values within what became our testing set. Using this, we will plot out the original vs the svm distribution of our state bottle cost trans, bottles sold trans, bottle volume ml and sale dollars trans, which was shown to be our strongest performing variables indicative of Whiskies vs Non Whiskies."]},{"cell_type":"code","metadata":{"id":"PSgQDqQnKxNk","colab_type":"code","colab":{}},"source":["# now lets see the statistics of these attributes\n","from pandas.plotting import boxplot\n","\n","# group the original data and the support vectors\n","df_grouped_support = df_support.groupby(['liquor_category_WHISKY'])\n","df_grouped = data_final.groupby(['liquor_category_WHISKY'])\n","\n","# plot KDE of Different variables\n","vars_to_plot = ['state_bottle_cost_trans','bottles_sold_trans','bottle_volume_ml','sale_dollars_trans']\n","\n","for v in vars_to_plot:\n","    plt.figure(figsize=(10,4))\n","    # plot support vector stats\n","    plt.subplot(1,2,1)\n","    ax = df_grouped_support[v].plot.kde() \n","    plt.legend(['Whiskey','Not Whiskey'])\n","    plt.title(v+' (Instances chosen as Support Vectors)')\n","    \n","    # plot original distributions\n","    plt.subplot(1,2,2)\n","    ax = df_grouped[v].plot.kde() \n","    plt.legend(['Whiskey','Not Whiskey'])\n","    plt.title(v+' (Original)')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xpn_F7R6LoNC","colab_type":"text"},"source":["By plotting our data density graphs next to one another, we can see the separation between distributions of the original data sets next to our support vector data on key variables, on the whiskey classifier. \n","\n","What we are looking for heere is the value of separation between what can be classified as a whiskey vs non whiskey (blue vs red lines). Since you can see that the separation on state bottle cost and the state bottle volume is more pronounced on the support vector model, this shows that they both play the most role in the prediction of whiskey. \n","\n","The other two variables, bottles sold and sale dollars, don't diverge as much between the original and SVM model, they are not as strong as predictors as the other two we discussed. And this is indicated on our weights we showed previously. "]},{"cell_type":"code","metadata":{"id":"vJjUGp6FLauF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}