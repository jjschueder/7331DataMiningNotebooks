{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab2_DC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjschueder/7331DataMiningNotebooks/blob/master/lab2/.ipynb_checkpoints/(exceptionalwork)Copy_of_Lab2_DC-checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-yNWpXMe4Jfa"
      },
      "source": [
        "#**Data Mining 7331 - Spring 2020**\n",
        "\n",
        "## Lab 2 - Classification\n",
        "\n",
        "#### Daniel Clark, Joe Schueder, Jeff Washburn, Armando Vela\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZMu95a14g15"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The Iowa Liquor  Sales dataset is an API from Google’s Bigquery which contains the wholesale purchases by retail stores in the Iowa area. The dataset includes the spirit purchase details by product, date of purchase, and location the item was purchased from an Iowa Class “E” liquor license holder (retail stores). The time frame of this data starts from January 1, 2012 through December 31, 2019. As part of the study commissioned by the Iowa Department of Commerce, all alcoholic sales within the state were logged into the Department system, and in turn, published as open data by the State of Iowa. The dataset contains detail on the name, product, quantity and location of the individual container or package sale between the wholesaler (vendor) and the retailer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcfneriE5rS4"
      },
      "source": [
        "### Data Description \n",
        "\n",
        "The initial data set was 4.63GB with 17.7 million rows. We subsetted that down to 400k rows using a random set of data from the 2019 sales data. This work was done in our initial processing from earlier.\n",
        "\n",
        "So taking our initial processing that was done from previous work, we further refined the 400k dataset by one-hot encoding the categorical features for sales month, liquor categories, and the stores. The stores will not be used in this analysis; however, it was still one-hot encoded for future analysis\n",
        "\n",
        "It's also worth noting, that we used the log transformed values for sales dollars, cost per liter, state bottle cost and volume sold in liters along with the liquor catogory to focus on our classification and ensure that outliers do not hold too heavy of a weight in our analysis. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v9WmuBys4fB5",
        "outputId": "ca7cfc0c-5c01-4d0d-e904-8b007caa3f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url_dataset = 'https://raw.githubusercontent.com/jjschueder/7331DataMiningNotebooks/master/Live%20Assignments/df1hotmerge2.csv'\n",
        "data = pd.read_csv(url_dataset, nrows = 40000)\n",
        "data.info()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 68 columns):\n",
            "Unnamed: 0                  40000 non-null int64\n",
            "pack                        40000 non-null int64\n",
            "bottle_volume_ml            40000 non-null int64\n",
            "state_bottle_cost           40000 non-null float64\n",
            "state_bottle_retail         40000 non-null float64\n",
            "bottles_sold                40000 non-null int64\n",
            "sale_dollars                40000 non-null float64\n",
            "volume_sold_liters          40000 non-null float64\n",
            "volume_sold_gallons         40000 non-null float64\n",
            "counter                     40000 non-null int64\n",
            "liquor_category             40000 non-null object\n",
            "store_parent                40000 non-null object\n",
            "month                       40000 non-null object\n",
            "year                        40000 non-null int64\n",
            "monthyear                   40000 non-null object\n",
            "liquor_category_AMARETTO    40000 non-null int64\n",
            "liquor_category_BRANDY      40000 non-null int64\n",
            "liquor_category_GIN         40000 non-null int64\n",
            "liquor_category_LIQUEUR     40000 non-null int64\n",
            "liquor_category_Other       40000 non-null int64\n",
            "liquor_category_RUM         40000 non-null int64\n",
            "liquor_category_SCHNAPPS    40000 non-null int64\n",
            "liquor_category_TEQUILA     40000 non-null int64\n",
            "liquor_category_VODKA       40000 non-null int64\n",
            "liquor_category_WHISKY      40000 non-null int64\n",
            "store_parent_CVS            40000 non-null int64\n",
            "store_parent_Caseys         40000 non-null int64\n",
            "store_parent_Hy-Vee         40000 non-null int64\n",
            "store_parent_Kum&Go         40000 non-null int64\n",
            "store_parent_Other          40000 non-null int64\n",
            "store_parent_QuikTrip       40000 non-null int64\n",
            "store_parent_SamsClub       40000 non-null int64\n",
            "store_parent_SmokingJoes    40000 non-null int64\n",
            "store_parent_Target         40000 non-null int64\n",
            "store_parent_Wal-Mart       40000 non-null int64\n",
            "store_parent_Walgreens      40000 non-null int64\n",
            "month_Apr                   40000 non-null int64\n",
            "month_Aug                   40000 non-null int64\n",
            "month_Dec                   40000 non-null int64\n",
            "month_Feb                   40000 non-null int64\n",
            "month_Jan                   40000 non-null int64\n",
            "month_Jul                   40000 non-null int64\n",
            "month_Jun                   40000 non-null int64\n",
            "month_Mar                   40000 non-null int64\n",
            "month_May                   40000 non-null int64\n",
            "month_Nov                   40000 non-null int64\n",
            "month_Oct                   40000 non-null int64\n",
            "month_Sep                   40000 non-null int64\n",
            "year_2019                   40000 non-null int64\n",
            "monthyear_Apr-2019          40000 non-null int64\n",
            "monthyear_Aug-2019          40000 non-null int64\n",
            "monthyear_Dec-2019          40000 non-null int64\n",
            "monthyear_Feb-2019          40000 non-null int64\n",
            "monthyear_Jan-2019          40000 non-null int64\n",
            "monthyear_Jul-2019          40000 non-null int64\n",
            "monthyear_Jun-2019          40000 non-null int64\n",
            "monthyear_Mar-2019          40000 non-null int64\n",
            "monthyear_May-2019          40000 non-null int64\n",
            "monthyear_Nov-2019          40000 non-null int64\n",
            "monthyear_Oct-2019          40000 non-null int64\n",
            "monthyear_Sep-2019          40000 non-null int64\n",
            "sale_dollars_trans          40000 non-null float64\n",
            "cost_per_liter              40000 non-null float64\n",
            "cost_per_liter_trans        40000 non-null float64\n",
            "state_bottle_cost_trans     40000 non-null float64\n",
            "bottles_sold_trans          40000 non-null float64\n",
            "volume_sold_liters_trans    40000 non-null float64\n",
            "grossmargin                 40000 non-null float64\n",
            "dtypes: float64(12), int64(52), object(4)\n",
            "memory usage: 20.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u55a530V-H5n"
      },
      "source": [
        "Here you can see that we have 400,000 non null objects one-hot encoded to include month, year, store name and alcohol type. For our previous classification analysis, we dropped the liquor category and all the liquor types and just used the binary classification for Whiskey and Non-Whiskey. With 1 representing whether the liquor type was whiskey and 0 representing a non whiskey. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tv3q-5jl_gi3"
      },
      "source": [
        "### Project Requirements\n",
        "\n",
        "####Data Preparation (15 points total)\n",
        "\n",
        "• Define and prepare your class variables. Use proper variable\n",
        "representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for\n",
        "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for\n",
        "the analysis.\n",
        "\n",
        "• Describe the final dataset that is used for classification/regression (include a\n",
        "description of any newly formed variables you created)\n",
        "\n",
        "####Modeling and Evaluation (70 points total)\n",
        "\n",
        "• Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
        "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s)\n",
        "appropriate for analyzing the results of your modeling? Give a detailed explanation\n",
        "backing up any assertions.\n",
        "\n",
        "• Choose the method you will use for dividing your data into training and\n",
        "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
        "your chosen method is appropriate or use more than one method as appropriate.\n",
        "\n",
        "• Create three different classification/regression models for each task  (e.g., random forest,\n",
        "KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or\n",
        "logistic regression). Adjust parameters as appropriate to increase generalization\n",
        "performance using your chosen metric.\n",
        "\n",
        "• Analyze the results using your chosen method of evaluation. Use\n",
        "visualizations of the results to bolster the analysis. Explain any visuals and analyze why\n",
        "they are interesting to someone that might use this model.\n",
        "\n",
        "• Discuss the advantages of each model for each classification task, if any. If\n",
        "there are not advantages, explain why. Is any model better than another? Is the\n",
        "difference significant with 95% confidence? Use proper statistical comparison methods.\n",
        "\n",
        "• Which attributes from your analysis are most important? Use proper\n",
        "methods discussed in class to evaluate the importance of different attributes. Discuss\n",
        "the results and hypothesize about why certain attributes are more important than others\n",
        "for a given classification task.\n",
        "\n",
        "\n",
        "####Deployment \n",
        "\n",
        "• How useful is your model for interested parties (i.e., the companies or\n",
        "organizations that might want to use it for prediction)? How would you measure the\n",
        "model's value if it was used by these parties? How would your deploy your model for\n",
        "interested parties? What other data should be collected? How often would the model\n",
        "need to be updated, etc.? \n",
        "\n",
        "\n",
        "####Exceptional Work \n",
        "• You have free reign to provide additional modeling.\n",
        "\n",
        "• One idea: grid search parameters in a parallelized fashion and visualize the\n",
        "performances across attributes. Which parameters are most significant for making a\n",
        "good model for each classification algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Pw8tu_zAi0Y"
      },
      "source": [
        "## Data Preparation Part 1\n",
        "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
        "\n",
        "We obtained a dataset on Iowa Liquor Sales from Google Bigquery. The aim of recording the data set is for the state to track alcohol and liquor sales from wholesalers and retailers from the year 2012 through 2019. You can see some Exploratory Data Analysis of this dataset below: \n",
        "\n",
        "https://github.com/jjschueder/7331DataMiningNotebooks/blob/master/lab1/msds7331_clark_schueder_vela_washburn.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZSxAGB4B6nLq",
        "outputId": "c7fe460a-2d1b-4e65-a2fb-233cb2f8bcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Import all necessary libraries we will be using in our dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import re\n",
        "import sklearn\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE, SelectFromModel\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, roc_curve\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor,AdaBoostClassifier,RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(data.shape)\n",
        "data.head()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 68)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pack</th>\n",
              "      <th>bottle_volume_ml</th>\n",
              "      <th>state_bottle_cost</th>\n",
              "      <th>state_bottle_retail</th>\n",
              "      <th>bottles_sold</th>\n",
              "      <th>sale_dollars</th>\n",
              "      <th>volume_sold_liters</th>\n",
              "      <th>volume_sold_gallons</th>\n",
              "      <th>counter</th>\n",
              "      <th>liquor_category</th>\n",
              "      <th>store_parent</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>monthyear</th>\n",
              "      <th>liquor_category_AMARETTO</th>\n",
              "      <th>liquor_category_BRANDY</th>\n",
              "      <th>liquor_category_GIN</th>\n",
              "      <th>liquor_category_LIQUEUR</th>\n",
              "      <th>liquor_category_Other</th>\n",
              "      <th>liquor_category_RUM</th>\n",
              "      <th>liquor_category_SCHNAPPS</th>\n",
              "      <th>liquor_category_TEQUILA</th>\n",
              "      <th>liquor_category_VODKA</th>\n",
              "      <th>liquor_category_WHISKY</th>\n",
              "      <th>store_parent_CVS</th>\n",
              "      <th>store_parent_Caseys</th>\n",
              "      <th>store_parent_Hy-Vee</th>\n",
              "      <th>store_parent_Kum&amp;Go</th>\n",
              "      <th>store_parent_Other</th>\n",
              "      <th>store_parent_QuikTrip</th>\n",
              "      <th>store_parent_SamsClub</th>\n",
              "      <th>store_parent_SmokingJoes</th>\n",
              "      <th>store_parent_Target</th>\n",
              "      <th>store_parent_Wal-Mart</th>\n",
              "      <th>store_parent_Walgreens</th>\n",
              "      <th>month_Apr</th>\n",
              "      <th>month_Aug</th>\n",
              "      <th>month_Dec</th>\n",
              "      <th>month_Feb</th>\n",
              "      <th>month_Jan</th>\n",
              "      <th>month_Jul</th>\n",
              "      <th>month_Jun</th>\n",
              "      <th>month_Mar</th>\n",
              "      <th>month_May</th>\n",
              "      <th>month_Nov</th>\n",
              "      <th>month_Oct</th>\n",
              "      <th>month_Sep</th>\n",
              "      <th>year_2019</th>\n",
              "      <th>monthyear_Apr-2019</th>\n",
              "      <th>monthyear_Aug-2019</th>\n",
              "      <th>monthyear_Dec-2019</th>\n",
              "      <th>monthyear_Feb-2019</th>\n",
              "      <th>monthyear_Jan-2019</th>\n",
              "      <th>monthyear_Jul-2019</th>\n",
              "      <th>monthyear_Jun-2019</th>\n",
              "      <th>monthyear_Mar-2019</th>\n",
              "      <th>monthyear_May-2019</th>\n",
              "      <th>monthyear_Nov-2019</th>\n",
              "      <th>monthyear_Oct-2019</th>\n",
              "      <th>monthyear_Sep-2019</th>\n",
              "      <th>sale_dollars_trans</th>\n",
              "      <th>cost_per_liter</th>\n",
              "      <th>cost_per_liter_trans</th>\n",
              "      <th>state_bottle_cost_trans</th>\n",
              "      <th>bottles_sold_trans</th>\n",
              "      <th>volume_sold_liters_trans</th>\n",
              "      <th>grossmargin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>375</td>\n",
              "      <td>3.85</td>\n",
              "      <td>5.78</td>\n",
              "      <td>20</td>\n",
              "      <td>115.60</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1</td>\n",
              "      <td>Other</td>\n",
              "      <td>Other</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2019</td>\n",
              "      <td>Nov-19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.750136</td>\n",
              "      <td>15.413333</td>\n",
              "      <td>2.735233</td>\n",
              "      <td>1.348073</td>\n",
              "      <td>2.995732</td>\n",
              "      <td>2.014903</td>\n",
              "      <td>0.333910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>50</td>\n",
              "      <td>8.75</td>\n",
              "      <td>13.13</td>\n",
              "      <td>1</td>\n",
              "      <td>13.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1</td>\n",
              "      <td>Other</td>\n",
              "      <td>Other</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2019</td>\n",
              "      <td>Nov-19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.574900</td>\n",
              "      <td>262.600000</td>\n",
              "      <td>5.570632</td>\n",
              "      <td>2.169054</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.995732</td>\n",
              "      <td>0.333587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>1000</td>\n",
              "      <td>16.50</td>\n",
              "      <td>24.75</td>\n",
              "      <td>6</td>\n",
              "      <td>148.50</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.58</td>\n",
              "      <td>1</td>\n",
              "      <td>GIN</td>\n",
              "      <td>Hy-Vee</td>\n",
              "      <td>May</td>\n",
              "      <td>2019</td>\n",
              "      <td>May-19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000585</td>\n",
              "      <td>24.750000</td>\n",
              "      <td>3.208825</td>\n",
              "      <td>2.803360</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>750</td>\n",
              "      <td>21.17</td>\n",
              "      <td>31.76</td>\n",
              "      <td>24</td>\n",
              "      <td>762.24</td>\n",
              "      <td>18.00</td>\n",
              "      <td>4.75</td>\n",
              "      <td>1</td>\n",
              "      <td>WHISKY</td>\n",
              "      <td>Hy-Vee</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2019</td>\n",
              "      <td>Nov-19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.636261</td>\n",
              "      <td>42.346667</td>\n",
              "      <td>3.745890</td>\n",
              "      <td>3.052585</td>\n",
              "      <td>3.178054</td>\n",
              "      <td>2.890372</td>\n",
              "      <td>0.333438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1750</td>\n",
              "      <td>9.31</td>\n",
              "      <td>13.97</td>\n",
              "      <td>12</td>\n",
              "      <td>167.64</td>\n",
              "      <td>21.00</td>\n",
              "      <td>5.54</td>\n",
              "      <td>1</td>\n",
              "      <td>Other</td>\n",
              "      <td>Hy-Vee</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2019</td>\n",
              "      <td>Nov-19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.121819</td>\n",
              "      <td>7.982857</td>\n",
              "      <td>2.077296</td>\n",
              "      <td>2.231089</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>0.333572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  pack  ...  volume_sold_liters_trans  grossmargin\n",
              "0           0    20  ...                  2.014903     0.333910\n",
              "1           1     8  ...                 -2.995732     0.333587\n",
              "2           2    12  ...                  1.791759     0.333333\n",
              "3           3     6  ...                  2.890372     0.333438\n",
              "4           4     6  ...                  3.044522     0.333572\n",
              "\n",
              "[5 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DUW_wNlNCTll"
      },
      "source": [
        "**Our data Preparation includes the folloiwng layout**\n",
        "\n",
        "*   Since we are predicting our liquor category type (opening to the entire list of categories and not just one), we can assign them a numerical value.\n",
        "* Create new variables using aggredate data on profit, total cost and revenue\n",
        "*   Since we transformed a number of continuous variables, we can drop them so that we are working directly on our normalized data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOjzppNpB_MF",
        "outputId": "adb30d05-f81e-4b4d-9f65-a4bca70b3362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Since we are predicting our liquor category type (opening to the entire list of categories and not just one), we can assign them a numerical value.\n",
        "\n",
        "print (data['liquor_category'].unique())\n",
        "\n",
        "\"\"\"df = pd.DataFrame({'col_1':[133,255,36,477,55,63]})\n",
        "d = {'1':'M', '2': 'C', '3':'a', '4':'f', '5':'r', '6':'s'}\n",
        "def ifef(col):\n",
        "    col = str(col)\n",
        "    return d[col[0]]\n",
        "\n",
        "df['id_label'] = df['col_1'].apply(ifef)\n",
        "print(df)\"\"\"\n",
        "\n",
        "d = {'O':'1', 'G': '2', 'W':'3', 'T':'4', 'L':'5', 'V':'6', 'R':'7', 'S': '8', 'A':'9', 'B':'10'}\n",
        "\n",
        "#data['id_label'] = data['liquor_category'].apply(ifef)\n",
        "\n",
        "data['id_label'] = data['liquor_category'].astype(str).str[0].map(d)\n",
        "\n",
        "print(data.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Other' 'GIN' 'WHISKY' 'TEQUILA' 'LIQUEUR' 'VODKA' 'RUM' 'SCHNAPPS'\n",
            " 'AMARETTO' 'BRANDY']\n",
            "   Unnamed: 0  pack  ...  grossmargin  id_label\n",
            "0           0    20  ...     0.333910         1\n",
            "1           1     8  ...     0.333587         1\n",
            "2           2    12  ...     0.333333         2\n",
            "3           3     6  ...     0.333438         3\n",
            "4           4     6  ...     0.333572         1\n",
            "\n",
            "[5 rows x 69 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCWpktQMHy3P",
        "colab": {}
      },
      "source": [
        "# Create new variables using aggredate data on profit, total cost and revenue\n",
        "\n",
        "#do some calculations for cost and profit\n",
        "data['profit'] = data['state_bottle_retail']*data['bottles_sold'] - data['state_bottle_cost']* data['bottles_sold']\n",
        "data['profit_trans']= np.log(data['profit'])\n",
        "\n",
        "data['totalcost'] = data['state_bottle_cost']* data['bottles_sold']\n",
        "data['totalcost_trans']= np.log(data['totalcost'])\n",
        "\n",
        "data['revenue'] = data['state_bottle_retail']*data['bottles_sold']\n",
        "data['revenue_trans']= np.log(data['revenue'])\n",
        "\n",
        "data['bottle_volume_ml_trans']= np.log(data['bottle_volume_ml'])\n",
        "\n",
        "data['pack_trans']= np.log(data['pack'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MIDsTP_TF4mD",
        "outputId": "95ac2568-73e8-4130-bb8f-c4d6f6e5aa4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "# Since we transformed a number of continuous variables, we can drop them so that we are working directly on our normalized data\n",
        "\n",
        "\n",
        "# Remove unwanted columns, which include all the specific liquor categories, \n",
        "# except for liquor_category_WHISKY since that is what we want to classify on, along\n",
        "# with all the store_ attributes\n",
        "\"\"\"\n",
        "cat_vars=['counter', 'liquor_category', 'store_parent',\n",
        " 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', \n",
        " 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', \n",
        " 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb',\n",
        " 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', \n",
        " 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', \n",
        " 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', \n",
        " 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens']\n",
        "data_vars=data.columns.values.tolist()\n",
        "to_keep=[i for i in data_vars if i not in cat_vars]\n",
        "\"\"\"\n",
        "#keep our transformed detail, along with the timing and store name detail\n",
        "\n",
        "to_keep=['sale_dollars_trans', 'cost_per_liter_trans', 'store_parent',\n",
        "      'state_bottle_cost_trans', 'bottles_sold_trans',\n",
        "       'volume_sold_liters_trans','pack_trans', 'bottle_volume_ml_trans', \n",
        "       'profit_trans', 'totalcost_trans', 'revenue_trans', 'id_label']\n",
        "data_final=data[to_keep]\n",
        "data_final.columns.values\n",
        "\n",
        "#boxplot of all the variables\n",
        "plt.figure(figsize=(15, 15))\n",
        "ax = data_final.boxplot()\n",
        "#ax.set_yscale('log')\n",
        "\n",
        "print(data_final.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAANPCAYAAABAfZeNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSCOEEEIIIYSQZYQGaYQQQgghhBCyjPwPnNd2vDrBWZEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lTcms3WzKFLT"
      },
      "source": [
        "From the standpoint of the boxplots, we can see that we have some uniformity in our continuous features with minimal outliers. To help us accurately predict our liquor categires, this will be helpful in us using these features build versious models to compare performance. Running a quick shape feature, we can see that we are not missing any data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D6CO8gBkJr7R",
        "outputId": "5e5d7999-18f1-4242-c1cd-534468bff9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "data_final.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 12 columns):\n",
            "sale_dollars_trans          40000 non-null float64\n",
            "cost_per_liter_trans        40000 non-null float64\n",
            "store_parent                40000 non-null object\n",
            "state_bottle_cost_trans     40000 non-null float64\n",
            "bottles_sold_trans          40000 non-null float64\n",
            "volume_sold_liters_trans    40000 non-null float64\n",
            "pack_trans                  40000 non-null float64\n",
            "bottle_volume_ml_trans      40000 non-null float64\n",
            "profit_trans                40000 non-null float64\n",
            "totalcost_trans             40000 non-null float64\n",
            "revenue_trans               40000 non-null float64\n",
            "id_label                    40000 non-null object\n",
            "dtypes: float64(10), object(2)\n",
            "memory usage: 3.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PMpjgnr2LXgv"
      },
      "source": [
        "Running a quick df describe, we can se that almost all of our variables are floats to account for the transformations. With the id_label and store parent being an object features used as our classifiers we are looking to use in our datasets. The last feature pack is represented as an int, which makes sense because they are typically sold in packs of 6, 12, 24 and 48."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8iGxO4LFQc6y"
      },
      "source": [
        "## Data Preparation Part 2\n",
        "\n",
        "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7I6d0XULJ83",
        "outputId": "854d5530-e542-4cf1-f59b-ca47e0cee40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "data_final.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sale_dollars_trans</th>\n",
              "      <th>cost_per_liter_trans</th>\n",
              "      <th>state_bottle_cost_trans</th>\n",
              "      <th>bottles_sold_trans</th>\n",
              "      <th>volume_sold_liters_trans</th>\n",
              "      <th>pack_trans</th>\n",
              "      <th>bottle_volume_ml_trans</th>\n",
              "      <th>profit_trans</th>\n",
              "      <th>totalcost_trans</th>\n",
              "      <th>revenue_trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "      <td>40000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.298454</td>\n",
              "      <td>2.971822</td>\n",
              "      <td>2.235238</td>\n",
              "      <td>1.657888</td>\n",
              "      <td>1.326632</td>\n",
              "      <td>2.334344</td>\n",
              "      <td>6.576622</td>\n",
              "      <td>3.200717</td>\n",
              "      <td>3.893126</td>\n",
              "      <td>4.298837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.127726</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>0.782898</td>\n",
              "      <td>1.106078</td>\n",
              "      <td>1.384640</td>\n",
              "      <td>0.525811</td>\n",
              "      <td>0.762413</td>\n",
              "      <td>1.127691</td>\n",
              "      <td>1.127849</td>\n",
              "      <td>1.127796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.943906</td>\n",
              "      <td>1.163151</td>\n",
              "      <td>-0.116534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.995732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.912023</td>\n",
              "      <td>-0.150823</td>\n",
              "      <td>0.536493</td>\n",
              "      <td>0.943906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.544432</td>\n",
              "      <td>2.345645</td>\n",
              "      <td>1.607436</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>6.620073</td>\n",
              "      <td>2.446685</td>\n",
              "      <td>3.138966</td>\n",
              "      <td>3.544432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.319087</td>\n",
              "      <td>2.987700</td>\n",
              "      <td>2.306577</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>1.504077</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>6.620073</td>\n",
              "      <td>3.222071</td>\n",
              "      <td>3.912823</td>\n",
              "      <td>4.319087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.050625</td>\n",
              "      <td>3.465319</td>\n",
              "      <td>2.839078</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>2.351375</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>6.907755</td>\n",
              "      <td>3.943522</td>\n",
              "      <td>4.634341</td>\n",
              "      <td>5.040582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.352874</td>\n",
              "      <td>7.495542</td>\n",
              "      <td>5.164729</td>\n",
              "      <td>7.901007</td>\n",
              "      <td>8.460623</td>\n",
              "      <td>3.871201</td>\n",
              "      <td>8.699515</td>\n",
              "      <td>9.254262</td>\n",
              "      <td>9.947409</td>\n",
              "      <td>10.352874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sale_dollars_trans  cost_per_liter_trans  ...  totalcost_trans  revenue_trans\n",
              "count        40000.000000          40000.000000  ...     40000.000000   40000.000000\n",
              "mean             4.298454              2.971822  ...         3.893126       4.298837\n",
              "std              1.127726              0.865337  ...         1.127849       1.127796\n",
              "min              0.943906              1.163151  ...         0.536493       0.943906\n",
              "25%              3.544432              2.345645  ...         3.138966       3.544432\n",
              "50%              4.319087              2.987700  ...         3.912823       4.319087\n",
              "75%              5.050625              3.465319  ...         4.634341       5.040582\n",
              "max             10.352874              7.495542  ...         9.947409      10.352874\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hRtcKC8UNBQf"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0Li0G5hMt0w",
        "outputId": "12c8364f-db27-4be6-97e0-c21baa572274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_final.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PFqUQ8VDQSd_"
      },
      "source": [
        "Our final data set includes 12 features and 400,000 entries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8KEzvBaYSb9",
        "outputId": "d5aaf4bc-8553-4d6a-950b-8f9a523523fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "(data_final['store_parent'].value_counts()/len(data_final))*100"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Other          46.6750\n",
              "Hy-Vee         39.0950\n",
              "Wal-Mart        4.3475\n",
              "Caseys          2.3775\n",
              "SamsClub        1.9700\n",
              "SmokingJoes     1.5275\n",
              "Kum&Go          1.2725\n",
              "QuikTrip        1.2575\n",
              "Target          0.8375\n",
              "Walgreens       0.4175\n",
              "CVS             0.2225\n",
              "Name: store_parent, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2LIhw3YKHcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics as mt\n",
        "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vlmmpWgzQfQC"
      },
      "source": [
        "## Modeling and Evaluation 1\n",
        "\n",
        "Within this notebook, we have built multiple models to evaluate two classification problems. The first, is a multi-classification problem in which our goal is to correctly classify the alcohol category based on sales data. The second, is a binary classification to determine whether the alcohol sold is from a Hy-vee store vs a Non-Hy-vee store. A detailed description of the metrics we will use to evaluate the models are noted below.\n",
        "\n",
        "An ROC curve, also known as a Reciever Operation Characteristic Curve is a plot that calculates and can be used to compare the classifiers we will use in our modeling along with the true positive rate (TPR) and false positive rate (FPR) they convey. From a topline level, you can use the Area Under the Curve (AUC) to help detirmine which can better pair an observation with the correct liquor cateogry. An AUC score of 1.0 denotes a perfect classifier and an area of 0.5 represents a model is no better than a random guess.\n",
        "\n",
        "In addition to the ROC-AUC value we will use to evaluate classifiers, we will be using other evaluators as well. \n",
        "\n",
        "**Accuracy** - Accuracy is the total number of correct predictions over the total number of predictions made. Accuracy will be plotted in our AUC curve and, while not perfect, is a good singlular measure to evaluate a model. Just be careful that we aren't overclassifying unbalalnced variables. \n",
        "\n",
        "Accuracy = (TP + TN)/(TP + FP + FN + TN)\n",
        "\n",
        "**Precision** - Precision is the Proportions of true positives over the total number of positive outcomes whether accurately predicted or inaccurately predicted. \n",
        "\n",
        "Precision = (TP) / (TP + FP)\n",
        "\n",
        "**Recall** - Also known as sensitivity, recall is the proportion of positive outcomes that were correctly classified by our model. Essentially it tells us how many values we incorrectly predicted while the precision can tell us more about what we correctly classified. \n",
        "\n",
        "Recall/Sensitivity = (TP) / (TP + FN)\n",
        "\n",
        "The aim of our modeling is to focus on maximizing our precision, recall, and accuracy scores in our models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWp5arOWqMM"
      },
      "source": [
        "## Modeling and Evaluation 2\n",
        "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
        "\n",
        "For our first classification problem, we identified an unbalanced distribution of alcohol categories within our dataset. For example, Vodka is 38% of sales while Rum and Whiskey is 24% and 10% respectively. To address this concern, we used a synthetic minority oversampling technique (SMOTE). The SMOTE technique accounts for the minority classes withon the data by creating new synthetic instances similar to these minorities. In other words, once SMOTE is used on the data we should have a balanced distribution between the different alcohol categories.\n",
        "\n",
        "Futhermore, to split our test & training data, we used a stratified k-fold cross validation method. The number of splits used was 10, and our ratio of train vs test size was 80% to 20%.\n",
        "\n",
        "The analysis and work performed regarding the SMOTE and CV method for Classification 1 is noted below.\n",
        "\n",
        "NOTE: Details regarding the training and test splits for the second classification problem can be found within the Classification 2 (Store Data) section.\n",
        "\n",
        "So we can begin to set a baseline for our accuracy score, let's do a quick look at the value counts and it's percent distribution of our liquor categories, the ones we are looking to classify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BlP7r4nNQPhY",
        "outputId": "b4537bc7-9fbc-4336-934d-024287bf03d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#class percentage split\n",
        "dfgrp = data.groupby(['id_label','liquor_category'])\n",
        "data_final['id_label'].value_counts()\n",
        "dfgrp['counter'].count() / len(data_final) *100"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id_label  liquor_category\n",
              "1         Other               5.9650\n",
              "10        BRANDY              0.0075\n",
              "2         GIN                 4.2900\n",
              "3         WHISKY             22.6125\n",
              "4         TEQUILA             7.1325\n",
              "5         LIQUEUR            13.4000\n",
              "6         VODKA              32.7700\n",
              "7         RUM                13.7875\n",
              "8         SCHNAPPS            0.0050\n",
              "9         AMARETTO            0.0300\n",
              "Name: counter, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yU3ttrh3QrDV",
        "outputId": "c51b1884-45bf-4ab2-ba71-f847d5accb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "data_final['id_label'].value_counts()\n",
        "dfgrp['counter'].count()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id_label  liquor_category\n",
              "1         Other               2386\n",
              "10        BRANDY                 3\n",
              "2         GIN                 1716\n",
              "3         WHISKY              9045\n",
              "4         TEQUILA             2853\n",
              "5         LIQUEUR             5360\n",
              "6         VODKA              13108\n",
              "7         RUM                 5515\n",
              "8         SCHNAPPS               2\n",
              "9         AMARETTO              12\n",
              "Name: counter, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6hYMIlsdU12R",
        "outputId": "659d9e16-56a3-412b-8e95-5841c12a54fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "data_final = data_final[data_final['id_label'] < \"8\"]\n",
        "data_final = data_final[data_final['id_label'] != \"10\"]\n",
        "data_final['id_label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6    13108\n",
              "3     9045\n",
              "7     5515\n",
              "5     5360\n",
              "4     2853\n",
              "1     2386\n",
              "2     1716\n",
              "Name: id_label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qp6JLDveRRFK"
      },
      "source": [
        "So that our model is able to run correctly without having to create a large amount of additional variables, we decided to drop the Amaretto, Schnapps and Brandy values since collectively, those represent 17 observations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "diJ1PTKiR_m7",
        "outputId": "7fad1245-c60b-4e6e-f0d6-1aff3872a17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_final.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39983, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Sr3tFa7XAAE"
      },
      "source": [
        "Looking at the distribution of liquors in our data_final dataset, we can see that vodka takes up 38% of the liquor sales in our set, followed by Run at 24%, Liquer at 12%, Whisky at 10% and so on. When we run our classifier, we will need to be careful to account for the unbalanced nature of our classifiers to help ensure we aren't overclassifying the majority labels. \n",
        "\n",
        "For our data training, we will need to employ synthetic minority oversampling technique (SMOTE), which is designed to subset our data from the minority classes as an example with new synthetic instances similar to these minorities are created and added to a new data set which we will use as a train to our classification models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jriu2c-MZ7SP"
      },
      "source": [
        "## Classification Task 1 (Liquor Type)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y713JBi1Mage"
      },
      "source": [
        "### Task 1 - Modeling and Evaluation 3\n",
        "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!  \n",
        "\n",
        "As previously mentioned within a section above, our goal of this classification problem is to correctly classify the alcohol category based on sales data.  We tried 3 different algorithms within our models and compared them using our evaluation metrics.  The algorithms used are as follows:\n",
        "\n",
        "*   K-Nearest Neighbor\n",
        "*   Random Forest Classification\n",
        "*   Logistic Regression\n",
        "\n",
        "A ROC/AUC plot will be created for each model and summarized our findings based on the combined results for these different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M4KqmxPVd3rj"
      },
      "source": [
        "Now that we built our cross validation, we can go ahead and start assinging our X's and Ys that we will classify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEuVb3hkcGeU",
        "outputId": "26541a95-f086-46a9-b491-2e892c207355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn import metrics as mt\n",
        "\n",
        "features = ['sale_dollars_trans', 'cost_per_liter_trans', \n",
        "       'state_bottle_cost_trans', 'bottles_sold_trans',\n",
        "       'volume_sold_liters_trans', 'pack_trans', 'bottle_volume_ml_trans',\n",
        "       'profit_trans', 'totalcost_trans', 'revenue_trans']\n",
        "\n",
        "X2 = data_final[features].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X2)\n",
        "\n",
        "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
        "#Notice we scaled the data before the cross validation\n",
        "X = scaler.transform(X2)\n",
        "\n",
        "Y= data_final[['id_label']].copy()\n",
        "Y2 = Y.values"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.1 ms, sys: 2.2 ms, total: 16.3 ms\n",
            "Wall time: 20.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hvike5CQTweb",
        "outputId": "c4b36f7d-22d2-42e0-f07f-42d1efb6b364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.40059766, -0.27331513, -1.13290409, ...,  0.40136648,\n",
              "         0.39966524,  0.40023281],\n",
              "       [-1.52798177,  3.0029126 , -0.08440289, ..., -1.52813006,\n",
              "        -1.5282738 , -1.52822628],\n",
              "       [ 0.62264747,  0.2739084 ,  0.72569099, ...,  0.6218905 ,\n",
              "         0.62245798,  0.62226876],\n",
              "       ...,\n",
              "       [-1.19383675,  2.16897105, -1.00666483, ..., -1.19326113,\n",
              "        -1.19452269, -1.19410211],\n",
              "       [ 0.43371756, -0.04688765, -1.68434759, ...,  0.43295476,\n",
              "         0.43354872,  0.43335064],\n",
              "       [ 2.07674914,  2.16897105, -1.00666483, ...,  2.07742563,\n",
              "         2.07570565,  2.0762797 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sM8OFT5wfYWX"
      },
      "source": [
        "#### Evaluation Model: SMOTE and CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cHwQiga_2Plg",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics as mt\n",
        "\n",
        "# train and test split before resampling\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y, test_size = 0.2, random_state = 101) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LOvSSzz0eelA",
        "outputId": "6a0aa9cb-584d-44ac-81a5-948de14da575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "\n",
        "print(\"Before OverSampling, counts of label 'Other': {}\".format(sum(y1_train['id_label'] == \"1\")))\n",
        "print(\"Before OverSampling, counts of label 'GIN': {} \\n\".format(sum(y1_train['id_label'] == \"2\"))) \n",
        "print(\"Before OverSampling, counts of label 'WHISKY': {} \\n\".format(sum(y1_train['id_label'] == \"3\"))) \n",
        "print(\"Before OverSampling, counts of label 'TEQUILA': {}\".format(sum(y1_train['id_label'] == \"4\")))\n",
        "print(\"Before OverSampling, counts of label 'LIQUEUR': {} \\n\".format(sum(y1_train['id_label'] == \"5\"))) \n",
        "print(\"Before OverSampling, counts of label 'VODKA': {} \\n\".format(sum(y1_train['id_label'] == \"6\"))) \n",
        "print(\"Before OverSampling, counts of label 'RUM': {} \\n\".format(sum(y1_train['id_label'] == \"7\"))) \n",
        "\n",
        "\n",
        "# import SMOTE module from imblearn library \n",
        "# pip install imblearn (if you don't have imblearn in your system) \n",
        "from imblearn.over_sampling import SMOTE \n",
        "sm = SMOTE(random_state = 2) \n",
        "X1_train_res, y1_train_res = sm.fit_sample(X1_train, y1_train.values.ravel()) \n",
        "  \n",
        "print('After OverSampling, the shape of train_X: {}'.format(X1_train_res.shape)) \n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y1_train_res.shape)) \n",
        "  \n",
        "print(\"After OverSampling, counts of label 'Other': {}\".format(sum(y1_train_res == \"1\"))) \n",
        "print(\"After OverSampling, counts of label 'GIN': {}\".format(sum(y1_train_res == \"2\"))) \n",
        "print(\"After OverSampling, counts of label 'WHISKY': {}\".format(sum(y1_train_res == \"3\"))) \n",
        "print(\"After OverSampling, counts of label 'TEQUILA': {}\".format(sum(y1_train_res == \"4\"))) \n",
        "print(\"After OverSampling, counts of label 'LIQUEUR': {}\".format(sum(y1_train_res == \"5\"))) \n",
        "print(\"After OverSampling, counts of label 'VODKA': {}\".format(sum(y1_train_res == \"6\"))) \n",
        "print(\"After OverSampling, counts of label 'RUM': {}\".format(sum(y1_train_res == \"7\"))) \n",
        " \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before OverSampling, counts of label 'Other': 1902\n",
            "Before OverSampling, counts of label 'GIN': 1374 \n",
            "\n",
            "Before OverSampling, counts of label 'WHISKY': 7242 \n",
            "\n",
            "Before OverSampling, counts of label 'TEQUILA': 2289\n",
            "Before OverSampling, counts of label 'LIQUEUR': 4275 \n",
            "\n",
            "Before OverSampling, counts of label 'VODKA': 10504 \n",
            "\n",
            "Before OverSampling, counts of label 'RUM': 4400 \n",
            "\n",
            "After OverSampling, the shape of train_X: (73528, 10)\n",
            "After OverSampling, the shape of train_y: (73528,) \n",
            "\n",
            "After OverSampling, counts of label 'Other': 10504\n",
            "After OverSampling, counts of label 'GIN': 10504\n",
            "After OverSampling, counts of label 'WHISKY': 10504\n",
            "After OverSampling, counts of label 'TEQUILA': 10504\n",
            "After OverSampling, counts of label 'LIQUEUR': 10504\n",
            "After OverSampling, counts of label 'VODKA': 10504\n",
            "After OverSampling, counts of label 'RUM': 10504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "it9ofY8j4Xi0"
      },
      "source": [
        "#### Model 1: KNN Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ8Q8uXxIHeC",
        "colab_type": "text"
      },
      "source": [
        "Details regarding this model are noted in the code below, but to quickly summarize, this model has an average accuracy score of 0.58 and a Precision, Recall, & F1 Score of 0.91, 0.91, & 0.91 respectively. After evaluation this is not our best fitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-xo_Rm-fe2j",
        "outputId": "1ab3c8dc-3f27-40d4-9e66-ff0f90dfc876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "\n",
        "param_grid = [\n",
        "    {\n",
        "         'weights': ['uniform','distance'],\n",
        "         'leaf_size': [5,10],\n",
        "         'metric': ['minkowski','euclidean'],\n",
        "         'n_neighbors':[2,3,5],\n",
        "         \n",
        "    }\n",
        "]\n",
        "\n",
        "clf_KNN = KNeighborsClassifier()\n",
        "grid_search_KNN = GridSearchCV(clf_KNN, param_grid=param_grid,cv=cv,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
        "\n",
        "KNearest_model1 = grid_search_KNN.fit(X1_train_res, y1_train_res)\n",
        "\n",
        "y_KNN_score1 = grid_search_KNN.predict(X1_test)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e2b163f2743f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mclf_KNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgrid_search_KNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_KNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mKNearest_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_KNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xXbdtmoF4itK",
        "colab": {}
      },
      "source": [
        "classifierEstimaterKNN1 = KNearest_model1.best_estimator_\n",
        "classifierEstimaterKNN1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NA-9z-pWZ2nX",
        "colab": {}
      },
      "source": [
        "#Source: https://github.com/jakemdrew/EducationDataNC/blob/master/2017/Models/2017ComparingSegregatedMiddleSchoolCampuses.ipynb\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "\n",
        "def EvaluateClassifierEstimator(classifierEstimator, X, Y2, cv):\n",
        "\n",
        "\n",
        "\n",
        "#Perform cross validation \n",
        "    scores = cross_validate(classifierEstimator, X, Y2,\n",
        "                            scoring=['accuracy', 'precision','recall'], \n",
        "                            cv=cv, return_train_score=True, \n",
        "                            )\n",
        "\n",
        "    Accavg = scores['test_accuracy'].mean()\n",
        "    Preavg = scores['test_precision'].mean()\n",
        "    Rreavg = scores['test_recall'].mean()\n",
        "\n",
        "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
        "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
        "    print_str3 = \"The average Recall for all cv folds is: \\t\\t\\t {Rreavg:.5}\"\n",
        "\n",
        "    print(print_str.format(Accavg=Accavg))\n",
        "    print(print_str2.format(Preavg=Preavg))\n",
        "    print(print_str3.format(Rreavg=Rreavg))\n",
        "    print('*********************************************************')\n",
        "\n",
        "    print('Cross Validation Fold Mean Error Scores')\n",
        "    scoresResults = pd.DataFrame()\n",
        "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
        "    scoresResults['Precision'] = scores['test_precision']\n",
        "    scoresResults['Recall'] = scores['test_recall']\n",
        "    print(scoresResults)\n",
        "    return scoresResults\n",
        "\n",
        "def EvaluateClassifierEstimator2(classifierEstimator, X, Y2, cv):\n",
        "    \n",
        "  #Perform cross validation \n",
        "  from sklearn.model_selection import cross_val_predict\n",
        "  predictions = cross_val_predict(classifierEstimator, \n",
        "                                  X, Y2, cv=cv)\n",
        "    \n",
        "#model evaluation \n",
        "  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "#pass true test set values and predictions to classification_report\n",
        "  classReport = classification_report(Y2,predictions)\n",
        "  confMat = confusion_matrix(Y2,predictions)\n",
        "  acc = accuracy_score(Y2,predictions)\n",
        "    \n",
        "  print(classReport)\n",
        "  print(confMat)\n",
        "  print(acc)\n",
        "    \n",
        "def EvaluateClassifierEstimator3(classifierEstimator, X, Y2, cv):\n",
        "  from sklearn import metrics as mt\n",
        "  for fold, (train_index, test_index) in enumerate(cv.split(X,Y2)):\n",
        "     X_train = X[train_index]    \n",
        "     y_train = Y2[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
        "     X_test = X[test_index]\n",
        "     y_test = Y2[test_index]  # See comment on ravel and  y_train\n",
        "     sm = SMOTE(random_state=101)\n",
        "     X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n",
        "     classifierEstimator.fit(X_train, y_train) \n",
        "     y_pred = classifierEstimator.predict(X_test)\n",
        "     acc = mt.accuracy_score(y_test,y_pred)\n",
        "     conf = mt.confusion_matrix(y_test,y_pred)\n",
        "     print(\"====Iteration\",fold,\" ====\")\n",
        "     print(\"accuracy\", acc )\n",
        "     print(\"confusion matrix\\n\",conf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FSI_M3-yIQ27",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Precision Score is: {}\" .format(precision_score(y1_test, y_KNN_score1, average='weighted')))\n",
        "print(\"Recall Score is: {}\" .format(recall_score(y1_test, y_KNN_score1, average='weighted')))\n",
        "print(\"F1 Score is: {}\" .format(f1_score(y1_test, y_KNN_score1, average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iS33r9IdE34T",
        "colab": {}
      },
      "source": [
        "classifierEstimaterKNN1.fit(X1_train_res,y1_train_res)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xDq0kA306IFd",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator3(classifierEstimaterKNN1, X, Y2, cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BG-rLwzQcuWm",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "KNN_accuracy = cross_val_score(classifierEstimaterKNN1, X, y=Y2, cv=cv)\n",
        "KNN_acc=KNN_accuracy.mean()\n",
        "print('Average KNN accuracy score is : {}' .format(KNN_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OlCXIFl9oaYm",
        "colab": {}
      },
      "source": [
        "##not sure about this one?\n",
        "#from sklearn.metrics import fbeta_score, make_scorer\n",
        "#nested_score = cross_val_score(classifierEstimaterKNN1, X=X, y=Y2, cv=cv, \\\n",
        "#               scoring=make_scorer(classification_report_with_accuracy_score))\n",
        "#print(nested_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZAPO787ThIhU",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "yhat= np.zeros(Y2.shape)\n",
        "\n",
        "def per_class_accuracy(ytrue,yhat):\n",
        "    conf = mt.confusion_matrix(ytrue,yhat)\n",
        "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
        "    return np.diag(norm_conf)\n",
        "\n",
        "def plot_class_acc(ytrue,yhat, title=''):\n",
        "    acc_list = per_class_accuracy(ytrue,yhat)\n",
        "    plt.bar(range(len(acc_list)), acc_list)\n",
        "    plt.xlabel('Class value (one per category)')\n",
        "    plt.ylabel('Accuracy within class')\n",
        "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
        "    plt.grid()\n",
        "    plt.ylim([0,1])\n",
        "    plt.show()\n",
        "#yhat = classifierEstimaterKNN1.predict(X1_test)       \n",
        "#plot_class_acc(y1_test,yhat,title=\"KNN\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KDQJt2eN7HNh",
        "colab": {}
      },
      "source": [
        "totalacc = 0\n",
        "totalprec = 0\n",
        "totalrec= 0\n",
        "totalf1  = 0\n",
        "list = []\n",
        "for fold, (train_index, test_index) in enumerate(cv.split(X,Y2)):\n",
        "     X_train = X[train_index]    \n",
        "     y_train = Y2[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
        "     X_test = X[test_index]\n",
        "     y_test = Y2[test_index]  # See comment on ravel and  y_train\n",
        "     sm = SMOTE(random_state=101)\n",
        "     X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n",
        "     classifierEstimaterKNN1.fit(X_train, y_train) \n",
        "     y_pred = classifierEstimaterKNN1.predict(X_test)\n",
        "     acc = mt.accuracy_score(y_test,y_pred)\n",
        "     prec = precision_score(y_test, y_pred, average='weighted')\n",
        "     rec = recall_score(y_test, y_pred, average='weighted')\n",
        "     f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "     perclassaccuracy = per_class_accuracy(y_test,y_pred)\n",
        "     perclasslist.append(perclassaccuracy.tolist())\n",
        "     totalacc += acc\n",
        "     totalprec += prec\n",
        "     totalrec += rec\n",
        "     totalf1 += f1\n",
        "     conf = mt.confusion_matrix(y_test,y_pred)\n",
        "#     print(\"Sum: \", totalacc, \"Current: \", acc,\"Per Class\",perclassaccuracy,\"\\n\")\n",
        "\n",
        "\n",
        "avgaccuracy = 100*totalacc / cv.n_splits\n",
        "avgprec = 100*totalprec / cv.n_splits\n",
        "avgrec = 100*totalrec / cv.n_splits\n",
        "avgf1 = 100*totalf1 / cv.n_splits\n",
        " \n",
        "list = [(\"Model 1\", \"KNN\", avgaccuracy, avgprec, avgrec, avgf1)]\n",
        "\n",
        "perclassdf = []\n",
        "perclassdf = pd.DataFrame(perclasslist)\n",
        "#perclassdf = perclassdf.transpose()\n",
        "#perclassdf\n",
        "\n",
        "perclassavg = perclassdf.mean(axis = 0) \n",
        "#perclassavg\n",
        "\n",
        "plt.bar(range(len(perclassavg)), perclassavg)\n",
        "plt.xlabel('Class value (one per category)')\n",
        "plt.ylabel('Accuracy within class')\n",
        "#plt.title(title+\", Total Acc=%.1f\"%(totalaccuracy))\n",
        "plt.title(\"KNN Avg Acc=%.1f\"%(avgaccuracy))\n",
        "plt.grid()\n",
        "plt.ylim([0,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MIw99resH1Ug",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "ybinary = label_binarize(ylist, classes=[1, 2, 3, 4, 5, 6, 7])\n",
        "n_classes = ybinary.shape[1]\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X,ybinary, test_size=0.2) # 70% training and 30% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynORMZcNMKas",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Learn to predict each class against the other\n",
        "classifier = OneVsRestClassifier(KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='distance'))\n",
        "knnbinarymodel = classifier.fit(X_train3, y_train3)\n",
        "knnbinaryscore = knnbinarymodel.predict(X_test3)\n",
        "y_score = cross_val_predict(classifier, X, ybinary, cv=10 ,method='predict_proba')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqm9pUt4FUtI",
        "colab": {}
      },
      "source": [
        "#X1_train, X1_test, y1_train, y1_test\n",
        "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(ybinary[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "#Plot of a ROC curve for a specific class\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[3])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic Whiskey')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l_6zEVYZiV38",
        "colab": {}
      },
      "source": [
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fi35yamMtVOm"
      },
      "source": [
        "#### Model 2: Random Forest Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq-vpyhoIHfa",
        "colab_type": "text"
      },
      "source": [
        "Details regarding this model are noted in the code below, but to quickly summarize, this model has an average accuracy score of 0.67 and a Precision, Recall, & F1 Score of 0.93, 0.93, & 0.93 respectively. After evaluation this is our best fitting model and reasoning can be found within section Task 1 - Model and Evaluation 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9vHxo7itYVh",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "         'n_estimators': [200, 500], \n",
        "         'max_depth': [20,30,35],\n",
        "         'random_state':[101]\n",
        "     }\n",
        "]\n",
        "\n",
        "clf_RF = RandomForestClassifier()\n",
        "grid_search_RF = GridSearchCV(clf_RF, param_grid=param_grid, cv=cv,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
        "\n",
        "RandomForest_model1 = grid_search_RF.fit(X1_train_res, y1_train_res)\n",
        "\n",
        "y_RF_score1 = grid_search_RF.predict(X1_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uc_OTys6trdf",
        "colab": {}
      },
      "source": [
        "classifierEstimaterRF1 = RandomForest_model1.best_estimator_\n",
        "classifierEstimaterRF1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqqmrudFz0zl",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Precision Score is: {}\" .format(precision_score(y1_test, y_RF_score1, average='weighted')))\n",
        "print(\"Recall Score is: {}\" .format(recall_score(y1_test, y_RF_score1, average='weighted')))\n",
        "print(\"F1 Score is: {}\" .format(f1_score(y1_test, y_RF_score1, average='weighted')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UWS6npWuzZbt",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "EvaluateClassifierEstimator3(classifierEstimaterRF1, X, Y2, cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TLxsUe8hAvVH",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "RF_accuracy = cross_val_score(classifierEstimaterRF1, X, y=Y2, cv=cv)\n",
        "RF_acc=RF_accuracy.mean()\n",
        "print('Average RF accuracy score is : {}' .format(RF_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeHybbBQPejX",
        "colab": {}
      },
      "source": [
        "totalacc = 0\n",
        "totalprec = 0\n",
        "totalrec= 0\n",
        "totalf1  = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(cv.split(X,Y2)):\n",
        "     X_train = X[train_index]    \n",
        "     y_train = Y2[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
        "     X_test = X[test_index]\n",
        "     y_test = Y2[test_index]  # See comment on ravel and  y_train\n",
        "     sm = SMOTE(random_state=101)\n",
        "     X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n",
        "     classifierEstimaterRF1.fit(X_train, y_train) \n",
        "     y_pred = classifierEstimaterRF1.predict(X_test)\n",
        "     acc = mt.accuracy_score(y_test,y_pred)\n",
        "     prec = precision_score(y_test, y_pred, average='weighted')\n",
        "     rec = recall_score(y_test, y_pred, average='weighted')\n",
        "     f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "     perclassaccuracy = per_class_accuracy(y_test,y_pred)\n",
        "     perclasslist.append(perclassaccuracy.tolist())\n",
        "     totalacc += acc\n",
        "     totalprec += prec\n",
        "     totalrec += rec\n",
        "     totalf1 += f1\n",
        "     conf = mt.confusion_matrix(y_test,y_pred)\n",
        "#     print(\"Sum: \", totalacc, \"Current: \", acc,\"Per Class\",perclassaccuracy,\"\\n\")\n",
        "\n",
        "\n",
        "avgaccuracy = 100*totalacc / cv.n_splits\n",
        "avgprec = 100*totalprec / cv.n_splits\n",
        "avgrec = 100*totalrec / cv.n_splits\n",
        "avgf1 = 100*totalf1 / cv.n_splits\n",
        " \n",
        "\n",
        "list.append((\"Model 2\", \"Random Forest\", avgaccuracy, avgprec, avgrec, avgf1))\n",
        "\n",
        "perclassdf = []\n",
        "perclassdf = pd.DataFrame(perclasslist)\n",
        "#perclassdf = perclassdf.transpose()\n",
        "#perclassdf\n",
        "\n",
        "perclassavg = perclassdf.mean(axis = 0) \n",
        "#perclassavg\n",
        "\n",
        "plt.bar(range(len(perclassavg)), perclassavg)\n",
        "plt.xlabel('Class value (one per category)')\n",
        "plt.ylabel('Accuracy within class')\n",
        "#plt.title(title+\", Total Acc=%.1f\"%(totalaccuracy))\n",
        "plt.title(\"Random Forest Avg Acc=%.1f\"%(avgaccuracy))\n",
        "plt.grid()\n",
        "plt.ylim([0,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JaPxeyaqRD_i",
        "colab": {}
      },
      "source": [
        "ylist = Y['id_label'].values.astype('int64')\n",
        "ylist\n",
        "ylist.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dAp5ZVOGSwZ4",
        "colab": {}
      },
      "source": [
        "list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ELigS-EI-vWd",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "ybinary = label_binarize(ylist, classes=[1, 2, 3, 4, 5, 6, 7])\n",
        "n_classes = ybinary.shape[1]\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X,ybinary, test_size=0.2) # 70% training and 30% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BA0VrcfN_Am7",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Learn to predict each class against the other\n",
        "rfclassifiercv = OneVsRestClassifier(RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                       criterion='gini', max_depth=20, max_features='auto',\n",
        "                       max_leaf_nodes=None, max_samples=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=1, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
        "                       n_jobs=None, oob_score=False, random_state=101,\n",
        "                       verbose=0, warm_start=False))\n",
        "rfbinarymodel = rfclassifiercv.fit(X_train4, y_train4)\n",
        "rfbinaryscore = rfclassifiercv.predict(X_test4)\n",
        "y_score = cross_val_predict(rfclassifiercv, X, ybinary, cv=10 ,method='predict_proba')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Clyn3ShEAb4u",
        "colab": {}
      },
      "source": [
        "#X1_train, X1_test, y1_train, y1_test\n",
        "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(ybinary[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "#Plot of a ROC curve for a specific class\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[3])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic Whiskey')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U5hAPjphA-y7",
        "colab": {}
      },
      "source": [
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qzWnWGTO04u9"
      },
      "source": [
        "#### Model 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt6Qh9NNIHgt",
        "colab_type": "text"
      },
      "source": [
        "Details regarding this model are noted in the code below, but to quickly summarize, this model has an average accuracy score of 0.27 and a Precision, Recall, & F1 Score of 0.46, 0.32, & 0.34 respectively. After evaluation this is not our best fitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M0OF0Ygn0WEc",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "parameters = { 'penalty':['l2']\n",
        "              ,'C': [0.1, 1, 10, 100]\n",
        "              ,'class_weight': ['balanced','none']\n",
        "              ,'solver': ['lbfgs']\n",
        "              ,'max_iter':[1500,2000]\n",
        "              ,'random_state':[101]\n",
        "             }\n",
        "\n",
        "clf_LR = LogisticRegression()\n",
        "grid_search_LR = GridSearchCV(clf_LR, param_grid=parameters, cv=cv,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
        "\n",
        "LogisticRegression_model = grid_search_LR.fit(X1_train_res, y1_train_res)\n",
        "\n",
        "y_LR_score = grid_search_LR.predict(X1_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sY3OgMqJ1Fpk",
        "colab": {}
      },
      "source": [
        "classifierEstimaterLR = LogisticRegression_model.best_estimator_\n",
        "classifierEstimaterLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSGpoKrj19XC",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(\"Precision Score is: {}\" .format(precision_score(y1_test, y_LR_score, average='weighted')))\n",
        "print(\"Recall Score is: {}\" .format(recall_score(y1_test, y_LR_score, average='weighted')))\n",
        "print(\"F1 Score is: {}\" .format(f1_score(y1_test, y_LR_score, average='weighted')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cw_52BLf1egP",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator3(classifierEstimaterLR, X, Y2, cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C8-DNCbo1hfq",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "LR_accuracy = cross_val_score(classifierEstimaterLR, X, y=Y2, cv=cv)\n",
        "LR_acc=LR_accuracy.mean()\n",
        "print('Average LR accuracy score is : {}' .format(LR_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N4UEPDtzB6ax",
        "colab": {}
      },
      "source": [
        "totalacc = 0\n",
        "totalprec = 0\n",
        "totalrec= 0\n",
        "totalf1  = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(cv.split(X,Y2)):\n",
        "     X_train = X[train_index]    \n",
        "     y_train = Y2[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
        "     X_test = X[test_index]\n",
        "     y_test = Y2[test_index]  # See comment on ravel and  y_train\n",
        "     sm = SMOTE(random_state=101)\n",
        "     X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n",
        "     classifierEstimaterLR.fit(X_train, y_train) \n",
        "     y_pred = classifierEstimaterLR.predict(X_test)\n",
        "     acc = mt.accuracy_score(y_test,y_pred)\n",
        "     prec = precision_score(y_test, y_pred, average='weighted')\n",
        "     rec = recall_score(y_test, y_pred, average='weighted')\n",
        "     f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "     perclassaccuracy = per_class_accuracy(y_test,y_pred)\n",
        "     perclasslist.append(perclassaccuracy.tolist())\n",
        "     totalacc += acc\n",
        "     totalprec += prec\n",
        "     totalrec += rec\n",
        "     totalf1 += f1\n",
        "     conf = mt.confusion_matrix(y_test,y_pred)\n",
        "#     print(\"Sum: \", totalacc, \"Current: \", acc,\"Per Class\",perclassaccuracy,\"\\n\")\n",
        "\n",
        "\n",
        "avgaccuracy = 100*totalacc / cv.n_splits\n",
        "avgprec = 100*totalprec / cv.n_splits\n",
        "avgrec = 100*totalrec / cv.n_splits\n",
        "avgf1 = 100*totalf1 / cv.n_splits\n",
        " \n",
        "\n",
        "list.append((\"Model 3\", \"Logistic Regression\", avgaccuracy, avgprec, avgrec, avgf1))\n",
        "\n",
        "perclassdf = []\n",
        "perclassdf = pd.DataFrame(perclasslist)\n",
        "#perclassdf = perclassdf.transpose()\n",
        "#perclassdf\n",
        "\n",
        "perclassavg = perclassdf.mean(axis = 0) \n",
        "#perclassavg\n",
        "\n",
        "plt.bar(range(len(perclassavg)), perclassavg)\n",
        "plt.xlabel('Class value (one per category)')\n",
        "plt.ylabel('Accuracy within class')\n",
        "#plt.title(title+\", Total Acc=%.1f\"%(totalaccuracy))\n",
        "plt.title(\"Logistic Regression Avg Acc=%.1f\"%(avgaccuracy))\n",
        "plt.grid()\n",
        "plt.ylim([0,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bngXJqSpCP4G",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "ybinary = label_binarize(ylist, classes=[1, 2, 3, 4, 5, 6, 7])\n",
        "n_classes = ybinary.shape[1]\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X,ybinary, test_size=0.2) # 70% training and 30% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v66Tr7WSCbFM",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Learn to predict each class against the other\n",
        "logclassifiercv = OneVsRestClassifier(classifierEstimaterLR)\n",
        "logbinarymodel = logclassifiercv.fit(X_train5, y_train5)\n",
        "logbinaryscore = logclassifiercv.predict(X_test5)\n",
        "y_score = cross_val_predict(logclassifiercv, X, ybinary, cv=10 ,method='predict_proba')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5N-hRo9DKU9",
        "colab": {}
      },
      "source": [
        "#X1_train, X1_test, y1_train, y1_test\n",
        "#y_score = classifier.fit(X_train3, y_train3).decision_function(X_test3)\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(ybinary[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ybinary.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "#Plot of a ROC curve for a specific class\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[3])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic Whiskey')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eMvC3oU0DQSp",
        "colab": {}
      },
      "source": [
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wgb5-8YRY_ec"
      },
      "source": [
        "#### Final Model chosen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "428A_s_5fLSW"
      },
      "source": [
        "### Task 1 - Modeling and Evaluation 4  \n",
        "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pMQPia078KYb",
        "colab": {}
      },
      "source": [
        "print ('accuracy for KNN classifier is :',KNN_acc)\n",
        "print ('accuracy for Random Forest classifier is :',RF_acc)\n",
        "print ('accuracy for Logistic Regression classifier is :',LR_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wRs27H7JpE8k",
        "colab": {}
      },
      "source": [
        "list\n",
        "dfobj = pd.DataFrame(list, columns = ['Model Number', 'Model','Accuracy' , 'Precision', 'Recall', 'F1'], index=['0', '1', '2'])\n",
        "dfobj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QsICZBA2NaO5"
      },
      "source": [
        "### Task 1 - Modeling and Evaluation 5  \n",
        "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1uoUpNu6TUUf"
      },
      "source": [
        "#### Statistical Comparisons of Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L0DkLi9QSybl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "acc_KNN_t1 = cross_val_score(classifierEstimaterKNN1, X, y=Y, cv=cv)\n",
        "acc_RF_t1 = cross_val_score(classifierEstimaterRF1, X, y=Y, cv=cv)\n",
        "acc_LR_t1 = cross_val_score(classifierEstimaterLR, X, y=Y, cv=cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "82FQ7BswS189",
        "colab": {}
      },
      "source": [
        "print ('accuracy for KNN classifier is :',acc_KNN_t1)\n",
        "print ('accuracy for Random Forest classifier is :',acc_RF_t1)\n",
        "print ('accuracy for Logistic Regression Classifier is :',acc_LR_t1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSgiUSguS5c1",
        "colab": {}
      },
      "source": [
        "#Is t-value right here???\n",
        "t = 2.26 / np.sqrt(10)\n",
        "\n",
        "e_KNN_RF_t1 = (1-acc_KNN_t1)-(1-acc_RF_t1)\n",
        "e_KNN_LR_t1 = (1-acc_KNN_t1)-(1-acc_LR_t1)\n",
        "e_RF_LR_t1 = (1-acc_RF_t1)-(1-acc_LR_t1)\n",
        "\n",
        "\n",
        "stdtot_K_R_t1 = np.std(e_KNN_RF_t1)\n",
        "stdtot_K_L_t1 = np.std(e_KNN_LR_t1)\n",
        "stdtot_R_L_t1 = np.std(e_RF_LR_t1)\n",
        "\n",
        "\n",
        "dbarKR_t1 = np.mean(e_KNN_RF_t1)\n",
        "dbarKL_t1 = np.mean(e_KNN_LR_t1)\n",
        "dbarRL_t1 = np.mean(e_RF_LR_t1)\n",
        "\n",
        "\n",
        "print ('Range of KNN_RF confidence interval:[%0.6f,%0.6f]' % (dbarKR_t1-t*stdtot_K_R_t1,dbarKR_t1+t*stdtot_K_R_t1))\n",
        "print ('Range of KNN_LR confidence interval:[%0.6f,%0.6f]' % (dbarKL_t1-t*stdtot_K_L_t1,dbarKL_t1+t*stdtot_K_L_t1))\n",
        "print ('Range of RF_LR confidence interval:[%0.6f,%0.6f]' % (dbarRL_t1-t*stdtot_R_L_t1,dbarRL_t1+t*stdtot_R_L_t1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mZ9en1WjTCSF"
      },
      "source": [
        "***Change this part***. According to the intervals above, the only relationship that is not statistically significant is the relationship between KNN and DT at the 95% level. This is indicated by the fact that the interval range passes through 0 while none of the others do. For the others, this means that they are statistically significant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nQoIH4fuNhez"
      },
      "source": [
        "### Task 1 - Modeling and Evaluation 6  \n",
        "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ot8lU13gMUOD",
        "colab": {}
      },
      "source": [
        "importances = classifierEstimaterRF1.feature_importances_\n",
        "importances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q_jVyxJn4mOk",
        "colab": {}
      },
      "source": [
        "classes = classifierEstimaterRF1.classes_\n",
        "classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XvfdAQMS5U7b",
        "colab": {}
      },
      "source": [
        "classifierEstimaterRF1.base_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SzXbDFF85gxC",
        "colab": {}
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "result = permutation_importance(classifierEstimaterRF1, X1_test, y1_test, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "result\n",
        "#X1_train, X1_test, y1_train, y1_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Wy3cU2b8W6Y",
        "colab": {}
      },
      "source": [
        "sorted_idx = result.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(result.importances[sorted_idx].T,\n",
        "           vert=False, labels=X2.columns[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6LgiR-AG9V-s",
        "colab": {}
      },
      "source": [
        "result = permutation_importance(classifierEstimaterRF1, X1_train, y1_train, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "sorted_idx = result.importances_mean.argsort()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(result.importances[sorted_idx].T,\n",
        "           vert=False, labels=X2.columns[sorted_idx])\n",
        "ax.set_title(\"Permutation Importances (train set)\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "koXaf2DROyJB"
      },
      "source": [
        "## Classification 2 (Store Data)\n",
        "\n",
        "Our second model we will be exploring will be a binary classification of the Hy-vee store which appeared to be the most popular alcohol seller on our list. Before we move forward, we will need to use our one hot encoded dataset wtih store names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_hcKIpM9V4t",
        "colab": {}
      },
      "source": [
        "# Since we transformed a number of continuous variables, we can drop them so that we are working directly on our normalized data\n",
        "\n",
        "\n",
        "# Remove unwanted columns, which include all the specific liquor categories, \n",
        "# except for liquor_category_WHISKY since that is what we want to classify on, along\n",
        "# with all the store_ attributes\n",
        "\"\"\"\n",
        "cat_vars=['counter', 'liquor_category', 'store_parent',\n",
        " 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', \n",
        " 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', \n",
        " 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb',\n",
        " 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', \n",
        " 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', \n",
        " 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', \n",
        " 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens']\n",
        "data_vars=data.columns.values.tolist()\n",
        "to_keep=[i for i in data_vars if i not in cat_vars]\n",
        "\"\"\"\n",
        "#keep our transformed detail, along with the timing and store name detail\n",
        "\n",
        "to_keep=['sale_dollars_trans', 'cost_per_liter_trans', \n",
        "      'state_bottle_cost_trans', 'bottles_sold_trans',\n",
        "       'volume_sold_liters_trans','pack_trans', 'bottle_volume_ml_trans', \n",
        "       'profit_trans', 'totalcost_trans', 'revenue_trans', 'id_label', 'store_parent_Hy-Vee']\n",
        "data_final2=data[to_keep]\n",
        "data_final2.columns.values\n",
        "\n",
        "#boxplot of all the variables\n",
        "plt.figure(figsize=(15, 15))\n",
        "ax = data_final2.boxplot()\n",
        "#ax.set_yscale('log')\n",
        "\n",
        "print(data_final2.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YCcShKC3989w",
        "colab": {}
      },
      "source": [
        "#cholesterol percentage split\n",
        "(data_final2['store_parent_Hy-Vee'].value_counts()/len(data_final2))*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6TxofarO-QVv"
      },
      "source": [
        "For our data training set, we will use a stratified k-fold cross validation method which will help to balance the ratio of labels used without having to run a SMOTE which was done previously. Our store_parent_Hy-Vee variable has roughly a 60 - 40% split, and with a data set of 40k, we will have a high enough level of confidence of a random split with the stratefied techniques. This method will cycle 10 tiems around so that all of the data can be used as a hold out as well as a training data set. \n",
        "\n",
        "Like previously, we will use the SMOTE technique to generate new samples to help balance the instances from Hy-Vee with the instances that were not from Hy-Vee and avoid overfitting. \n",
        "\n",
        "For this task, we will use accuracy as a key metric to evvaluate the models which are KNN, SVM and Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gLU8TxrmCcA2"
      },
      "source": [
        "### Task 2 Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqYFpa7hCaMZ",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics as mt\n",
        "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=101)\n",
        "#separating input data into two parts X (features) and Y (target)\n",
        "features2 = ['sale_dollars_trans', 'cost_per_liter_trans', \n",
        "       'state_bottle_cost_trans', 'bottles_sold_trans',\n",
        "       'volume_sold_liters_trans', 'pack_trans', 'bottle_volume_ml_trans',\n",
        "       'profit_trans', 'totalcost_trans', 'revenue_trans']\n",
        "\n",
        "Xt2 = data_final2[features2].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(Xt2)\n",
        "\n",
        "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
        "#Notice we scaled the data before the cross validation\n",
        "Xt = scaler.transform(Xt2)\n",
        "\n",
        "Yt2= data_final2[['store_parent_Hy-Vee']].copy()\n",
        "Yt2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1cLRHzdOAUe"
      },
      "source": [
        "### Task 2 - Modeling and Evaluation 3\n",
        "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
        "\n",
        "We tried 3 different algorithms to create a model which has superior prediction capabilities based on the ROC/AUC scoring parameter:\n",
        "\n",
        "*   Support Vector Machine\n",
        "*   Random Forest Classification\n",
        "*   Decision Tree Classifier\n",
        "\n",
        "A ROC/AUC plot will be created for each model and summarized our findings based on the combined results for these different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vxlg--UZ-QRW"
      },
      "source": [
        "#### Model 1: Support vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtAY4tYi-DoO",
        "colab": {}
      },
      "source": [
        "\n",
        "#SVM_SGD 10-fold cross-validation \n",
        "\n",
        "param_grid = { 'loss': ['modified_huber']\n",
        "              ,'penalty':['l2']\n",
        "              ,'alpha': [0.01, 0.1, 1, 10]\n",
        "              ,'class_weight': ['balanced', None]\n",
        "              ,'random_state': [101]\n",
        "              ,'max_iter':[1000,1500]\n",
        "              \n",
        "             }\n",
        "clf_SVM_t2 = SGDClassifier()\n",
        "#Create a grid search object using the above parameters \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "SVMGridSearch_t2 = GridSearchCV(clf_SVM_t2, param_grid=param_grid, cv=cv,n_jobs=8, verbose=1, scoring='roc_auc' )\n",
        "\n",
        "#Perform hyperparameter search to find the best combination of parameters for our data\n",
        "SVMGridSearch_t2.fit(Xt,y=Yt2.values.ravel())\n",
        "y_SVM_score_t2 = SVMGridSearch_t2.predict(Xt)\n",
        "\n",
        "y_SVM_prob_t2=SVMGridSearch_t2.predict_proba(Xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ek-3eaCwD9k2",
        "colab": {}
      },
      "source": [
        "classifierEstimaterSVM_t2 =SVMGridSearch_t2.best_estimator_\n",
        "classifierEstimaterSVM_t2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_yMchtuEW5c",
        "colab": {}
      },
      "source": [
        "def EvaluateClassifierEstimator_t2(classifierEstimator, X, Y, cv):\n",
        "\n",
        "\n",
        "\n",
        "#Perform cross validation \n",
        "    scores = cross_validate(classifierEstimator, X, Y,\n",
        "                            scoring=['accuracy', 'precision','recall'], \n",
        "                            cv=cv, return_train_score=True, \n",
        "                            )\n",
        "\n",
        "    Accavg = scores['test_accuracy'].mean()\n",
        "    Preavg = scores['test_precision'].mean()\n",
        "    Rreavg = scores['test_recall'].mean()\n",
        "\n",
        "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
        "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
        "    print_str3 = \"The average Recall for all cv folds is: \\t\\t\\t {Rreavg:.5}\"\n",
        "\n",
        "    print(print_str.format(Accavg=Accavg))\n",
        "    print(print_str2.format(Preavg=Preavg))\n",
        "    print(print_str3.format(Rreavg=Rreavg))\n",
        "    print('*********************************************************')\n",
        "\n",
        "    print('Cross Validation Fold Mean Error Scores')\n",
        "    scoresResults = pd.DataFrame()\n",
        "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
        "    scoresResults['Precision'] = scores['test_precision']\n",
        "    scoresResults['Recall'] = scores['test_recall']\n",
        "    print(scoresResults)\n",
        "    return scoresResults\n",
        "\n",
        "SVM_scores_t2 = EvaluateClassifierEstimator_t2(classifierEstimaterSVM_t2,Xt,Yt2,cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OOH4kUAEgg4",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fprSVM_t2 = dict()\n",
        "tprSVM_t2 = dict()\n",
        "roc_auc_SVM_t2 = dict()\n",
        "for i in np.unique(Yt2):\n",
        "    fprSVM_t2[i], tprSVM_t2[i], _ = mt.roc_curve(Yt2, y_SVM_prob_t2[:,i], pos_label=i)\n",
        "    roc_auc_SVM_t2[i] = mt.auc(fprSVM_t2[i], tprSVM_t2[i])\n",
        "plt.figure(figsize=(12,8));    \n",
        "for i in np.unique(Yt2):\n",
        "    plt.plot(fprSVM_t2[i], tprSVM_t2[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_SVM_t2[i])))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "plt.title('Receiver operating characteristic for SVM classifier')\n",
        "plt.legend(loc=\"lower right\")  \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f4rP6ouUGW7L",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator2(classifierEstimaterSVM_t2, Xt, Yt2, cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XosfJzswHpmt"
      },
      "source": [
        "#### Model 2 KNN Classification with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TGqTZYI6GscU",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "         'weights': ['uniform','distance'],\n",
        "         'leaf_size': [10,30],\n",
        "         'metric': ['minkowski','euclidean'],\n",
        "         'n_neighbors':[13,15,17],\n",
        "         \n",
        "    }\n",
        "]\n",
        "clf_KNN_t2 = KNeighborsClassifier()\n",
        "grid_search_KNN_t2 = GridSearchCV(clf_KNN_t2, param_grid=param_grid,cv=cv,n_jobs=8, verbose=1, scoring='roc_auc' )\n",
        "\n",
        "KNearest_model_t2 = grid_search_KNN_t2.fit(Xt, Yt2.values.ravel())\n",
        "y_KNN_score_t2 = grid_search_KNN_t2.predict(Xt)\n",
        "\n",
        "y_KNN_prob_t2=grid_search_KNN_t2.predict_proba(Xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSZi_lI1Iu2R",
        "colab": {}
      },
      "source": [
        "classifierEstimaterKNN_t2 = KNearest_model_t2.best_estimator_\n",
        "classifierEstimaterKNN_t2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZEBxxlTvI4Ic"
      },
      "source": [
        "\n",
        "The GridSearch algorithm determined the following optimal parameters for K-Neighbors model.\n",
        "\n",
        "Leaf-Size: 10\n",
        "Number of Neighbors: 17\n",
        "\n",
        "Distance Matric: Minkowski\n",
        "\n",
        "Weights: Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Us6qm-EgI2LW",
        "colab": {}
      },
      "source": [
        "KNearest_scores_t2 = EvaluateClassifierEstimator(classifierEstimaterKNN_t2,Xt,Yt2,cv=cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lj9bXSjZJFd-",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator2(classifierEstimaterKNN_t2, Xt,Yt2,cv=cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y9BbRWXpJXSH",
        "colab": {}
      },
      "source": [
        "\n",
        "fprKNN_t2 = dict()\n",
        "tprKNN_t2 = dict()\n",
        "roc_auc_KNN_t2 = dict()\n",
        "for i in np.unique(Yt2):\n",
        "    fprKNN_t2[i], tprKNN_t2[i], _ = mt.roc_curve(Yt2, y_KNN_prob_t2[:, i], pos_label=i)\n",
        "    roc_auc_KNN_t2[i] = mt.auc(fprKNN_t2[i], tprKNN_t2[i])\n",
        "plt.figure(figsize=(12,8));    \n",
        "for i in np.unique(Yt2):\n",
        "    plt.plot(fprKNN_t2[i], tprKNN_t2[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_KNN_t2[i])))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "plt.title('Receiver operating characteristic for KNN classifier')\n",
        "plt.legend(loc=\"lower right\")  \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4kfjkWVTMJxK"
      },
      "source": [
        "#### Model 3 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3-W5CnzMbA4",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "         'n_estimators': [200, 500], \n",
        "         'max_depth': [5,10,15],\n",
        "         'random_state':[101]\n",
        "     }\n",
        "]\n",
        "\n",
        "clf_RF_t2 = RandomForestClassifier()\n",
        "grid_search_RF_t2 = GridSearchCV(clf_RF_t2, param_grid=param_grid, cv=cv,n_jobs=8, verbose=1, scoring='roc_auc' )\n",
        "\n",
        "RandomForest_model_t2 = grid_search_RF_t2.fit(Xt, Yt2.values.ravel())\n",
        "\n",
        "y_RF_score_t2 = grid_search_RF_t2.predict(Xt)\n",
        "y_RF_prob_t2=grid_search_RF_t2.predict_proba(Xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T7nPomlcNgTP",
        "colab": {}
      },
      "source": [
        "classifierEstimaterRF_t2 = RandomForest_model_t2.best_estimator_\n",
        "classifierEstimaterRF_t2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91PK4icdNnT3",
        "colab": {}
      },
      "source": [
        "Random_Forest_scores_t2 = EvaluateClassifierEstimator(classifierEstimaterRF_t2,Xt,Yt2,cv=cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "faw5QvDhNyTN",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator2(classifierEstimaterRF_t2,Xt,Yt2,cv=cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnTFCD6dN5s2",
        "colab": {}
      },
      "source": [
        "# create ROC cuve for Random Forest model:\n",
        "fprRF_t2 = dict()\n",
        "tprRF_t2 = dict()\n",
        "roc_auc_RF_t2 = dict()\n",
        "for i in np.unique(Yt2):\n",
        "    fprRF_t2[i], tprRF_t2[i], _ = mt.roc_curve(Yt2, y_RF_prob_t2[:, i], pos_label=i)\n",
        "    roc_auc_RF_t2[i] = mt.auc(fprRF_t2[i], tprRF_t2[i])\n",
        "plt.figure(figsize=(12,8));    \n",
        "for i in np.unique(Yt2):\n",
        "    plt.plot(fprRF_t2[i], tprRF_t2[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_RF_t2[i])))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "plt.title('Receiver operating characteristic for Random Forest classifier')\n",
        "plt.legend(loc=\"lower right\")  \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWLTUPvfTsWM"
      },
      "source": [
        "#### Model 4 Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35xNHzsWTxJB",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "         'max_depth': [5,10],\n",
        "         'random_state':[101] \n",
        "     }\n",
        "]\n",
        "\n",
        "clf_DT_t2 = DecisionTreeClassifier()\n",
        "\n",
        "\n",
        "grid_searchDT_t2 = GridSearchCV(clf_DT_t2, param_grid=param_grid, cv=cv,n_jobs=-1, verbose=1, scoring='roc_auc')\n",
        "\n",
        "\n",
        "# # Here we are training the model, this is \n",
        "# # what takes the most amount of time to run\n",
        "DT_model_t2 = grid_searchDT_t2.fit(Xt, Yt2.values.ravel())\n",
        "\n",
        "y_DT_score_t2 = grid_searchDT_t2.predict(Xt)\n",
        "y_DT_prob_t2 = grid_searchDT_t2.predict_proba(Xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V2Y7bowLTx-u",
        "colab": {}
      },
      "source": [
        "classifierEstimaterDT_t2 = DT_model_t2.best_estimator_\n",
        "classifierEstimaterDT_t2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "23ewCvFHTx0Z",
        "colab": {}
      },
      "source": [
        "DT_scores_t2 = EvaluateClassifierEstimator(classifierEstimaterDT_t2,Xt,Yt2,cv=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y92WJgMoTxpB",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator2(classifierEstimaterDT_t2,Xt,Yt2, cv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1ib-mc6Txd6",
        "colab": {}
      },
      "source": [
        "# create ROC curve for  Decision Tree Classifier:\n",
        "fprDT_t2 = dict()\n",
        "tprDT_t2 = dict()\n",
        "roc_auc_DT_t2 = dict()\n",
        "for i in np.unique(Yt2):\n",
        "    fprDT_t2[i], tprDT_t2[i], _ = mt.roc_curve(Yt2, y_DT_prob_t2[:, i], pos_label=i)\n",
        "    roc_auc_DT_t2[i] = mt.auc(fprDT_t2[i], tprDT_t2[i])\n",
        "plt.figure(figsize=(12,8));    \n",
        "for i in np.unique(Yt2):\n",
        "    plt.plot(fprDT_t2[i], tprDT_t2[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_DT_t2[i])))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "plt.title('Receiver operating characteristic for Decision Tree Classifier')\n",
        "plt.legend(loc=\"lower right\")  \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5iXAAVqZNm1"
      },
      "source": [
        "#### Final Model chosen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FGOtsfk9lvan"
      },
      "source": [
        "### Task 2 - Modeling and Evaluation 4  \n",
        "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
        "\n",
        "\n",
        "### see task 1 model and evlauation 4 to see a godo way to get a table of the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SxnQKI4_lPS0"
      },
      "source": [
        "### Task 2 - Modeling and Evaluation 5  \n",
        "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LkGLWi3dWi4I",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,9));\n",
        "plt.plot(fprSVM_t2[1], tprSVM_t2[1], color='green', lw=1, label='SVM Calasifier (area = %0.3f)' % roc_auc_SVM_t2[1])\n",
        "plt.plot(fprKNN_t2[1], tprKNN_t2[1], color='blue', lw=1, label='KNN Classifier (area = %0.3f)' % roc_auc_KNN_t2[1])\n",
        "plt.plot(fprRF_t2[1], tprRF_t2[1], color='black', lw=1, label='RF Calasifier (area = %0.3f)' % roc_auc_RF_t2[1])\n",
        "plt.plot(fprDT_t2[1], tprDT_t2[1], color='magenta', lw=1, label='DT Calasifier (area = %0.3f)' % roc_auc_DT_t2[1])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for Store Location Hy-Vee = 1')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eFtBt9oAVVoM",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,9));\n",
        "plt.plot(fprSVM_t2[0], tprSVM_t2[0], color='darkorange', lw=1, label='SVM Calasifier (area = %0.3f)' % roc_auc_SVM_t2[0])\n",
        "plt.plot(fprKNN_t2[0], tprKNN_t2[0], color='red', lw=1, label='KNN Classifier (area = %0.3f)' % roc_auc_KNN_t2[0])\n",
        "plt.plot(fprRF_t2[0], tprRF_t2[0], color='black', lw=1, label='RF Calasifier (area = %0.3f)' % roc_auc_RF_t2[0])\n",
        "plt.plot(fprDT_t2[0], tprDT_t2[0], color='yellow', lw=1, label='DT Calasifier (area = %0.3f)' % roc_auc_DT_t2[0])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for Store Location Hy-Vee = 0')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V1UM7HzdXcN2"
      },
      "source": [
        "#### Statistical Comparisons of Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHFuHHWqXgZx",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "acc_SVM_t2 = cross_val_score(classifierEstimaterSVM_t2, Xt, y=Yt2, cv=cv)\n",
        "acc_KNN_t2 = cross_val_score(classifierEstimaterKNN_t2, Xt, y=Yt2, cv=cv)\n",
        "acc_RF_t2 = cross_val_score(classifierEstimaterRF_t2, Xt, y=Yt2, cv=cv)\n",
        "acc_DT_t2 = cross_val_score(classifierEstimaterDT_t2, Xt, y=Yt2, cv=cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h22Ba0FFpqoj",
        "colab": {}
      },
      "source": [
        "print ('accuracy for SVM classifier is :', acc_SVM_t2)\n",
        "print ('accuracy for KNN classifier is :',acc_KNN_t2)\n",
        "print ('accuracy for Random Forest classifier is :',acc_RF_t2)\n",
        "print ('accuracy for Decision Tree Classifier is :',acc_DT_t2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CGMfUxEvTYzK",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lt9fteq4YT-B",
        "colab": {}
      },
      "source": [
        "t = 2.26 / np.sqrt(10)\n",
        "\n",
        "e_SVM_KNN_t2 = (1-acc_SVM_t2)-(1-acc_KNN_t2)\n",
        "e_SVM_RF_t2 = (1-acc_SVM_t2)-(1-acc_RF_t2)\n",
        "e_SVM_DT_t2 = (1-acc_SVM_t2)-(1-acc_DT_t2)\n",
        "e_KNN_RF_t2 = (1-acc_KNN_t2)-(1-acc_RF_t2)\n",
        "e_KNN_DT_t2 = (1-acc_KNN_t2)-(1-acc_DT_t2)\n",
        "e_RF_DT_t2 = (1-acc_RF_t2)-(1-acc_DT_t2)\n",
        "\n",
        "stdtot_S_K_t2 = np.std(e_SVM_KNN_t2)\n",
        "stdtot_S_R_t2 = np.std(e_SVM_RF_t2)\n",
        "stdtot_S_D_t2 = np.std(e_SVM_DT_t2)\n",
        "stdtot_K_R_t2 = np.std(e_KNN_RF_t2)\n",
        "stdtot_K_D_t2 = np.std(e_KNN_DT_t2)\n",
        "stdtot_R_D_t2 = np.std(e_RF_DT_t2)\n",
        "\n",
        "\n",
        "\n",
        "dbarSK_t2 = np.mean(e_SVM_KNN_t2)\n",
        "dbarSR_t2 = np.mean(e_SVM_RF_t2)\n",
        "dbarSD_t2 = np.mean(e_SVM_DT_t2)\n",
        "dbarKR_t2 = np.mean(e_KNN_RF_t2)\n",
        "dbarKD_t2 = np.mean(e_KNN_DT_t2)\n",
        "dbarRD_t2 = np.mean(e_RF_DT_t2)\n",
        "\n",
        "\n",
        "print ('Range of SVM_KNN confidence interval:[%0.6f,%0.6f]' % (dbarSK_t2-t*stdtot_S_K_t2,dbarSK_t2+t*stdtot_S_K_t2))\n",
        "print ('Range of SVM_RF confidence interval:[%0.6f,%0.6f]' % (dbarSR_t2-t*stdtot_S_R_t2,dbarSR_t2+t*stdtot_S_R_t2))\n",
        "print ('Range of SVM_DT confidence interval:[%0.6f,%0.6f]' % (dbarSD_t2-t*stdtot_S_D_t2,dbarSD_t2+t*stdtot_S_D_t2))\n",
        "print ('Range of KNN_RF confidence interval:[%0.6f,%0.6f]' % (dbarKR_t2-t*stdtot_K_R_t2,dbarKR_t2+t*stdtot_K_R_t2))\n",
        "print ('Range of KNN_DT confidence interval:[%0.6f,%0.6f]' % (dbarKD_t2-t*stdtot_K_D_t2,dbarKD_t2+t*stdtot_K_D_t2))\n",
        "print ('Range of RF_DT confidence interval:[%0.6f,%0.6f]' % (dbarRD_t2-t*stdtot_R_D_t2,dbarRD_t2+t*stdtot_R_D_t2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xN0XmRLAZqkX"
      },
      "source": [
        "According to the intervals above, the only relationship that is not statistically significant is the relationship between KNN and DT at the 95% level. This is indicated by the fact that the interval range passes through 0 while none of the others do. For the others, this means that they are statistically significant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VJ-oVYs9qFrE",
        "colab": {}
      },
      "source": [
        "#add confusion matrix\n",
        "#add ROC/AUC combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7KNjS6xwaV8a"
      },
      "source": [
        "### Task 2 - Modeling and Evaluation 6  \n",
        "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UoZbjceJaeMk",
        "colab": {}
      },
      "source": [
        "# set the parameters of this estimator and fit the model\n",
        "\n",
        "classifierEstimaterRF_t2.fit(Xt, Yt2.values.ravel())\n",
        "\n",
        "coef_t2 = classifierEstimaterRF_t2.feature_importances_\n",
        "\n",
        "X2_t2 = data_final2[features2].copy()\n",
        "\n",
        "feature_names_t2=list(X2_t2.columns.values)\n",
        "\n",
        "#Creates a new dataframe with the coefficients and the features \n",
        "Top_Features_t2 = pd.DataFrame({'feature_names':feature_names_t2, 'weights':coef_t2})\n",
        "print(\"The Top Feature are the following\")\n",
        "display(Top_Features_t2.sort_values(by='weights', ascending=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zKmn-UU7Zpxu",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "ax = sns.barplot(x =Top_Features_t2['weights'], y = Top_Features_t2.sort_values(by='weights', ascending=False)['feature_names'], \n",
        "                 orient= 'h')\n",
        "ax.set_title(\"Top Feature Correlations\")\n",
        "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTDyh7HCD_lm"
      },
      "source": [
        "## Deployment (5 points total)\n",
        "\n",
        "How useful is your model for interested parties (i.e., the companies or organizations that\n",
        "might want to use it for prediction)? How would you measure the model's value if it was used\n",
        "by these parties? How would you deploy your model for interested parties? What other data\n",
        "should be collected? How often would the model need to be updated, etc.? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WSL5nbXWEHlC"
      },
      "source": [
        "## Exceptional Work (10 points total) \n",
        "• You have free reign to provide additional analyses.\n",
        "• One idea: grid search parameters in a parallelized fashion and visualize the performances\n",
        "across attributes. Which parameters are most significant for making a good model for each\n",
        "classification algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wO6qifu-nmTQ"
      },
      "source": [
        "### XGBoost Classifier on THe Store Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CVzPKClvnsQi",
        "colab": {}
      },
      "source": [
        "__author__ = 'Tilii: https://kaggle.com/tilii7' \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uSKayrKnoAA0"
      },
      "source": [
        "Adapted from https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
        "\n",
        "Let's run a timer to see how long our model takes to run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jo_3GqkFoHcd",
        "colab": {}
      },
      "source": [
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YcGkMa7soXFd"
      },
      "source": [
        "Set up a parameter grid for the XGBoost that will have an exhaustive exploration in our XG boost model. The total number of combinations we will explore will be 405 X5 to account for the 5 fold cross validation we will use, so 2020 total combinations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4WewozphogAw",
        "colab": {}
      },
      "source": [
        "# A parameter grid for XGBoost\n",
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rr9KqlwPo9Y7"
      },
      "source": [
        "Next, let's set up our classifier using the API of theXG boost, which is a requirement for the grid search. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcM1eC84pd6V",
        "colab": {}
      },
      "source": [
        "xgb_t2 = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
        "                    silent=True, nthread=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IKZ7EUUprNh"
      },
      "source": [
        "From here, we wills et up the stratefied folds for our grid search parameters, here will will do 5 folds for 5 param combos. . The param combo declares how many different combinations should be picked out of the 405 we previously discussed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0D0NpvkiqDxv",
        "colab": {}
      },
      "source": [
        "folds = 10\n",
        "param_comb = 5\n",
        "\n",
        "skf_t2 = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
        "\n",
        "random_search = RandomizedSearchCV(xgb_t2, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf_t2.split(Xt,Yt2), verbose=3, random_state=1001 )\n",
        "\n",
        "# Here we go\n",
        "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "XGB_t2 = random_search.fit(Xt, Yt2.values.ravel())\n",
        "timer(start_time) # timing ends here for \"start_time\" variabl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JeBOY2t3tJBv",
        "colab": {}
      },
      "source": [
        "# # Here we are training the model, this is \n",
        "# # what takes the most amount of time to run\n",
        "\n",
        "\n",
        "y_XGB_score_t2 = random_search.predict(Xt)\n",
        "y_XGB_prob_t2 = random_search.predict_proba(Xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_3tEfPwuHhF",
        "colab": {}
      },
      "source": [
        "classifierEstimaterXGB_t2 = XGB_t2.best_estimator_\n",
        "classifierEstimaterXGB_t2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_TzbS95uwrkc"
      },
      "source": [
        "After running a grid search on our hyper parameters, the search told us that the optimal tree has a base score of 0.5 with a gbtree booster, which essentialliy is a modified version of decision trees. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uphecrRVtAGI",
        "colab": {}
      },
      "source": [
        "XGB_scores_t2 = EvaluateClassifierEstimator(classifierEstimaterXGB_t2,Xt,Yt2,cv=cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lrhwfvP9wadF",
        "colab": {}
      },
      "source": [
        "EvaluateClassifierEstimator2(classifierEstimaterXGB_t2, Xt,Yt2,cv=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XEXy95akxnjZ",
        "colab": {}
      },
      "source": [
        "fprXGB_t2 = dict()\n",
        "tprXGB_t2 = dict()\n",
        "roc_auc_XGB_t2 = dict()\n",
        "for i in np.unique(Yt2):\n",
        "    fprXGB_t2[i], tprXGB_t2[i], _ = mt.roc_curve(Yt2, y_XGB_prob_t2[:, i], pos_label=i)\n",
        "    roc_auc_XGB_t2[i] = mt.auc(fprXGB_t2[i], tprXGB_t2[i])\n",
        "plt.figure(figsize=(12,8));    \n",
        "for i in np.unique(Yt2):\n",
        "    plt.plot(fprXGB_t2[i], tprXGB_t2[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_XGB_t2[i])))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "plt.title('Receiver operating characteristic for XGB classifier')\n",
        "plt.legend(loc=\"lower right\")  \n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OivSUy-UEDUE"
      },
      "source": [
        "After running our XGBoost, we are able to get a class 0 and 1 area of 0.71 across the board. Looking back at the previous models, it did not perform as well as our Random Forest classification model or KNN which scored in the mid 0.70s. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lhEPlSle0FyS"
      },
      "source": [
        "### Exceptional work 2: Voting Ensembles\n",
        "\n",
        "Adapted from: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html \n",
        "\n",
        "Since election season is upon us, it's time for our models to perform their civit duty to best classify the stores we will be exploring. One method of doing so is through a voting ensemble that takes the performance from our previous classifiers to build a wrapped training and testing algorithm or our LR, KNN, DT, RF and XGB Boost model we ran with our location classifier. \n",
        "\n",
        "OUr ensemble model will employ the following strategies:\n",
        "* Majority voting - the class that recieves the largest number of votes will be chosen as the best model. \n",
        "* Average Probability: The probability vector for each class is totaled and the mean probability vector is collected, and the winner has the highest value. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2-upNhsF1iL0"
      },
      "source": [
        "#### Majority Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R_eTmqyYyTC2",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.gridspec as gridspec\n",
        "import itertools\n",
        "\n",
        "# Training classifiers\n",
        "\n",
        "#removing SVM because it was overfit\n",
        "#clf1_t2 = classifierEstimaterSVM_t2\n",
        "\n",
        "clf2_t2 = classifierEstimaterKNN_t2\n",
        "\n",
        "clf3_t2 = classifierEstimaterRF_t2\n",
        "\n",
        "clf4_t2 = classifierEstimaterDT_t2\n",
        "\n",
        "clf5_t2 = classifierEstimaterXGB_t2\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('KNN', clf2_t2),\n",
        "                                    ('RF', clf3_t2),('DT',clf4_t2),('XGB',clf5_t2)],voting='hard',)\n",
        "\n",
        "labels = ['KNN', 'Random Forest','Decision Tree','XG Boost', 'Ensemble']\n",
        "\n",
        "for clf, label in zip([ clf2_t2, clf3_t2,clf4_t2,clf5_t2,eclf], labels):\n",
        "\n",
        "    scores = model_selection.cross_val_score(clf, Xt, Yt2, \n",
        "                                              cv=StratifiedKFold(n_splits=10, random_state=101), \n",
        "                                              scoring='accuracy')\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
        "          % (scores.mean(), scores.std(), label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4oeuAvCF_-2s"
      },
      "source": [
        "After running our majority voting model, we were able to pull a majority voting classifier to generate a 53% accuracy using our ensemble. This was based on the combination of the decision tree model that received the most votes among KNN, Rnadom Forest, Decision tree and XG boost. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iyigHEnQ3DSz"
      },
      "source": [
        "#### Average Probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKK5h8eK1hiH",
        "colab": {}
      },
      "source": [
        "clf1_1_t2 = SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,\n",
        "                       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
        "                       learning_rate='optimal', loss='modified_huber', max_iter=1000,\n",
        "                       n_jobs=1, penalty='l2', power_t=0.5, random_state=101,\n",
        "                       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
        "\n",
        "eclf_soft_t2 = VotingClassifier(estimators=[('KNN', clf2_t2),\n",
        "                                    ('RF', clf3_t2),('DT',clf4_t2),('XGB',clf5_t2)],voting='soft',)\n",
        "\n",
        "labels = ['KNN', 'Random Forest','Decision Tree','XG Boost', 'Ensemble']\n",
        "\n",
        "for clf, label in zip([clf2_t2, clf3_t2,clf4_t2,clf5_t2,eclf_soft_t2], labels):\n",
        "\n",
        "    scores = model_selection.cross_val_score(clf, Xt, Yt2,\n",
        "                                              cv=StratifiedKFold(n_splits=10, random_state=101), \n",
        "                                              scoring='accuracy')\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
        "          % (scores.mean(), scores.std(), label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vrOmjvSICG5h"
      },
      "source": [
        "After running an average probability model, we were able to generate an accuracy score of 0.46 using our Average probability ensemble. this number was generated using the mean probability collected from each class wtih 46% having the highest value. It was not considered our strongest classifier compared to the majority. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eou-gxaICoHj"
      },
      "source": [
        "#### Majority Voting Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ri3ulhKECnVv",
        "colab": {}
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Plotting decision regions\n",
        "x_min, x_max = Xt[:, 0].min() - 1, Xt[:, 0].max() + 1\n",
        "y_min, y_max = Xt[:, 1].min() - 1, Xt[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
        "\n",
        "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
        "                        [clf2_t2, clf3_t2,clf4_t2,clf5_t2,eclf],\n",
        "                        ['KNN', 'Random Forest','Decision Tree','XG Boost', 'Ensemble']):\n",
        "\n",
        "    Z = eclf\n",
        "    Z = Z.reshape(Xt)\n",
        "\n",
        "    axarr[idx[0], idx[1]].contourf(Xt, Yt2, Z, alpha=0.4)\n",
        "    axarr[idx[0], idx[1]].scatter(Xt[:, 0], Xt[:, 1], c=Yt2,\n",
        "                                  s=20, edgecolor='k')\n",
        "    axarr[idx[0], idx[1]].set_title(tt)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OZYDDy556Ttl"
      },
      "source": [
        "#### Exceptional Work 3: Used SMOTE to balance the classifiers \n",
        "Adapted from: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html \n",
        "\n",
        "For our Task 1 that classified the liquor type, we had an issue of very unbalanced classifiers, with Vodka representing 33% of the total variables, and with Whisky taking up 23% of our liquor types with 7 more variables sharing the remaining 44%. To counteract this, we decided to create a series of synthetic datasets that create an even balance of liquor types. \n",
        "\n",
        "Since Schnapps, Amaretto and Brandy make up such low percentages, we dropped them and ran smote on the remaining. What this ended up doing was it created 30k new observations so that each class had just ober 10k observations assigned to it. From here, we ran our classifier models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IB54ODguyNYN"
      },
      "source": [
        "#### Exceptional Work 4: Linear Regression (Cost per Liter)\n",
        "\n",
        "Moving away from classifiers, we will take a look at using our data to formulate a model that can be used to do a linear regression to predict the cost per liter/trans. Once we prepare our data, we will utilize multiple models to see if we can minimize our RMSE score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H7UbQ6uM-7F_",
        "colab": {}
      },
      "source": [
        "#separating input data into two parts X (features) and Y (target)\n",
        "features3 = ['sale_dollars_trans', 'state_bottle_cost_trans', 'bottles_sold_trans',\n",
        "       'volume_sold_liters_trans', 'pack_trans', 'bottle_volume_ml_trans',\n",
        "       'profit_trans', 'totalcost_trans', 'revenue_trans']\n",
        "\n",
        "Xt3 = data_final2[features3].copy()\n",
        "\n",
        "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
        "#Notice we scaled the data before the cross validation\n",
        "\n",
        "Yt3= data_final2[['cost_per_liter_trans']].copy()\n",
        "Yt3.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yavCCuYZFHdl"
      },
      "source": [
        "TO get started, we will look create a new X and Y variable based on the cost per liter trans variable we created in the data preparation stage at the very top. Since this and all of our variables are normalized, we will not perform a scalar on our dependent variables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RiVoDoiU2_YJ"
      },
      "source": [
        "##### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R7dAj7W73BNG",
        "colab": {}
      },
      "source": [
        "#Divide data into test and training splits\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVLTmfGT3K2w",
        "colab": {}
      },
      "source": [
        "#Use mean absolute error (MAE) to score the regression models created \n",
        "#(the scale of MAE is identical to the response variable)\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
        "\n",
        "#Function for Root mean squared error\n",
        "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
        "def rmse(y_actual, y_predicted):\n",
        "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
        "\n",
        "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
        "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
        "def mape(y_actual, y_predicted): \n",
        "    mask = y_actual != 0\n",
        "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
        "\n",
        "#Create scorers for rmse and mape functions\n",
        "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
        "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
        "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
        "\n",
        "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
        "errorScoring = {'MAE':  mae_scorer, \n",
        "                'RMSE': rmse_scorer,\n",
        "                'MAPE': mape_scorer\n",
        "               }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2jnSi7L3ieP"
      },
      "source": [
        "So we can equally compare our evaluators, we are going to build an evaluation model evaluation function that all the evaluators will share. \n",
        "\n",
        "They will each use a cross validation object and custom score performance in two cells. We will also use a random see to ensure that all regressions are carried out the same way each time the function runs (to ensure consistent performance). THe mean error scores are made using an average of the folds generated in our kfold, and tested in the same way we ran each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "csOu-NBk3QT0",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
        "    \n",
        "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
        "\n",
        "    #cross val score sign-flips the outputs of MAE\n",
        "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
        "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
        "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
        "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
        "\n",
        "    #print mean MAE for all folds \n",
        "    maeAvg = scores['test_MAE'].mean()\n",
        "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
        "    print(print_str.format(maeAvg=maeAvg))\n",
        "\n",
        "    #print mean test_MAPE for all folds\n",
        "    scores['test_MAPE'] = scores['test_MAPE']\n",
        "    mape_avg = scores['test_MAPE'].mean()\n",
        "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
        "    print(print_str.format(mape_avg=mape_avg))\n",
        "\n",
        "    #print mean MAE for all folds \n",
        "    RMSEavg = scores['test_RMSE'].mean()\n",
        "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
        "    print(print_str.format(RMSEavg=RMSEavg))\n",
        "    print('*********************************************************')\n",
        "\n",
        "    print('Cross Validation Fold Mean Error Scores')\n",
        "    scoresResults = pd.DataFrame()\n",
        "    scoresResults['MAE'] = scores['test_MAE']\n",
        "    scoresResults['MAPE'] = scores['test_MAPE']\n",
        "    scoresResults['RMSE'] = scores['test_RMSE']\n",
        "    return scoresResults"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UY94dvI2Gc09"
      },
      "source": [
        "Across all of our models, our scoring criteria will be Mean Absoute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Square Error (RMSE) and used to compare one another. See below for more detail on their definitions. \n",
        "\n",
        "- RMSE - standard deviations of the residuals of errors from the lines. \n",
        "- MAE - average vertical distance between each point and the identity line. \n",
        "- MAPE - Similar to MAE, it calculates the mean vertical error as a percentage of the verticality of the line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D1f3eJxB4tkM"
      },
      "source": [
        "##### Making or modifying custom estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c7jrj5dc42Tx",
        "colab": {}
      },
      "source": [
        "#Make new estimator compatible for use with GridSearchCV() and cross_validate()\n",
        "# -  Cap predict function for LinearRegression between 0 and 100\n",
        "# -  See: Roll your own estimator links above for details. \n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "class CappedLinearRegression(LinearRegression):\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.clip(super(CappedLinearRegression, self).predict(X), 0, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "msNZ1EcT49I2"
      },
      "source": [
        "##### Baseline Linear Regression Grid Search\n",
        "\n",
        "Perform a grid search using multiple models to find the best parameters for our linear regression to maximize the mean absolute error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wlERlU6M5KsC",
        "colab": {}
      },
      "source": [
        "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
        "linreg = CappedLinearRegression()\n",
        "parameters3 = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
        "\n",
        "#Create a grid search object using the  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "regGridSearchbase = GridSearchCV(estimator=linreg\n",
        "                   , verbose=1 # low verbosity\n",
        "                   , param_grid=parameters3\n",
        "                   , cv=cv # KFolds = 10\n",
        "                   , scoring=mae_scorer)\n",
        "\n",
        "#Perform hyperparameter search to find the best combination of parameters for our data\n",
        "regGridSearchbase.fit(Xt3, Yt3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ULcUDyXQI2Zu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "if2tKGIW5gEC",
        "colab": {}
      },
      "source": [
        "#Print the parameterization of the best estimator\n",
        "regGridSearchbase.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6QtrKS86w4V",
        "colab": {}
      },
      "source": [
        "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
        "regEstimatorbase = regGridSearchbase.best_estimator_\n",
        "\n",
        "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
        "EvaluateRegressionEstimator(regEstimatorbase, Xt3, Yt3, cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v05Pg6ErJhCg"
      },
      "source": [
        "Looking at our linear regression baseline model, we found that our scores are very low for the performance, which is suggesting that our model might be overfit to our data set. Since our RMSE, MAE adn MAPE are all extremely close to zero. Let's run our other models to see how they compare. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vC4BtlES7PVU"
      },
      "source": [
        "##### Ridge Regression\n",
        "\n",
        "This model will use Lasso Regressions (L2 Norm) for regression of continuious variables. Documentation below:\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "StnCcTNZ7i44",
        "colab": {}
      },
      "source": [
        "#Create a regression object and perform a grid search to find the best parameters\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "regrid = Ridge(fit_intercept=True, normalize=True,copy_X=True\n",
        "          , max_iter=1000, tol=0.0001, random_state=0)\n",
        "\n",
        "#Test parameters \n",
        "alpharid = [0.001, 0.1, 1, 5, 10, 20]\n",
        "solverrid = [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
        "parametersrid = {'alpha': alpharid, 'solver': solverrid}\n",
        "\n",
        "#Create a grid search object using the parameters above\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "regGridSearchrid = GridSearchCV(estimator=regrid\n",
        "                   , n_jobs=8 # jobs to run in parallel\n",
        "                   , verbose=1 # low verbosity\n",
        "                   , param_grid=parametersrid\n",
        "                   , cv=cv # KFolds = 10\n",
        "                   , scoring=mae_scorer)\n",
        "\n",
        "#Perform hyperparameter search to find the best combination of parameters for our data\n",
        "regGridSearchrid.fit(Xt3, Yt3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHTuu5B-7v3o",
        "colab": {}
      },
      "source": [
        "\n",
        "#Display the best estimator parameters\n",
        "regGridSearchrid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjMvg03G70Je",
        "colab": {}
      },
      "source": [
        "#Create a regression estimator with best parameters for cross validation\n",
        "regEstimatorrid = regGridSearchrid.best_estimator_\n",
        "\n",
        "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
        "EvaluateRegressionEstimator(regEstimatorrid, Xt3, Yt3, cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eqfZrxzwKk-O"
      },
      "source": [
        "Now we have recieved a much more normal regression performance with an RMSE of 0.01 and a MAE percentage of 15% after averaging all of the folds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUghN17v8NR_"
      },
      "source": [
        "##### Elastic Net Regression\n",
        "\n",
        "This model uses Elastic Net Regression (L1 and L2 Norming) Documentation below:\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QmDS4NXB9A4z",
        "colab": {}
      },
      "source": [
        "\n",
        "#Create a regression object and perform a grid search to find the best parameters\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "regen = ElasticNet(fit_intercept=True, normalize=True, precompute=True, copy_X=True\n",
        "          , max_iter=10000, tol=0.0001, random_state=0)\n",
        " \n",
        "#Test parameters\n",
        "l1_ratioen = [0.001, 0.01, 0.1, 0.5, 0.75, 1]\n",
        "alphaen = [0.001, 0.1, 1, 10]\n",
        "selectionen = ['cyclic','random']\n",
        "warm_starten = [True, False]\n",
        "parametersen = {'l1_ratio': l1_ratioen, 'alpha': alphaen, 'selection': selectionen, 'warm_start': warm_starten}\n",
        "\n",
        "#Create a grid search object using the parameters above\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "regGridSearchen = GridSearchCV(estimator=regen\n",
        "                   , n_jobs=8 # jobs to run in parallel\n",
        "                   , verbose=1 # low verbosity\n",
        "                   , param_grid=parametersen\n",
        "                   , cv=cv # KFolds = 10\n",
        "                   , scoring=mae_scorer)\n",
        "\n",
        "#Perform hyperparameter search to find the best combination of parameters for our data\n",
        "regGridSearchen.fit(Xt3, Yt3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrL2P0aM9Qd7",
        "colab": {}
      },
      "source": [
        "#Display the best estimator parameters\n",
        "regGridSearchen.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dNSyKfLmAngq",
        "colab": {}
      },
      "source": [
        "#Create a regression estimator with best parameters for cross validation\n",
        "regEstimatoren = regGridSearchen.best_estimator_\n",
        "\n",
        "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
        "EvaluateRegressionEstimator(regEstimatoren, Xt3, Yt3, cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E0kD3z5MCFIJ"
      },
      "source": [
        "##### Random Forest Regression\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
        "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
        "https://www.kaggle.com/general/4092"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WhPljuSwBa9c",
        "colab": {}
      },
      "source": [
        "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "linregrf = RandomForestRegressor()\n",
        "parametersrf = { 'min_samples_split':[2,3,4,5]\n",
        "              ,'n_estimators' : [10]\n",
        "              ,'min_samples_leaf': [10, 25, 50]\n",
        "              ,'criterion': ['mae']\n",
        "              ,'n_jobs':[8] \n",
        "              ,'random_state': [0]\n",
        "             }\n",
        "\n",
        "#Create a grid search object using the  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "regGridSearchrf = GridSearchCV(estimator=linregrf\n",
        "                   , n_jobs=8 \n",
        "                   , verbose=1 # low verbosity\n",
        "                   , param_grid=parametersrf\n",
        "                   , cv=cv # KFolds = 10\n",
        "                   , scoring=mae_scorer)\n",
        "\n",
        "#Perform hyperparameter search to find the best combination of parameters for our data\n",
        "regGridSearchrf.fit(Xt3, Yt3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vqbp1d3ACbA2",
        "colab": {}
      },
      "source": [
        "regGridSearchrf.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nDlHuDBKCd1t",
        "colab": {}
      },
      "source": [
        "regEstimatorrf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=None,\n",
        "           max_features='auto', max_leaf_nodes=None,\n",
        "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "           min_samples_leaf=25, min_samples_split=5,\n",
        "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=8,\n",
        "           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
        "\n",
        "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
        "EvaluateRegressionEstimator(regEstimatorrf, Xt3, Yt3, cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m73rYLRx6g9B",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}