{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab3_Daniel_Jeff_Armando_Joe_small_spectral.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/jjschueder/7331DataMiningNotebooks/blob/master/lab3/Lab3_Daniel_Jeff_Armando_Joe_small_spectral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"_ulvqQ_BeNdT","colab_type":"text"},"source":["#**Data Mining 7331 - Spring 2020**"]},{"cell_type":"markdown","metadata":{"id":"okMFqheqf3sa","colab_type":"text"},"source":["## Lab 3 -  Clustering, Association Rules, or Recommenders \n","\n","#### Daniel Clark, Joe Schueder, Jeff Washburn, Armando Vela"]},{"cell_type":"markdown","metadata":{"id":"QvVBRUFaUX4O","colab_type":"text"},"source":["Final Team Project\n","CRISP-DM Capstone: Association Rule Mining, Clustering, or Collaborative Filtering  \n","In the final assignment for this course, you will be using one of three different analysis methods:  \n","• Option A: Use clustering on an unlabeled dataset to provide insight or features  \n","• Option B: Use transaction data for mining associations rules  \n","• Option C: Use collaborative filtering to build a custom recommendation system\n","Your choice of dataset will largely determine the task that you are trying to achieve, though the\n","dataset does not need to change from your previous tasks.  \n","• For example, you might choose to use clustering on your data as a preprocessing step that  \n","extracts different features. Then you can use those features to build a classifier and analyze\n","its performance in terms of accuracy (precision, recall) and speed.  \n","• Alternatively, you might choose a completely different dataset and perform rule mining or\n","build a recommendation system.\n","Dataset Selection and Toolkits\n","As before, you need to choose a dataset that is not small. It might be massive in terms of the\n","number of attributes (or transactions), classes (or items, users, etc.) or whatever is appropriate\n","for the task you are performing. Note that scikit-learn can be used for clustering analysis, but not\n","for Association Rule Mining (you should use R) or collaborative filtering (you should use\n","graphlabcreate from Dato). Both can be run using Jupyter notebooks as shown in lecture.  \n","• One example of a recommendation dataset is the movie lens rating data:\n","http://grouplens.org/ datasets/movielens/  \n","• Some examples of association rule mining datasets: http://fimi.ua.ac.be/data/\n","Write a report covering in detail all the steps of the project. The results need to be reproducible\n","using only this report. Describe all assumptions you make and include all code you use in the\n","Jupyter notebook or as supplemental functions. Follow the CRISP-DM framework in your\n","analysis (you are performing all of the CRISP-DM outline).  \n","This report is worth 20% of the final grade. "]},{"cell_type":"markdown","metadata":{"id":"Pn-BG9OZUIrA","colab_type":"text"},"source":["## Business Understanding 1 (10 points)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-5qVCcMiVnJ9","colab_type":"text"},"source":["Describe the purpose of the data set you selected (i.e., why was this data\n","collected in the first place?). How will you measure the effectiveness of a good algorithm?\n","Why does your chosen validation method make sense for this specific dataset and the\n","stakeholders needs?"]},{"cell_type":"markdown","metadata":{"id":"PXl2Sd_vUyRS","colab_type":"text"},"source":["## Data Understanding 1 (10 points)\n"]},{"cell_type":"markdown","metadata":{"id":"fY2s2HiiWPyx","colab_type":"text"},"source":["Describe the meaning and type of data (scale, values, etc.) for each attribute in\n","the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are\n","those mistakes? How do you deal with these problems?"]},{"cell_type":"code","metadata":{"id":"DTqvs6TgcRPl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","url_dataset = 'https://raw.githubusercontent.com/jjschueder/7331DataMiningNotebooks/master/Live%20Assignments/df1hotmerge2.csv'\n","data = pd.read_csv(url_dataset, nrows = 40000)\n","data.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWj8ZP2CcXRi","colab_type":"code","colab":{}},"source":["# Import all necessary libraries we will be using in our dataset\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","import re\n","import sklearn\n","import statistics\n","import random\n","\n","from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE, SelectFromModel\n","\n","from sklearn.preprocessing import StandardScaler, Binarizer\n","\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.svm import SVC, LinearSVC\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, roc_curve\n","from IPython.display import display, HTML\n","\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor,AdaBoostClassifier,RandomForestClassifier, BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","%matplotlib inline\n","\n","matplotlib.style.use('ggplot')\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","print(data.shape)\n","data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"htTMMAq3cbb3","colab_type":"code","colab":{}},"source":["# Since we are predicting our liquor category type (opening to the entire list of categories and not just one), we can assign them a numerical value.\n","\n","print (data['liquor_category'].unique())\n","\n","\"\"\"df = pd.DataFrame({'col_1':[133,255,36,477,55,63]})\n","d = {'1':'M', '2': 'C', '3':'a', '4':'f', '5':'r', '6':'s'}\n","def ifef(col):\n","    col = str(col)\n","    return d[col[0]]\n","\n","df['id_label'] = df['col_1'].apply(ifef)\n","print(df)\"\"\"\n","\n","d = {'O':'1', 'G': '2', 'W':'3', 'T':'4', 'L':'5', 'V':'6', 'R':'7', 'S': '8', 'A':'9', 'B':'10'}\n","\n","#data['id_label'] = data['liquor_category'].apply(ifef)\n","\n","data['id_label'] = data['liquor_category'].astype(str).str[0].map(d)\n","\n","print(data.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeRee5dbcd7u","colab_type":"code","colab":{}},"source":["# Create new variables using aggredate data on profit, total cost and revenue\n","\n","#do some calculations for cost and profit\n","data['profit'] = data['state_bottle_retail']*data['bottles_sold'] - data['state_bottle_cost']* data['bottles_sold']\n","data['profit_trans']= np.log(data['profit'])\n","\n","data['totalcost'] = data['state_bottle_cost']* data['bottles_sold']\n","data['totalcost_trans']= np.log(data['totalcost'])\n","\n","data['revenue'] = data['state_bottle_retail']*data['bottles_sold']\n","data['revenue_trans']= np.log(data['revenue'])\n","\n","data['bottle_volume_ml_trans']= np.log(data['bottle_volume_ml'])\n","\n","data['pack_trans']= np.log(data['pack'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PP78rFZEV2kw","colab_type":"text"},"source":["## Data Understanding 2 (10 points)\n","\n","Visualize any important attributes appropriately\n","\n","Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs"]},{"cell_type":"code","metadata":{"id":"rDaje5MNcjkf","colab_type":"code","colab":{}},"source":["# Since we transformed a number of continuous variables, we can drop them so that we are working directly on our normalized data\n","\n","\n","# Remove unwanted columns, which include all the specific liquor categories, \n","# except for liquor_category_WHISKY since that is what we want to classify on, along\n","# with all the store_ attributes\n","\"\"\"\n","cat_vars=['counter', 'liquor_category', 'store_parent',\n"," 'month', 'year', 'monthyear', 'liquor_category_AMARETTO', 'liquor_category_BRANDY', 'liquor_category_GIN', \n"," 'liquor_category_LIQUEUR', 'liquor_category_Other', 'liquor_category_RUM', 'liquor_category_SCHNAPPS', \n"," 'liquor_category_TEQUILA', 'liquor_category_VODKA', 'month_Apr', 'month_Aug', 'month_Dec', 'month_Feb',\n"," 'month_Jan', 'month_Jul', 'month_Jun', 'month_Mar', 'month_May', 'month_Nov', 'month_Oct', 'month_Sep', \n"," 'store_parent_CVS', 'store_parent_Caseys', 'store_parent_Hy-Vee', 'store_parent_Kum&Go', \n"," 'store_parent_Other', 'store_parent_QuikTrip', 'store_parent_SamsClub', 'store_parent_SmokingJoes', \n"," 'store_parent_Target', 'store_parent_Wal-Mart', 'store_parent_Walgreens']\n","data_vars=data.columns.values.tolist()\n","to_keep=[i for i in data_vars if i not in cat_vars]\n","\"\"\"\n","#keep our transformed detail, along with the timing and store name detail\n","\n","to_keep=['sale_dollars_trans', 'cost_per_liter_trans',\n","      'state_bottle_cost_trans', 'bottles_sold_trans',\n","       'volume_sold_liters_trans','pack_trans', 'bottle_volume_ml_trans', \n","       'profit_trans', 'totalcost_trans', 'revenue_trans', 'id_label']\n","data_final=data[to_keep]\n","data_final.columns.values\n","\n","#boxplot of all the variables\n","plt.figure(figsize=(15, 15))\n","ax = data_final.boxplot()\n","#ax.set_yscale('log')\n","\n","print(data_final.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ph5XCN-xeF8u","colab_type":"code","colab":{}},"source":["data_final.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjH6zCtFnmWu","colab_type":"code","colab":{}},"source":["data_final = data_final[data_final['id_label'] < \"8\"]\n","data_final = data_final[data_final['id_label'] != \"10\"]\n","data_final['id_label'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"akc0GQ-GU9sM","colab_type":"text"},"source":["## Modeling and Evaluation 1 (10 points)\n","\n","Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results. Each option is broken down by:\n","\n","Train and adjust parameters (10 Points)\n","• Train: Perform cluster analysis using several clustering methods (adjust parameters).  \n","    "]},{"cell_type":"code","metadata":{"id":"MmYXGsIBhlKD","colab_type":"code","colab":{}},"source":["%%time\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","from sklearn import metrics as mt\n","\n","cv = StratifiedKFold(n_splits=10)\n","\n","features = ['sale_dollars_trans', 'cost_per_liter_trans', \n","       'state_bottle_cost_trans', 'bottles_sold_trans',\n","       'volume_sold_liters_trans', 'pack_trans', 'bottle_volume_ml_trans',\n","       'profit_trans', 'totalcost_trans', 'revenue_trans']\n","\n","X2 = data_final[features].copy()\n","\n","scaler = StandardScaler()\n","scaler.fit(X2)\n","\n","#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n","#Notice we scaled the data before the cross validation\n","X = scaler.transform(X2)\n","\n","Y= data_final[['id_label']].copy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZBscJ_JkUCe","colab_type":"code","colab":{}},"source":["from sklearn import metrics as mt\n","\n","# train and test split before resampling\n","X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y, test_size = 0.2, random_state = 101) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HayqDmLek18i","colab_type":"code","colab":{}},"source":["\n","print(\"Before OverSampling, counts of label 'Other': {}\".format(sum(y1_train['id_label'] == \"1\")))\n","print(\"Before OverSampling, counts of label 'GIN': {} \\n\".format(sum(y1_train['id_label'] == \"2\"))) \n","print(\"Before OverSampling, counts of label 'WHISKY': {} \\n\".format(sum(y1_train['id_label'] == \"3\"))) \n","print(\"Before OverSampling, counts of label 'TEQUILA': {}\".format(sum(y1_train['id_label'] == \"4\")))\n","print(\"Before OverSampling, counts of label 'LIQUEUR': {} \\n\".format(sum(y1_train['id_label'] == \"5\"))) \n","print(\"Before OverSampling, counts of label 'VODKA': {} \\n\".format(sum(y1_train['id_label'] == \"6\"))) \n","print(\"Before OverSampling, counts of label 'RUM': {} \\n\".format(sum(y1_train['id_label'] == \"7\"))) \n","\n","\n","# import SMOTE module from imblearn library \n","# pip install imblearn (if you don't have imblearn in your system) \n","from imblearn.over_sampling import SMOTE \n","sm = SMOTE(random_state = 2) \n","X1_train_res, y1_train_res = sm.fit_sample(X1_train, y1_train.values.ravel()) \n","  \n","print('After OverSampling, the shape of train_X: {}'.format(X1_train_res.shape)) \n","print('After OverSampling, the shape of train_y: {} \\n'.format(y1_train_res.shape)) \n","  \n","print(\"After OverSampling, counts of label 'Other': {}\".format(sum(y1_train_res == \"1\"))) \n","print(\"After OverSampling, counts of label 'GIN': {}\".format(sum(y1_train_res == \"2\"))) \n","print(\"After OverSampling, counts of label 'WHISKY': {}\".format(sum(y1_train_res == \"3\"))) \n","print(\"After OverSampling, counts of label 'TEQUILA': {}\".format(sum(y1_train_res == \"4\"))) \n","print(\"After OverSampling, counts of label 'LIQUEUR': {}\".format(sum(y1_train_res == \"5\"))) \n","print(\"After OverSampling, counts of label 'VODKA': {}\".format(sum(y1_train_res == \"6\"))) \n","print(\"After OverSampling, counts of label 'RUM': {}\".format(sum(y1_train_res == \"7\"))) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK2TweDupOft","colab_type":"code","colab":{}},"source":["X2.columns\n","X1DF = pd.DataFrame(X1_train_res, columns =X2.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkAUjuLcdbtm","colab_type":"code","colab":{}},"source":["#https://towardsdatascience.com/credit-risk-unsupervised-clients-clustering-9eacae6807a0\n","\n","to_keep=['sale_dollars_trans', 'cost_per_liter_trans',\n","      'state_bottle_cost_trans', 'bottles_sold_trans',\n","       'volume_sold_liters_trans','pack_trans', 'bottle_volume_ml_trans', \n","       'profit_trans', 'totalcost_trans', 'revenue_trans']\n","data_final2=X1DF[to_keep]\n","data_final2.columns.values\n","from sklearn.decomposition import PCA \n","pca = PCA(2)  \n","projected = pca.fit_transform(data_final2)\n","print(X1DF.shape)\n","print(projected.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wgoVXYsdGpi","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","wcss = []\n","K = range(1,15)\n","for k in K:\n","    km = KMeans(n_clusters=k)\n","    km = km.fit(projected)\n","    wcss.append(km.inertia_)\n","plt.plot(K, wcss, 'bx-')\n","plt.xlabel('Number of centroids')\n","plt.ylabel('WCSS')\n","plt.title('Elbow Method For Optimal k')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IN5qE2bg8FL","colab_type":"code","colab":{}},"source":["%%time\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","#cv = StratifiedKFold(n_splits=10)\n","#trained one time then dumped out to google drive for re-use withouth having to re-train\n","#clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","#                       criterion='gini', max_depth=20, max_features='auto',\n","#                       max_leaf_nodes=None, max_samples=None,\n","#                       min_impurity_decrease=0.0, min_impurity_split=None,\n","#                       min_samples_leaf=1, min_samples_split=2,\n","#                       min_weight_fraction_leaf=0.0, n_estimators=500,\n","#                       n_jobs=None, oob_score=False, random_state=101,\n","#                       verbose=0, warm_start=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1bxDBY7xgwm","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","#My Drive/ColabNotebooks/')\n","url_rfmodel = '/content/drive/My Drive/ColabNotebooks/rfmodelbasecluster.joblib'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMLM0AK6xhtE","colab_type":"code","colab":{}},"source":["#exporting model to drive\n","from joblib import dump, load\n","#dump(clf, url_rfmodel) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TS6MqHybxkZD","colab_type":"code","colab":{}},"source":["%%time\n","#importing model to new model name\n","clf = load(url_rfmodel)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2ylONr5yNd5","colab_type":"code","colab":{}},"source":["#X1_train_res, y1_train_res\n","acc = cross_val_score(clf,X1_train_res,y=y1_train_res,cv=cv)\n","\n","print (\"Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cE5mp8f8etAo","colab_type":"code","colab":{}},"source":["#converting our projected array to pandas df\n","pca=pd.DataFrame(projected)\n","pca.columns=['First component','Second Component']\n","#build our algorithm with k=7, train it on pca and make predictions\n","kmeans = KMeans(n_clusters=7, init='k-means++', random_state=0).fit(pca)\n","y_kmeans = kmeans.predict(pca)\n","newfeature = kmeans.labels_\n","X1CV = np.column_stack((X1_train_res,pd.get_dummies(newfeature)))\n","#plotting the results \n","plt.scatter(pca['First component'], pca['Second Component'], c=y_kmeans, s=50, alpha=0.5,cmap='viridis')\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:, 0], centers[:, 1], c='red', s=50)\n","plt.scatter(centers[:, 0], centers[:, 1], c='red', s=50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jEWx9B5fFNC","colab_type":"code","colab":{}},"source":["acc = cross_val_score(clf,X1CV,y=y1_train_res,cv=cv)\n","\n","print (\"Average accuracy (with kmeans for liquor type)= \", acc.mean()*100, \"+-\", acc.std()*100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jv0cbbJk4GEb","colab_type":"code","colab":{}},"source":["# lets first look at the connectivity of the graphs and distance to the nearest neighbors\n","from sklearn.neighbors import kneighbors_graph\n","\n","#=======================================================\n","# CHANGE THESE VALUES TO ADJUST MINPTS FOR EACH DATASET\n","X1_N = 50\n","#=======================================================\n","\n","# create connectivity graphs before calcualting the hierarchy\n","X1_knn_graph = kneighbors_graph(X1_train_res, X1_N, mode='distance') # calculate distance to four nearest neighbors\n","\n","\n","N1 = X1_knn_graph.shape[0]\n","X1_4nn_distances = np.zeros((N1,1))\n","for i in range(N1):\n","    X1_4nn_distances[i] = X1_knn_graph[i,:].max()\n","\n","X1_4nn_distances = np.sort(X1_4nn_distances, axis=0)\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(range(N1), X1_4nn_distances, 'r.', markersize=2) #plot the data\n","plt.title('Dataset name: X1, sorted by neighbor distance')\n","plt.xlabel('X1, Instance Number')\n","plt.ylabel('X1, Distance to {0}th nearest neighbor'.format(X1_N))\n","plt.grid()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgIimWyLr8Dl","colab_type":"code","colab":{}},"source":["%%time \n","\n","from sklearn.cluster import DBSCAN\n","\n","#=====================================\n","# ENTER YOUR CODE HERE TO CHANGE MINPTS AND EPS FOR EACH DATASET\n","X1_minpts = X1_N # from above\n","X1_eps = .45\n","#=====================================\n","\n","\n","\n","db = DBSCAN(eps=X1_eps, min_samples=X1_minpts).fit(pca)\n","labels = db.labels_\n","\n","# Number of clusters in labels, ignoring noise if present.\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","\n","# mark the samples that are considered \"core\"\n","core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n","core_samples_mask[db.core_sample_indices_] = True\n","\n","plt.figure(figsize=(15,4))\n","unique_labels = set(labels) # the unique labels\n","colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n","for k, col in zip(unique_labels, colors):\n","        if k == -1:\n","            # Black used for noise.\n","            col = 'k'\n","\n","        class_member_mask = (labels == k)\n","\n","        xy = X1_train_res[class_member_mask & core_samples_mask]\n","        # plot the core points in this class\n","        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n","                 markeredgecolor='w', markersize=6)\n","\n","        # plot the remaining points that are edge points\n","        xy = X1_train_res[class_member_mask & ~core_samples_mask]\n","        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n","                 markeredgecolor='w', markersize=3)\n","\n","plt.title('Estimated number of clusters: %d' % n_clusters_)\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFJc_HtC-AMZ","colab_type":"code","colab":{}},"source":["pca=pd.DataFrame(projected)\n","pca.columns=['First component','Second Component']\n","pcasubset=pca.sample(n=2000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtbH-WwG83Jn","colab_type":"code","colab":{}},"source":["%%time\n","# an example using SpectralClustering, which assumes that the graphical data needs to be calculated from the structure\n","from sklearn.cluster import SpectralClustering\n","\n","X = pcasubset\n","nclust = 7\n","\n","# If a string, this may be one of \n","#  ‘nearest_neighbors’, ‘precomputed’, ‘rbf’ \n","#  or one of the kernels supported by sklearn.metrics.pairwise_kernels\n","spc = SpectralClustering(n_clusters=nclust, affinity = 'nearest_neighbors')\n","labels = spc.fit_predict(X)\n","\n","plt.scatter(X.iloc[:, 0].values, X.iloc[:, 1].values, c=labels,\n","                    cmap=plt.cm.rainbow, s=5, linewidths=0)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvCoEU8V_Ji5","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWCWKHUx_NeI","colab_type":"code","colab":{}},"source":["#My Drive/ColabNotebooks/')\n","url_rfmodel = '/content/drive/My Drive/ColabNotebooks/spc1.joblib'\n","#exporting model to drive\n","from joblib import dump, load\n","dump(spc, url_rfmodel) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjCYxfwo9mEx","colab_type":"code","colab":{}},"source":["%%time\n","# an example using precomputed affinity, which uses the actual proximity graph\n","from sklearn.cluster import SpectralClustering\n","from sklearn.neighbors import kneighbors_graph\n","\n","X = pca\n","nclust = 7\n","Xknn = kneighbors_graph(X, 10) \n","\n","spc = SpectralClustering(n_clusters=nclust, affinity = 'precomputed')\n","labels = spc.fit_predict(Xknn) \n","\n","plt.scatter(X[:, 0], X[:, 1], c=labels,\n","                    cmap=plt.cm.rainbow, s=5, linewidths=0)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7ERGljg_X4p","colab_type":"code","colab":{}},"source":["#My Drive/ColabNotebooks/')\n","url_rfmodel = '/content/drive/My Drive/ColabNotebooks/spc2.joblib'\n","#exporting model to drive\n","from joblib import dump, load\n","dump(spc2, url_rfmodel) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgSyD4n79o4P","colab_type":"code","colab":{}},"source":["%%time\n","from sklearn.metrics.pairwise import pairwise_distances\n","\n","X = pca\n","nclust = 7\n","\n","d = pairwise_distances(X, metric='euclidean')\n","d = np.exp(- d**2 / (2.* 0.1**2)) # convert from euclidean distance to similarity\n","# this is Eric's function, empirically it works\n","\n","spc = SpectralClustering(n_clusters=nclust, affinity = 'precomputed', eigen_solver = 'arpack')\n","labels = spc.fit_predict(d) \n","\n","plt.scatter(X[:, 0], X[:, 1], c=labels,\n","                    cmap=plt.cm.rainbow, s=5, linewidths=0)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hWKfGFv_bX5","colab_type":"code","colab":{}},"source":["#My Drive/ColabNotebooks/')\n","url_rfmodel = '/content/drive/My Drive/ColabNotebooks/spc3.joblib'\n","#exporting model to drive\n","from joblib import dump, load\n","dump(spc3, url_rfmodel) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4lxQVjmhVBgn","colab_type":"text"},"source":["## Modeling and Evaluation 2 (10 points)\n","\n","Evaluate and Compare (10 Points)\n","• Eval: Use internal and/or external validation measures to describe and compare the\n","clusterings and the clusters— how did you determine a suitable number of clusters for each\n","method?  \n"]},{"cell_type":"markdown","metadata":{"id":"pY6DKoWDVERA","colab_type":"text"},"source":["## Modeling and Evaluation 3 (10 points)\n","\n","Visualize Results\n","• Visualize: Use tables/visualization to discuss the found results. Explain each visualization in\n","detail.  \n"]},{"cell_type":"markdown","metadata":{"id":"eA4km17QVHiQ","colab_type":"text"},"source":["## Modeling and Evaluation 4 (20 points)\n","\n","Summarize the Ramifications\n","• Summarize: Describe your results. What findings are the most interesting and why"]},{"cell_type":"markdown","metadata":{"id":"5MLbWLnGhRy5","colab_type":"text"},"source":["Option B: Association Rule Mining    \n","• Train: Create frequent itemsets and association rules (adjust parameters).  \n","• Eval: Use several measures for evaluating how interesting different rules are.  \n","• Visualize: Use tables/visualization to discuss the found results.  \n","• Summarize: Describe your results. What findings are the most compelling and why?    \n","Option C: Collaborative Filtering     \n","• Train: Create user-item matrices or item-item matrices using collaborative filtering (adjust\n","parameters).  \n","• Eval: Determine performance of the recommendations using different performance\n","measures (explain the ramifications of each measure). \n","MSDS 7331 Course Overview & Schedule 29  \n","• Visualize: Use tables/visualization to discuss the found results. Explain each visualization in\n","detail.      "]},{"cell_type":"markdown","metadata":{"id":"R_4fOaBVVa8f","colab_type":"text"},"source":["## Deployment (10 points)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v31EybaCXOmE","colab_type":"text"},"source":["Be critical of your performance and tell the reader how you current model might be usable by\n","other parties.  \n","• Did you achieve your goals? If not, can you reign in the utility of your modeling?  \n","• How useful is your model for interested parties (i.e., the companies or organizations that\n","might want to use it)?  \n","• How would you deploy your model for interested parties?  \n","• What other data should be collected?  \n","• How often would the model need to be updated, etc.?  "]},{"cell_type":"markdown","metadata":{"id":"-FUXzWlIbfp_","colab_type":"text"},"source":["https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html\n","Interactive\tGet Coffee\tOver Lunch\tOvernight\n","AffinityPropagation\t2000\t10000\t25000\t100000\n","Spectral\t2000\t5000\t25000\t75000\n","Agglomerative\t2000\t10000\t25000\t100000\n","DeBaCl\t5000\t25000\t75000\t250000\n","ScipySingleLinkage\t25000\t50000\t100000\t250000\n","Fastcluster\t50000\t100000\t500000\t1000000\n","HDBSCAN\t100000\t500000\t1000000\t5000000\n","DBSCAN\t75000\t250000\t1000000\t2500000\n","SKLearn KMeans\t1000000000\t1000000000\t1000000000\t1000000000"]},{"cell_type":"markdown","metadata":{"id":"td0rKRIbVdEC","colab_type":"text"},"source":["## Exceptional Work (10 points)\n"]},{"cell_type":"markdown","metadata":{"id":"4z2VwzHEXU2N","colab_type":"text"},"source":["You have free reign to provide additional analyses or combine analyses"]}]}